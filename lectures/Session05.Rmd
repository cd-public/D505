---
title: "Logistic Regression"
author: "Jameson Watts, Ph.D."
date: "02/01/2020"
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    toc_smooth: true
    number_sections: true
    df_print: kable
    fig_width: 11
    fig_height: 8
---
<style>
strong{
  color: #018080;
}
table.rmdtable th {
    background: #791716;
}

</style>

## Agenda

1. The math of logistic regression
2. Implementation with Caret
3. Regularization (penalized logit)

## First Model Due 3/7

**Turn in:**

1. An annotated RMD file 
2. that generates a dataframe 
3. with *only* the features you want in the model.

**Rules:**

1. You must specify whether I should use KNN or Naive Bayes and the *exact* values of any tuning parameters.
2. Models will be run with the following equation: "province ~ ." on your data frame
3. I will use repeated 10-fold cross validation
4. and the same index for partitioning training and test sets for every group.
5. Assignments must be turned in before class on the 7th
6. Grade is based on highest Kappa, with grades of 100, 95 and 90 to the first, second and third place finishers.
7. RMD files will be published to the class after grading is complete


# Logistic Regression

## Algorithm

We first assume a linear relationship between the log odds and a set of predictor variables.

$log(\frac{p}{1-p})=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}$

With a bit of algebra you can get the probabilities as...

$p=\frac{1}{1+e^{-(\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2})}}$

Why do we call this regression instead of classification?

# Implementation with Caret

## (Long) Setup
```{r setup, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(caret)
source('theme.R')

wine = as.data.frame(read_rds("../resources/pinot-project.rds")) %>% 
  mutate(oregon = as.integer(province=="Oregon")) %>% 
  rowid_to_column("id") %>% 
  select(-province)

#create a function to extract words with totals > j
wine_words <- function(data, j = 1000, stem=F){ 
  library(tidytext)
  library(SnowballC)
  data(stop_words)

  data <- data %>%
    unnest_tokens(word, description) %>%
    anti_join(stop_words) %>% # get rid of stop words
    filter(!(word %in% c("wine","pinot","vineyard")))
  
  if(stem){
    data <- data %>% 
      mutate(word = wordStem(word))
  }
  
  data <- data %>% 
    count(id, word) %>% 
    group_by(id) %>% 
    mutate(exists = (n>0)) %>% 
    ungroup %>% 
    group_by(word) %>% 
    mutate(total = sum(n)) %>% 
    filter(total > j) %>% 
    pivot_wider(id_cols = id, names_from = word, values_from = exists, values_fill = list(exists=0)) %>% 
    right_join(select(wine,id, oregon)) %>% 
    drop_na() %>% 
    select(-id)
}

wino <- wine_words(wine)

head(wino, 10) %>% 
  select(1:5,oregon)

```

## Split the data 

```{r}
wine_index <- createDataPartition(wino$oregon, p = 0.80, list = FALSE)
train <- wino[ wine_index, ]
test <- wino[-wine_index, ]
table(train$oregon)
```

## A basic model

```{r}
fit <- train(oregon ~ .,
             data = train, 
             trControl = trainControl(method = "cv", number = 5),
             method = "glm",
             family = "binomial")

#show the odds ratios for top coefficients
odds <- exp(coef(fit$finalModel))
data.frame(name = names(odds), odds = odds) %>%  
  mutate(probability=odds/(1+odds)) %>% 
  arrange(desc(odds)) %>% 
  head()
```

## Confusion Matrix

```{r}
prob <- predict(fit, newdata=test)
pred <- ifelse(prob > 0.5, 1, 0)

confusionMatrix(factor(pred),factor(test$oregon))
```

Not bad. But what if we increase the number of words used?

## Using more words

```{r}
wino <- wine_words(wine, j=500)
wine_index <- createDataPartition(wino$oregon, p = 0.80, list = FALSE)
train <- wino[ wine_index, ]
test <- wino[-wine_index, ]
fit <- train(oregon ~ .,
             data = train, 
             trControl = trainControl(method = "cv", number = 5),
             method = "glm",
             family = "binomial")

#show the odds ratios for top coefficients
odds <- exp(coef(fit$finalModel))
data.frame(name = names(odds), odds = odds) %>%  
  mutate(probability=odds/(1+odds)) %>% 
  arrange(desc(odds)) %>% 
  head()
```

## Confusion Matrix

```{r}
prob <- predict(fit, newdata=test)
pred <- ifelse(prob > 0.5, 1, 0)

confusionMatrix(factor(pred),factor(test$oregon))
```

## Using stems

```{r}
wino <- wine_words(wine, j=500, stem = T)
wine_index <- createDataPartition(wino$oregon, p = 0.80, list = FALSE)
train <- wino[ wine_index, ]
test <- wino[-wine_index, ]
fit <- train(oregon ~ .,
             data = train, 
             trControl = trainControl(method = "cv", number = 5),
             method = "glm",
             family = "binomial")

#show the odds ratios for top coefficients
odds <- exp(coef(fit$finalModel))
data.frame(name = names(odds), odds = odds) %>%  
  mutate(probability=odds/(1+odds)) %>% 
  arrange(desc(odds)) %>% 
  head()
```

## Confusion Matrix

```{r}
prob <- predict(fit, newdata=test)
pred <- ifelse(prob > 0.5, 1, 0)

confusionMatrix(factor(pred),factor(test$oregon))
```

Even better!

## ROC Curve evaluation

Remember: 

$Sensitivity = \frac{truePos}{allPos} = TruePosRate$

$Specificity = \frac{trueNeg}{allNeg} = TrueNegRate = 1 - FalsePosRate$

```{r}
library(pROC)
myRoc <- roc(test$oregon, prob)
plot(myRoc)
auc(myRoc)
```

```{r}
labels <- test$oregon[order(prob, decreasing=TRUE)]
roc_df <- data.frame(TruePosRate=cumsum(labels)/sum(labels), FalsePosRate=cumsum(!labels)/sum(!labels), labels)

roc_df %>% 
  ggplot(aes(FalsePosRate,TruePosRate))+
  geom_point()

```

## Exercise

1. Gather into your prediction teams.
2. Choose a Pinot province other than Oregon
3. Use logistic regression to find the words/terms that increase the odds of choosing that province the most


# Regularization

![](images/Regularization.png)

Often alsoAlso called 'penalized' because it adds a penalizing term to the loss function.

$\min_{f}\sum_{i=1}^{n}V(f(x_{i}),y_{i})+\lambda R(f)$

The amount of the penalty is fine-tuned using a constant called lambda. When lambda = 0, no penalty is enforced. The best lambda can be found by finding a value that minimizes prediction error after cross validating the model with different values.

```{r}
library(glmnet)
```

## Ridge Regression

$\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij}    \right) ^ 2 + \lambda \sum_{j=1}^{p} \beta_j^2$

Ridge regression shrinks the regression coefficients, so that variables, with minor contribution to the outcome, have their coefficients close to zero.

The shrinkage of the coefficients is achieved by penalizing the regression model with a penalty term called **L2-norm**, which is the sum of the squared coefficients. 

## Lasso Regression

$\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij}    \right) ^ 2 + \lambda \sum_{j=1}^{p} |\beta_j|$

Lasso stands for Least Absolute Shrinkage and Selection Operator. It shrinks the regression coefficients toward zero by penalizing the regression model with a penalty term called L1-norm, which is the sum of the absolute coefficients.

In the case of lasso regression, the penalty has the effect of forcing some of the coefficient estimates, with a minor contribution to the model, to be exactly equal to zero. This means that, lasso can be also seen as an alternative to the subset selection methods for performing variable selection in order to reduce the complexity of the model.

## Elastic Net

$\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij}  \right) ^ 2 + \alpha\lambda \sum_{j=1}^{p} |\beta_j|+(1-\alpha)\lambda \sum_{j=1}^{p} \beta_j^2$

Generally, lasso might perform better in a situation where some of the predictors have large coefficients, and the remaining predictors have very small coefficients.

Ridge regression will perform better when the outcome is a function of many predictors, all with coefficients of roughly equal size (James et al. 2014).


```{r}
wino <- wine_words(wine)
wine_index <- createDataPartition(wino$oregon, p = 0.80, list = FALSE)
train <- wino[ wine_index, ]
test <- wino[-wine_index, ]
fit <- train(
  oregon ~., 
  data = train,
  method = "glmnet",
  trControl = trainControl("cv"),
  tuneLength = 10
)

# Best tuning parameter
fit$bestTune
# to specify the best lambda
exp(coef(fit$finalModel, fit$bestTune$lambda))
```

## Confusion Matrix

```{r}
prob <- predict(fit, newdata=test)
pred <- ifelse(prob > 0.5, 1, 0)

confusionMatrix(factor(pred),factor(test$oregon))
```

## ROC Curve

```{r}
myRoc <- roc(test$oregon, prob)
plot(myRoc)
auc(myRoc)
```

# References

James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2014. An Introduction to Statistical Learning: With Applications in R. Springer Publishing Company, Incorporated.

http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/153-penalized-regression-essentials-ridge-lasso-elastic-net/

https://en.wikipedia.org/wiki/Regularization_(mathematics)

