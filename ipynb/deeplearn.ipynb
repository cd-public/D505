{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cd-public/D505/blob/master/ipynb/deeplearn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geJpLC0K2F26"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TphVPT9O2F29"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1YIk1LZ2F2_"
      },
      "source": [
        "# Neural Networks\n",
        "\n",
        "## Neural Networks Basics\n",
        "\n",
        "> A neural network is a computational model inspired by the structure and functioning of the human brain. It consists of interconnected nodes (neurons) organized in layers.\n",
        "\n",
        "- It turns out maybe they don't model brains that well but that's okay, they run really well on GPUs, the thing that used to just by a toy for games.\n",
        "  \n",
        "## Basic Architecture\n",
        "  - Input Layer: Receives input data features.\n",
        "  - Hidden Layers: Intermediate layers that perform computations.\n",
        "  - Output Layer: Produces the final output or prediction.\n",
        "\n",
        "## Forward Pass/Activation Functions\n",
        "\n",
        "  - Forward Pass: Input data flows through the network, and computations are performed layer by layer until the output is generated.\n",
        "  - Activation Functions: Non-linear functions applied to the weighted sum of inputs to introduce non-linearity and enable the network to learn complex patterns.\n",
        "  - Automatic feature engineering: we imagine that all the sophisticated feature engineering we are used to doing by hand happen automatically in the hidden layers.\n",
        "\n",
        "##\n",
        "\n",
        "<a title=\"Paskari at the English-language Wikipedia, CC BY-SA 3.0 &lt;http://creativecommons.org/licenses/by-sa/3.0/&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Feed_forward_neural_net.gif\"><img width=\"50%\" alt=\"Feed forward neural net\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/54/Feed_forward_neural_net.gif?20240322215401\"></a>\n",
        "\n",
        "## Functional Form of a Neuron\n",
        "- Input features or values.\n",
        "$$\n",
        "x = (x_1, x_2, ..., x_n)\n",
        "$$\n",
        "\n",
        "## Functional Form of a Neuron\n",
        "\n",
        "- Weighted Sum: Linear combination of inputs with weights and bias.\n",
        "$$\n",
        "z = \\sum_{i=1}^{n} w_i \\cdot x_i + b\n",
        "$$\n",
        "\n",
        "## Functional Form of a Neuron\n",
        "\n",
        "- Linear combination of inputs with weights and bias.\n",
        "- Activation Function: Non-linear function applied to the weighted sum.\n",
        "$$\n",
        "a = f(z)\n",
        "$$\n",
        "\n",
        "\n",
        "## Functional Form of a Neuron\n",
        "\n",
        "<center>\n",
        "<a title=\"Funcs, CC0, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Artificial_neuron_structure.svg\"><img style=\"background-color:white\" width=\"100%\" alt=\"Artificial neuron structure\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c6/Artificial_neuron_structure.svg/512px-Artificial_neuron_structure.svg.png?20240531082700\"></a>\n",
        "</center>\n",
        "\n",
        "# Activation Functions\n",
        "\n",
        "## Sigmoid Function\n",
        "\n",
        "- S-shaped curve mapping input to a range between 0 and 1.\n",
        "- Used in binary classification tasks.\n",
        "- [Throwback](https://github.com/cd-public/cdml15/blob/main/hw1/sigmoid.pdf)\n",
        "$$s\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "## Sigmoid\n",
        "\n",
        "<a title=\"Qef, Public domain, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Logistic-curve.svg\"><img width=\"80%\" alt=\"Sigmoid Function Plot / Logistic Curve\" style=\"background-color:white\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/512px-Logistic-curve.svg.png?20140704193223\"></a>\n",
        "\n",
        "\n",
        "## Softmax Function\n",
        "$$\n",
        "\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{N} e^{z_j}}\n",
        "$$\n",
        "\n",
        "- Outputs a probability distribution over multiple classes. Used in multi-class classification tasks.\n",
        "\n",
        "## ReLU (Rectified Linear Unit)\n",
        "$$\n",
        "\\text{ReLU}(z) = \\max(0, z)\n",
        "$$\n",
        "\n",
        "- Outputs the input if it's positive, otherwise, outputs zero. Helps in overcoming the vanishing gradient problem.\n",
        "\n",
        "## Others\n",
        "- **Tanh:** Hyperbolic tangent function, mapping input to a range between -1 and 1.\n",
        "- **Leaky ReLU:** Variation of ReLU that allows a small gradient for negative inputs, addressing the dying ReLU problem.\n",
        "- Many other activation functions exist, each with different properties and use cases.\n",
        "\n",
        "# Python\n",
        "\n",
        "## Why Python?\n",
        "\n",
        "- [https://www.indeed.com/viewjob?jk=38667955752f8d57&from=shareddesktop_copy](https://www.indeed.com/viewjob?jk=38667955752f8d57&from=shareddesktop_copy)\n",
        "- [https://www.indeed.com/viewjob?jk=d876a09728e8a21c&from=shareddesktop_copy](https://www.indeed.com/viewjob?jk=d876a09728e8a21c&from=shareddesktop_copy)\n",
        "- [https://www.indeed.com/viewjob?jk=3eaf03f7179b791c&from=shareddesktop_copy](https://www.indeed.com/viewjob?jk=3eaf03f7179b791c&from=shareddesktop_copy)\n",
        "- [https://www.indeed.com/viewjob?jk=8cc913a6e60fdb12&from=shareddesktop_copy](https://www.indeed.com/viewjob?jk=8cc913a6e60fdb12&from=shareddesktop_copy)\n",
        "- [https://www.indeed.com/viewjob?jk=6be9e463d5db8fe0&from=shareddesktop_copy](https://www.indeed.com/viewjob?jk=6be9e463d5db8fe0&from=shareddesktop_copy)\n",
        "\n",
        "## Why Python?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8gmInju2F3C"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmWw3U2X2F3E"
      },
      "source": [
        "# Here is an example neural network in PyTorch\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        print(out.shape)\n",
        "        out = self.fc2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAljHCXp2F3F"
      },
      "source": [
        "## Why not R?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwRv7JaS2F3G"
      },
      "source": [
        "import rpy2\n",
        "import rpy2.rinterface"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKx3M2092F3H"
      },
      "source": [
        "%load_ext rpy2.ipython"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_toyu_kF2F3H"
      },
      "source": [
        "## Equivalent code, ish\n",
        "\n",
        "- Not GPU accelerated!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiIZFTfS2F3I"
      },
      "source": [
        "%%R\n",
        "library(torch)\n",
        "\n",
        "# Define the neural network class\n",
        "SimpleNN <- nn_module(\n",
        "  initialize = function(input_size, hidden_size, output_size) {\n",
        "    self$fc1 <- nn_linear(input_size, hidden_size)\n",
        "    self$relu <- nn_relu()\n",
        "    self$fc2 <- nn_linear(hidden_size, output_size)\n",
        "    self$sigmoid <- nn_sigmoid()\n",
        "  },\n",
        "\n",
        "  forward = function(x) {\n",
        "    out <- self$fc1(x)\n",
        "    out <- self$relu(out)\n",
        "    print(dim(out))\n",
        "    out <- self$fc2(out)\n",
        "    out <- self$sigmoid(out)\n",
        "    out\n",
        "  }\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzcyu6ws2F3J"
      },
      "source": [
        "## Universal Approximation Theorem (UAT)\n",
        "\n",
        "- The UAT states that a feed-forward neural network with a single hidden layer and a non-linear activation function can approximate any continuous function to arbitrary accuracy given enough neurons in the hidden layer.\n",
        "- This theorem highlights the expressive power of neural networks in capturing complex relationships and functions.\n",
        "\n",
        "## Key Points\n",
        "- Neural networks with non-linear activation functions can learn and represent highly nonlinear and intricate mappings between inputs and outputs.\n",
        "- The flexibility and adaptability of neural networks make them suitable for a wide range of tasks, including regression and classification.\n",
        "- The number of neurons in the hidden layer and the choice of activation function play crucial roles in the network's capacity to approximate complexfunctions.\n",
        "\n",
        "\n",
        "# Training Neural Networks\n",
        "\n",
        "## Training Process\n",
        "\n",
        "- Initialize all parameter values to small random numbers.\n",
        "- Forward Pass:\n",
        "  - Input data is passed through the network, and computations are performed layer by layer.\n",
        "  - Activation functions introduce non-linearity into the model.\n",
        "  \n",
        "## Loss Calculation\n",
        "- The output of the network is compared to the target values using a loss function (more or less error).\n",
        "- Common loss functions include Mean Squared Error (MSE), Cross Entropy Loss, etc.\n",
        "  \n",
        "## Backward Pass (Gradient Descent)\n",
        "- Gradients of the loss function with respect to the model parameters are computed using backpropagation.\n",
        "- Optimizers update the model parameters (weights and biases) to minimize the loss.\n",
        "- [Read more](https://cd-public.github.io/courses/old/ccf24/slides/08_1_3d.html)\n",
        "\n",
        "## Update Weights and Biases\n",
        "\n",
        "- Optimizers like stochastic gradient descent, root mean square propagation, adjust the model parameters based on computed gradients and learning rate.\n",
        "\n",
        "## Training Neural Networks\n",
        "- Training Process\n",
        "- Loss Calculation\n",
        "- Backward Pass (Gradient Descent)\n",
        "- Update Weights and Biases\n",
        "\n",
        "## Iris\n",
        "\n",
        "- In Python, we use capital `X` to denote a matrix (data frame), for clarity.\n",
        " - Consistent with linear algebra - $X$ and $y$ often used.\n",
        " - The Iris data set is built into seaborn, the visualization package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izyEM-Mx2F3K"
      },
      "source": [
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGYusMZ52F3K"
      },
      "source": [
        "## Standardize the features\n",
        "\n",
        "- Like caret boxcox, preprocessing is fairly straightfoward in Python, here to entire matrix, or data frame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVBfz7-b2F3L"
      },
      "source": [
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhH3HZO62F3M"
      },
      "source": [
        "## Train/test split single-line"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1Wqhuq-2F3M"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFxyDMAv2F3M"
      },
      "source": [
        "## Convert to PyTorch tensors\n",
        "\n",
        "- What are \"float\" and \"long\"?\n",
        "- What is that 32 for?\n",
        "- What is a tensor?\n",
        "  - To matrix as matrix to vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LJqeXzi2F3N"
      },
      "source": [
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqrumwBN2F3N"
      },
      "source": [
        "## TensorDataset and DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOygRany2F3O"
      },
      "source": [
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qan1g11c2F3O"
      },
      "source": [
        "## Define loss & optimizer\n",
        "\n",
        "\n",
        "- Cross entropy is common classification loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxVvHcc-2F3O"
      },
      "source": [
        "model = SimpleNN(input_size=4, hidden_size=10, output_size=3)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvg9rgFe2F3O"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkbUng1S2F3P"
      },
      "source": [
        "for epoch in range(10): # num of epochs\n",
        "\n",
        "    for inputs, targets in train_loader: # iterate over data\n",
        "        optimizer.zero_grad()  # Zero the gradients from previous step\n",
        "        outputs = model(inputs) # Forward pass\n",
        "        loss = criterion(outputs, targets) # Compute the loss\n",
        "\n",
        "        loss.backward()  # Compute gradients\n",
        "        optimizer.step()  # Update model parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKVpJLwv2F3P"
      },
      "source": [
        "## Without Gradient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj1O97XF2F3P"
      },
      "source": [
        "with torch.no_grad():  # Disable gradient calculation\n",
        "    test_outputs = model(X_test)\n",
        "    _, predicted = torch.max(test_outputs, 1)  # Get the class with the highest score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnm9sVVw2F3P"
      },
      "source": [
        "## Print the predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gm7vjKAp2F3P"
      },
      "source": [
        "rand = np.random.choice(np.unique(y_test.numpy()), size=len(y_test), replace=True)\n",
        "print(\"Predicted labels:\", predicted.numpy())\n",
        "print(\"Reference labels:\", y_test.numpy())\n",
        "print(\"~Random guessing:\", rand)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0e55tcS2F3Q"
      },
      "source": [
        "## Count them\n",
        "\n",
        "- What is \"int64\"?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkimN-8H2F3Q"
      },
      "source": [
        "sum(predicted.numpy() == y_test.numpy()), sum(rand == y_test.numpy()), len(predicted) # Count correct predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40-DiPG62F3Q"
      },
      "source": [
        "# Practical Applications of Neural Networks\n",
        "\n",
        "## Image Classification\n",
        "  - Identifying objects, scenes, or patterns within images.\n",
        "  - Applications in healthcare, autonomous vehicles, security, etc.\n",
        "  - Was the basis of the new research direction in GPU acceleration ML\n",
        "\n",
        "## Natural Language Processing (NLP)\n",
        "  - Text analysis, sentiment analysis, language translation, chatbots, etc.\n",
        "  - Used in social media, customer support, content generation, etc.\n",
        "  - Basis of Jameson's ML interest, like tidytext.\n",
        "\n",
        "## Medical diagnosis\n",
        "  - Disease diagnosis, medical imaging analysis, patient monitoring, drug discovery, etc.\n",
        "  - Improving healthcare outcomes and decision-making.\n",
        "  - To me, either a subset of vision or very sketchy very quickly, but...\n",
        "  - AlphaFold\n",
        "\n",
        "# Let's Learn Something\n",
        "\n",
        "## I am cold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW4jhjrE2F3Q"
      },
      "source": [
        "# Input features (temperature in Celsius)\n",
        "t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0]\n",
        "x = torch.tensor(t_c).view(-1, 1)  # Reshape to a 2D tensor with 11 rows and 1 column\n",
        "\n",
        "# Target values (temperature in Fahrenheit)\n",
        "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
        "y = torch.tensor(t_u).view(-1, 1)  # Reshape to a 2D tensor with 11 rows and 1 column"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsAZc2Bx2F3R"
      },
      "source": [
        "## Plot it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asUF3FR-2F3R"
      },
      "source": [
        "plt.scatter(t_c, t_u);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El2wqDGM2F3R"
      },
      "source": [
        "## Scale it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6AzZoTs2F3R"
      },
      "source": [
        "# Data normalization\n",
        "scaler_x = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "x_normalized = scaler_x.fit_transform(x.float())\n",
        "y_normalized = scaler_y.fit_transform(y.float())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZPhUuhs2F3S"
      },
      "source": [
        "## Think it\n",
        "\n",
        "- Just no graceful way to do this part in R."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn8ZTda32F3S"
      },
      "source": [
        "class LinearNet(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(LinearNet, self).__init__()\n",
        "        self.lin_coeffs = nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lin_coeffs(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpMDcZqf2F3S"
      },
      "source": [
        "## Also an LM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrpaFdGM2F3S"
      },
      "source": [
        "# Define a simple linear regression model\n",
        "class LinearRegression(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinearRegression, self).__init__()\n",
        "        self.linear = nn.Linear(1, 1)  # One input feature, one output\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37wuD-SB2F3T"
      },
      "source": [
        "## Set it up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xptw2eB2F3T"
      },
      "source": [
        "# Instantiate the linear regression model, loss function, and optimizer\n",
        "model = LinearNet(1,1)\n",
        "criterion = nn.MSELoss()  # Mean Squared Error loss\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc7_-WB52F3T"
      },
      "source": [
        "## Train it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djOGqpCP2F3T"
      },
      "source": [
        "for epoch in range(1000):\n",
        "    y_pred = model(torch.tensor(x_normalized, dtype=torch.float32))\n",
        "    loss = criterion(y_pred, torch.tensor(y_normalized, dtype=torch.float32))\n",
        "    epoch % 100 == 0 and print(f'Epoch {epoch + 1}, Loss: {loss.item()}', model.state_dict())\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkOCY5ub2F3T"
      },
      "source": [
        "## Examine model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVzwu_G-2F3U"
      },
      "source": [
        "# After training, print the final model parameters\n",
        "print(f'Final Model Parameters: {model.state_dict()}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDTNimLX2F3U"
      },
      "source": [
        "# Penguins\n",
        "\n",
        "## Inspect Data frame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ET_OPeCb2F3U"
      },
      "source": [
        "penguins = sns.load_dataset(\"penguins\")\n",
        "penguins = penguins.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGu641Z_2F3U"
      },
      "source": [
        "## Inspect Data frame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5-NJ_QH2F3V"
      },
      "source": [
        "penguins.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PQMiUBJ2F3V"
      },
      "source": [
        "penguins.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8N6RHDS2F3W"
      },
      "source": [
        "## We wish to *label*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e_-1cxl2F3W"
      },
      "source": [
        "label_encoder = LabelEncoder()\n",
        "penguins['species_encoded'] = label_encoder.fit_transform(penguins['species'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WhTz-py2F3W"
      },
      "source": [
        "## Create a class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mG-0Ck12F3W"
      },
      "source": [
        "class PenguinDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.X = data[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']].values\n",
        "        self.y = data['species_encoded'].values # DONT FORGET .VALUES\n",
        "        self.n_samples = len(data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return torch.tensor(self.X[index], dtype=torch.float32), torch.tensor(self.y[index], dtype=torch.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ-izLXJ2F3W"
      },
      "source": [
        "## Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFk5G0ob2F3X"
      },
      "source": [
        "train_data, test_data = train_test_split(penguins, test_size=0.2, random_state=97301)\n",
        "\n",
        "train_dataset = PenguinDataset(train_data)\n",
        "test_dataset = PenguinDataset(test_data)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=16, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVp80QAC2F3X"
      },
      "source": [
        "## Have a Look"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkjIfL4m2F3X"
      },
      "source": [
        "train_dataset.__getitem__(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GszT2Axx2F3X"
      },
      "source": [
        "# We can ReLU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFDKn1NF2F3X"
      },
      "source": [
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48liKhbC2F3Y"
      },
      "source": [
        "## Configure the NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzWYPOwW2F3Y"
      },
      "source": [
        "input_size = 4  # Number of features\n",
        "hidden_size = 64  # Size of the hidden layer\n",
        "num_classes = len(label_encoder.classes_)\n",
        "learning_rate = 0.001\n",
        "num_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUKTbFhR2F3Y"
      },
      "source": [
        "## Run it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JC5rVbG2F3Y"
      },
      "source": [
        "model = SimpleNN(input_size, hidden_size, num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Adam is an loss fncn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAMEfWU52F3Y"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CS_QZu-i2F3Z"
      },
      "source": [
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/10, Loss: {epoch_loss:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Tj71u3h2F3Z"
      },
      "source": [
        "## Evaluation on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hvBjvh32F3Z"
      },
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qirnN2tQ2F3Z"
      },
      "source": [
        "## Go line-by-line"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA4Wx25W2F3Z"
      },
      "source": [
        "with torch.no_grad(): # to use for inference\n",
        "    for inputs, targets in test_loader: # test data\n",
        "        outputs = model(inputs) # infer\n",
        "        vals, predicted = torch.max(outputs.data, 1) # get best\n",
        "        total += targets.size(0) # count observations\n",
        "        correct += (predicted == targets).sum().item() # count same"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNaq6ULr2F3a"
      },
      "source": [
        "## See it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01OmU6yE2F3a"
      },
      "source": [
        "accuracy = correct / total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ8BUxPD2F3a"
      },
      "source": [
        "- Well that was awful. Before we fix it...\n",
        "\n",
        "\n",
        "## Print it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTUYZLha2F3b"
      },
      "source": [
        "with torch.no_grad():\n",
        "    for i in range(3):\n",
        "      for batch_idx, batch in enumerate(test_loader):\n",
        "          outputs = model(inputs)\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          print(inputs, outputs, predicted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btgCIo322F3b"
      },
      "source": [
        "## Scale it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTtbFRcL2F3b"
      },
      "source": [
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(penguins[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']])\n",
        "penguins[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']] = scaled_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efDpV6DS2F3b"
      },
      "source": [
        "## Re-split\n",
        "\n",
        "- Why do we have to resplit?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIShuAw72F3c"
      },
      "source": [
        "train_data, test_data = train_test_split(penguins, test_size=0.2, random_state=12345)\n",
        "\n",
        "train_dataset = PenguinDataset(train_data)\n",
        "test_dataset = PenguinDataset(test_data)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=16, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex-gtotc2F3c"
      },
      "source": [
        "## Model Again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhnQHeKl2F3c"
      },
      "source": [
        "model = SimpleNN(input_size, hidden_size, num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvrXtn6J2F3c"
      },
      "source": [
        "## Learn with scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awkW-lMK2F3c"
      },
      "source": [
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/10, Loss: {epoch_loss:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrMrtUnI2F3c"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0hTgWfs2F3d"
      },
      "source": [
        "# Evaluation on the test set\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "correct / total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpjqKktM2F3d"
      },
      "source": [
        "# Exercise\n",
        "\n",
        "## Exercise: the titanic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8npLigc2F3d"
      },
      "source": [
        "url = \"https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv\"\n",
        "titanic_df = pd.read_csv(url)\n",
        "titanic_df = titanic_df.dropna() # Drop rows with missing values for simplicity\n",
        "titanic_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqIS_pqi2F3d"
      },
      "source": [
        "## Your exercise:\n",
        "\n",
        "- Create a neural network as above to model survival on the titanic dataset.\n",
        "- There are several ways to do this:\n",
        "  * change the size of the output layer (a simple probability, so 1)\n",
        "  * change the output of the final hidden layer to be a probability using nn.Sigmoid()\n",
        "  * change the loss criterion to be nn.BCELoss()\n",
        "\n",
        "## Notes:\n",
        "\n",
        "- You can do this all differently: use 2 outputs (one per class), omit sigmoid and keep the same loss function, but the difference might be instructive.\n",
        "- Explore variations of the model architecture (multiple hidden layers? hidden layer size? etc.)\n",
        "-  encourage you to print out lots of intermediate things.\n",
        "- I learned a lot doing it and I bet you will too.\n",
        "\n",
        "## One solution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-Y2Byt92F3d"
      },
      "source": [
        "# Select relevant features and target\n",
        "features = ['Pclass', 'Age', 'Siblings/Spouses Aboard', 'Parents/Children Aboard', 'Fare']\n",
        "target = 'Survived'\n",
        "\n",
        "# Standard scale the features\n",
        "scaler = StandardScaler()\n",
        "titanic_df[features] = scaler.fit_transform(titanic_df[features])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hvt1EQlj2F3e"
      },
      "source": [
        "## One solution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ifRevC42F3e"
      },
      "source": [
        "# Define a custom PyTorch dataset\n",
        "class TitanicDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.X = data[features].values\n",
        "        self.y = data[target].values\n",
        "        self.n_samples = len(data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return torch.tensor(self.X[index], dtype=torch.float32), torch.tensor(self.y[index], dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkreYAz02F3e"
      },
      "source": [
        "## One solution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esk0fYsK2F3e"
      },
      "source": [
        "# Split data into train and test sets\n",
        "train_data, test_data = train_test_split(titanic_df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create PyTorch datasets and dataloaders\n",
        "train_dataset = TitanicDataset(train_data)\n",
        "test_dataset = TitanicDataset(test_data)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=16, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G-usUjo2F3f"
      },
      "source": [
        "## One solution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ocB71wK2F3f"
      },
      "source": [
        "train_dataset.__getitem__(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzHKNwzF2F3f"
      },
      "source": [
        "## One solution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FB3P_rY92F3f"
      },
      "source": [
        "# Define a simple neural network with one hidden layer\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Rm9kXUc2F3h"
      },
      "source": [
        "## One solution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZbs3Y8X2F3h"
      },
      "source": [
        "# Initialize the model, loss function, and optimizer\n",
        "input_size = len(features)  # Number of features\n",
        "hidden_size = 64  # Size of the hidden layer\n",
        "output_size = 1  # Output size (binary classification for survival)\n",
        "\n",
        "model = SimpleNN(input_size, hidden_size, output_size)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAkf2jYO2F3i"
      },
      "source": [
        "## One solution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NFeOSmf2F3i"
      },
      "source": [
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        #print(outputs)\n",
        "        #print(targets)\n",
        "        #print(outputs.squeeze())\n",
        "        #print(outputs.shape)\n",
        "        #print(outputs.squeeze().shape)\n",
        "        loss = criterion(outputs.squeeze(), targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/10, Loss: {epoch_loss:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0_lVD6U2F3j"
      },
      "source": [
        "## One solution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_T8-OHt72F3j"
      },
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        predicted = torch.round(outputs)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets.unsqueeze(1)).sum().item()  # Ensure targets are 2D\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Accuracy on test set: {accuracy:.2%}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\calvin\\AppData\\Local\\Programs\\Python\\Python313\\share\\jupyter\\kernels\\python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}