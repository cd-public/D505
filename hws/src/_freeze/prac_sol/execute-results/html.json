{
  "hash": "8d9dd89bb6194c566f1b6a94d1f107db",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Characterizing Colleges\"\nauthor: \"Solutions\"\ndate: \"03/10/2025\"\n---\n\n\n\n# Setup\n\n- Setup\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsh <- suppressPackageStartupMessages\nsh(library(tidyverse))\nsh(library(caret))\nsh(library(class))\nsh(library(ISLR)) # for the \"College\" dataframe\n```\n:::\n\n\n\n# Dataframe\n\n- We use the `College` dataframe.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(College)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                             Private Apps Accept Enroll Top10perc Top25perc\nAbilene Christian University     Yes 1660   1232    721        23        52\nAdelphi University               Yes 2186   1924    512        16        29\nAdrian College                   Yes 1428   1097    336        22        50\nAgnes Scott College              Yes  417    349    137        60        89\nAlaska Pacific University        Yes  193    146     55        16        44\nAlbertson College                Yes  587    479    158        38        62\n                             F.Undergrad P.Undergrad Outstate Room.Board Books\nAbilene Christian University        2885         537     7440       3300   450\nAdelphi University                  2683        1227    12280       6450   750\nAdrian College                      1036          99    11250       3750   400\nAgnes Scott College                  510          63    12960       5450   450\nAlaska Pacific University            249         869     7560       4120   800\nAlbertson College                    678          41    13500       3335   500\n                             Personal PhD Terminal S.F.Ratio perc.alumni Expend\nAbilene Christian University     2200  70       78      18.1          12   7041\nAdelphi University               1500  29       30      12.2          16  10527\nAdrian College                   1165  53       66      12.9          30   8735\nAgnes Scott College               875  92       97       7.7          37  19016\nAlaska Pacific University        1500  76       72      11.9           2  10922\nAlbertson College                 675  67       73       9.4          11   9727\n                             Grad.Rate\nAbilene Christian University        60\nAdelphi University                  56\nAdrian College                      54\nAgnes Scott College                 59\nAlaska Pacific University           15\nAlbertson College                   55\n```\n\n\n:::\n:::\n\n\n\n- States the ISLR textbook:\n\n|Name|Description|\n|-|-|\n| `Private` | Public/private indicator |\n| `Apps` | Number of applications received |\n| `Accept` | Number of applicants accepted |\n| `Enroll` | Number of new students enrolled |\n| `Top10perc` | New students from top 10 % of high school class |\n| `Top25perc` | New students from top 25 % of high school class |\n| `F.Undergrad` | Number of full-time undergraduates |\n| `P.Undergrad` | Number of part-time undergraduates |\n| `Outstate` | Out-of-state tuition |\n| `Room.Board` | Room and board costs |\n| `Books` | Estimated book costs |\n| `Personal` | Estimated personal spending |\n| `PhD` | Percent of faculty with Ph.D.â€™s |\n| `Terminal` | Percent of faculty with terminal degree |\n| `S.F.Ratio` | Student/faculty ratio |\n| `perc.alumni` | Percent of alumni who donate |\n| `Expend` | Instructional expenditure per student |\n| `Grad.Rate` | Graduation rate |\n\n# Multiple Regression\n\n- Run a linear regression model with `Grad.Rate` as the dependent variable and `PhD` and `Expend` as features (variables).\n    - Regard `PhD` and `Expend` as two forms of investment in education - in training for instructors, and in resources for students.\n- Compute and comment on the RMSE.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm1 = lm(Grad.Rate ~ PhD, College)\nm2 = lm(Grad.Rate ~ Expend, College)\nm3 = lm(Grad.Rate ~ PhD + Expend, College)\nm4 = lm(Grad.Rate ~ PhD * Expend, College)\nm5 = lm(Grad.Rate ~ ., College)\n\nget_rmse <- function(m) {\n    pred <- predict(m, newdata = College)\n    sqrt(mean((College$Grad.Rate - pred)^2))\n}\n\nunlist(lapply(list(m1, m2, m3, m4, m5), get_rmse))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 16.34849 15.80482 15.59084 15.57864 12.59685\n```\n\n\n:::\n:::\n\n\n\n> While neither PhD attainment of teaching faculty nor the institutional expenditure per student are extraordinarily accurate - an error of ~16 with regards to a percentage intuitively feels high - nothing in the data set necessarily naively predicts graduation rate so well, and the interaction of these features predicts better than either feature individually or even both features without an interaction term. So, I expect both educational attainment of faculty and expenditure per student are important parts of delivering a high quality education, and each improves the other's value, though I would need to separately ensure the coefficients are positive to validate this claim:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm4\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Grad.Rate ~ PhD * Expend, data = College)\n\nCoefficients:\n(Intercept)          PhD       Expend   PhD:Expend  \n  4.899e+01    1.002e-01    2.757e-04    8.829e-06  \n```\n\n\n:::\n:::\n\n\n\n# Feature Engineering\n\n- Create 10 total features. Consider:\n    - Attributes of the student body.\n        - For example, an acceptance rate, or a percentages of students in other categories vs. accepted/enrolled.\n    - Costs of the university.\n    - Some other category, such as related to success, alumni, or faculty.\n- Remove all rows with a missing value.\n- Ensure only `Grad.Rate` and the engineered features remain.\n- Compute and comment on the RMSE.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_all <- College %>%\n            mutate(AcceptRate=Accept/Apps) %>%\n            mutate(EnrollRate=Enroll/Accept) %>%\n            mutate(Top10Rate=Top10perc/Enroll) %>%\n            mutate(Top10Rate=Top25perc/Enroll) %>%\n            mutate(Cost=Outstate+Room.Board+Books+Personal) %>%\n            mutate(MS=Terminal-PhD) %>% \n            mutate(AppToAlum = AcceptRate * EnrollRate * perc.alumni)\n        \ndf_feat <- df_all %>%\n             select(-Private,-Apps,-Accept,-Enroll,-Top10perc,-Top25perc,-F.Undergrad,-P.Undergrad,-Outstate,-Room.Board) %>%\n             select(-Books,-Personal,-PhD,-Terminal,-S.F.Ratio,-perc.alumni,-Expend)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsqrt(mean((df_all$Grad.Rate - predict(lm(formula = Grad.Rate ~ ., data = df_all), newdata = df_all))^2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 12.36756\n```\n\n\n:::\n\n```{.r .cell-code}\nsqrt(mean((df_feat$Grad.Rate - predict(lm(formula = Grad.Rate ~ ., data = df_feat), newdata = df_feat))^2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 13.4235\n```\n\n\n:::\n:::\n\n\n\n> Adding my novel features did marginally improve predictive power, but removing the initial, provided features, worsened performances versus just using the original data set. I should be more intentional and systematic about:\n(1) including all elements of the original data frame which may be relevant in my engineered features, and (2) ensuring independence between retained features to ensure that the assumptions of linear models are satisfied.\n\n# Classification Methods\n\n- Use either of $K$-NN or Naive Bayes to predict whether a college is `Private`.\n- Explain your choice of technique.\n- Report on your Kappa value.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncontrol = trainControl(method = \"cv\", number = 5)\n```\n:::\n\n\n\n- We try a $K$-NN over a few features.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_knn = df_all %>% \n           select(Private, AcceptRate, PhD, Cost, Top10perc)\n\nsplit <- createDataPartition(df_knn$Private, p = 0.8, list = FALSE)\ntrain_knn <- df_knn[split, ]\ntest_knn <- df_knn[-split, ]\n\nfit_knn = train(Private ~ .,\n                data = train_knn, \n                method = \"knn\",\n                tuneLength = 15,\n                metric = \"Kappa\",\n                trControl = control)\n\nconfusionMatrix(predict(fit_knn, test_knn),factor(test_knn$Private))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No   23   7\n       Yes  19 106\n                                         \n               Accuracy : 0.8323         \n                 95% CI : (0.764, 0.8874)\n    No Information Rate : 0.729          \n    P-Value [Acc > NIR] : 0.001745       \n                                         \n                  Kappa : 0.5336         \n                                         \n Mcnemar's Test P-Value : 0.030984       \n                                         \n            Sensitivity : 0.5476         \n            Specificity : 0.9381         \n         Pos Pred Value : 0.7667         \n         Neg Pred Value : 0.8480         \n             Prevalence : 0.2710         \n         Detection Rate : 0.1484         \n   Detection Prevalence : 0.1935         \n      Balanced Accuracy : 0.7428         \n                                         \n       'Positive' Class : No             \n                                         \n```\n\n\n:::\n:::\n\n\n\n- We bin a few features and try Naive.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_nb = df_knn %>% \n          mutate(HighAccept=AcceptRate > mean(df_knn$AcceptRate)) %>%\n          mutate(HighPhD=PhD > mean(df_knn$PhD)) %>%\n          mutate(HighCost=Cost > mean(df_knn$Cost)) %>%\n          mutate(HighTop10=Top10perc > mean(df_knn$Top10perc)) %>%\n          select(-AcceptRate, -PhD, -Cost, -Top10perc)\n\nsplit <- createDataPartition(df_nb$Private, p = 0.8, list = FALSE)\ntrain_nb <- df_nb[split, ]\ntest_nb <- df_nb[-split, ]\n\nfit_nb = train(Private ~ .,\n               data = train_nb, \n               method = \"naive_bayes\",\n               tuneLength = 15,\n               metric = \"Kappa\",\n               trControl = control)\n\nconfusionMatrix(predict(fit_nb, test_nb),factor(test_nb$Private))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No   20  11\n       Yes  22 102\n                                          \n               Accuracy : 0.7871          \n                 95% CI : (0.7142, 0.8487)\n    No Information Rate : 0.729           \n    P-Value [Acc > NIR] : 0.05965         \n                                          \n                  Kappa : 0.4128          \n                                          \n Mcnemar's Test P-Value : 0.08172         \n                                          \n            Sensitivity : 0.4762          \n            Specificity : 0.9027          \n         Pos Pred Value : 0.6452          \n         Neg Pred Value : 0.8226          \n             Prevalence : 0.2710          \n         Detection Rate : 0.1290          \n   Detection Prevalence : 0.2000          \n      Balanced Accuracy : 0.6894          \n                                          \n       'Positive' Class : No              \n                                          \n```\n\n\n:::\n:::\n\n\n\n> I expect public schools, due to the forms of regulatory oversight, they experience, to be clustered fairly neatly around certain admission, instruction preparation, and acceptance metrics, with of course a few exceptions for e.g. the \"Public Ivies\" like UC Berkley and UT Austin. This is reflected in drastically more accurate Kappa values for Naive Bayes, which are reflective, to me, of public universities as a rule being on the same side of the means across various metrics as other public universities.\n\n# Classification Techniques\n\n- Predict whether a college is `Private`.\n- Use model weights.\n- Display and comment on an ROC curve.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncounts <- table(df_knn$Private)\ncount_y <- counts[\"Yes\"]\ncount_n <- counts[\"No\"]\nweigh_y <- max(count_y,count_n)/count_y\nweigh_n <- max(count_y,count_n)/count_n\n\nc(count_y,count_n,weigh_y,weigh_n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       Yes         No        Yes         No \n565.000000 212.000000   1.000000   2.665094 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_knn <- train_knn %>% \n               mutate(weight=ifelse(Private==\"Yes\", weigh_y, weigh_n))\n\nfit_weights = train(Private ~ .,\n                    data = train_knn %>% select(-weight), \n                    method = \"naive_bayes\",\n                    tuneLength = 15,\n                    metric = \"Kappa\",\n                    trControl = control,\n                    weights = train_knn$weight)\n\nconfusionMatrix(predict(fit_weights, test_knn),factor(test_knn$Private))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No   28   8\n       Yes  14 105\n                                         \n               Accuracy : 0.8581         \n                 95% CI : (0.793, 0.9089)\n    No Information Rate : 0.729          \n    P-Value [Acc > NIR] : 9.277e-05      \n                                         \n                  Kappa : 0.6239         \n                                         \n Mcnemar's Test P-Value : 0.2864         \n                                         \n            Sensitivity : 0.6667         \n            Specificity : 0.9292         \n         Pos Pred Value : 0.7778         \n         Neg Pred Value : 0.8824         \n             Prevalence : 0.2710         \n         Detection Rate : 0.1806         \n   Detection Prevalence : 0.2323         \n      Balanced Accuracy : 0.7979         \n                                         \n       'Positive' Class : No             \n                                         \n```\n\n\n:::\n:::\n\n\n\n> In this case, where we had relatively high predictive power to find both private and non-private colleges or universities without weights, adding weights actually disrupted this process and reduced our Kappa. Using weights is not effective in all cases - there are many private colleges and universities, and maybe many different kinds of colleges and universities we find through nearest neighbors or naive bayes, and it is unhelpful to apply weights to public colleges and universities which we already identified relatively effective. It may be worthwhile, however, to add something like a \"college or university\" feature or something of this nature to further differentiate types of institutions.\n\n# Ethics\n\n- Based on your analysis, comment on the for-profit privatization of education, perhaps through the framework advanced by this article:\n\n> [In mid-May 2018, The New York Times reported that under DeVos, the size of the team investigating abuses and fraud by for-profit colleges was reduced from about twelve members under the Obama administration to three, with their task also being scaled back to \"processing student loan forgiveness applications and looking at smaller compliance cases\".](https://en.wikipedia.org/wiki/Betsy_DeVos#Staffing)\n\n- Discuss the civic reposibilities of data scientists for:\n    - Big Data and Human-Centered Computing\n    - Democratic Institutions\n    - Education and Educational Policy\n- Provide at least one statistical measure for each, such as a RMSE, Kappa value, or ROC curve.\n\n### Big Data and Human-Centered Computing\n\n> We note that private institutions make up 73% of institutions while educating 43% of students. From our earlier work, we note that private/public predicts graduation rate, and these institutions may be weakly differentiated with regards to measures like acceptance rate, instuctor educational attainment, and cost. A data driven approach to policy may want to investigate closely whether private schools, which enjoy non-profit status and federal loan assistance, are serving students better, or worse, given their lower level of regulatory oversight.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlist(\n    sum(filter(College, Private == \"Yes\")$Enroll)/sum(College$Enroll),\n    count(filter(College, Private == \"Yes\"))/count(College)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n[1] 0.4260023\n\n[[2]]\n          n\n1 0.7271557\n```\n\n\n:::\n:::\n\n\n\n> We note that public institutions have dramatically lower cost, including for out-of-state students, in aggregate, possible due to their obligation to serve the broader public through the democratic process. A strong claim, which is partially but not fully supported here, is that democratic pressures dramatically expand access and cut costs, but this at least appears true for the specific niche of higher education.\n\n### Democratic Institutions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlist(\n    mean(filter(df_knn, Private == \"Yes\")$Cost),\n    mean(filter(df_knn, Private == \"No\")$Cost)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n[1] 18149.78\n\n[[2]]\n[1] 12793.01\n```\n\n\n:::\n:::\n\n\n\n### Education and Educational Policy\n\n> We note that private universities achieve dramatically lower student-to-faculty ratios, roughly in line with their higher costs...\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlist(\n    mean(filter(College, Private == \"Yes\")$S.F.Ratio),\n    mean(filter(College, Private == \"No\")$S.F.Ratio)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n[1] 12.94549\n\n[[2]]\n[1] 17.13915\n```\n\n\n:::\n:::\n\n\n\n> Yet we do not see lower student-to-faculty ratios necessarily corresponding to a higher graduation rate.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(Grad.Rate ~ S.F.Ratio, College)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Grad.Rate ~ S.F.Ratio, data = College)\n\nCoefficients:\n(Intercept)    S.F.Ratio  \n     84.217       -1.331  \n```\n\n\n:::\n:::\n\n\n\n### Closing Thoughts\n\n> As data scientists with a private school affiliation, we should advocate strongly for democratic process within our institution and our region.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}