{
  "hash": "56c82e583e3a3d2fa20923aefe7d3fae",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: $K$NN\nauthor: \"Calvin Deutschbein\"\ndate: \"02/10/2025\"\n\nformat: \n  html:  # You will quite likely want to change all but the last one, to taste\n    theme: superhero  \n    mainfont: monospace\n    highlight-style: github\n    title-block-banner: true\n\n---\n\n\n\n**Abstract:**\n\nThis is a technical blog post of **both** an HTML file *and* [.qmd file](https://raw.githubusercontent.com/cd-public/D505/refs/heads/master/hws/src/knn.qmd) hosted on GitHub pages.\n\n# 0. Quarto Type-setting\n\n- This document is rendered with Quarto, and configured to embed an images using the `embed-resources` option in the header.\n- If you wish to use a similar header, here's is the format specification for this document:\n\n```email\nformat: \n  html:\n    embed-resources: true\n```\n\n# 1. Setup\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsh <- suppressPackageStartupMessages\nsh(library(tidyverse))\nsh(library(caret))\nsh(library(fastDummies))\nsh(library(class))\nwine <- readRDS(gzcon(url(\"https://github.com/cd-public/D505/raw/master/dat/pinot.rds\")))\n```\n:::\n\n\n\n## 2. $K$NN Concepts\n\n> Generally we regard selection of appropriate $K$ as similar to the context of precision and recall, discussed in [earlier post](wine_of_pnw_sol.html). However with $K$NN we have a slightly different trade-off - between *bias*, where the model is overgeneralized and begins to lose nuance in niche cases, and *variance*, where a model captures noise within the data set and extrapolates it to the population. Small $K$ values tend to high variance, and larger $K$ values tend to high bias.\n\n## 3. Feature Engineering\n\n1. Create a version of the year column that is a *factor* (instead of numeric).\n2. Create dummy variables that indicate the presence of \"cherry\", \"chocolate\" and \"earth\" in the description.\n  - Take care to handle upper and lower case characters.\n3. Create 3 new features that represent the interaction between *time* and the cherry, chocolate and earth inidicators.\n4. Remove the description column from the data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwine <- wine %>%\n  mutate(fct_year = factor(year)) %>%\n  mutate(description = tolower(description)) %>%\n  mutate(cherry = str_detect(description, \"cherry\"),\n         chocolate = str_detect(description, \"chocolate\"),\n         earth = str_detect(description, \"earth\")) %>%\n  mutate(cherry_year = year*cherry,\n         chocolate_year = year*chocolate,\n         earth_year = year*earth) %>%\n  select(-description)\n```\n:::\n\n\n## 4. Preprocessing\n\n1. Preprocess the dataframe from the previous code block using BoxCox, centering and scaling of the numeric features\n2. Create dummy variables for the `year` factor column\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwine <- wine %>%\n  preProcess(method = c(\"BoxCox\", \"center\", \"scale\")) %>%\n  predict(wine) %>%\n  dummy_cols(select_columns = \"fct_year\",\n             remove_most_frequent_dummy = TRUE,\n             remove_selected_columns = TRUE)\n```\n:::\n\n\n\n\n## 5. Running $K$NN\n\n1. Split the dataframe into an 80/20 training and test set\n2. Use Caret to run a $K$NN model that uses our engineered features to predict province\n  - use 5-fold cross validated subsampling \n  - allow Caret to try 15 different values for $K$\n3. Display the confusion matrix on the test data\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsplit <- createDataPartition(wine$province, p = 0.8, list = FALSE)\ntrain <- wine[split, ]\ntest <- wine[-split, ]\nfit <- train(province ~ .,\n             data = train, \n             method = \"knn\",\n             tuneLength = 15,\n             metric = \"Kappa\",\n             trControl = trainControl(method = \"cv\", number = 5))\nconfusionMatrix(predict(fit, test),factor(test$province))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n                   Reference\nPrediction          Burgundy California Casablanca_Valley Marlborough New_York\n  Burgundy               104         23                 2           3        0\n  California              66        674                10          15       14\n  Casablanca_Valley        0          0                 0           0        0\n  Marlborough              1          0                 0           0        0\n  New_York                 1          1                 2           3        0\n  Oregon                  66         93                12          24       12\n                   Reference\nPrediction          Oregon\n  Burgundy              31\n  California           254\n  Casablanca_Valley      0\n  Marlborough            0\n  New_York               1\n  Oregon               261\n\nOverall Statistics\n                                          \n               Accuracy : 0.621           \n                 95% CI : (0.5973, 0.6444)\n    No Information Rate : 0.4728          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.3712          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: Burgundy Class: California Class: Casablanca_Valley\nSensitivity                  0.43697            0.8521                  0.00000\nSpecificity                  0.95889            0.5930                  1.00000\nPos Pred Value               0.63804            0.6525                      NaN\nNeg Pred Value               0.91126            0.8172                  0.98446\nPrevalence                   0.14226            0.4728                  0.01554\nDetection Rate               0.06216            0.4029                  0.00000\nDetection Prevalence         0.09743            0.6175                  0.00000\nBalanced Accuracy            0.69793            0.7225                  0.50000\n                     Class: Marlborough Class: New_York Class: Oregon\nSensitivity                   0.0000000        0.000000        0.4771\nSpecificity                   0.9993857        0.995143        0.8162\nPos Pred Value                0.0000000        0.000000        0.5577\nNeg Pred Value                0.9730861        0.984384        0.7627\nPrevalence                    0.0268978        0.015541        0.3270\nDetection Rate                0.0000000        0.000000        0.1560\nDetection Prevalence          0.0005977        0.004782        0.2797\nBalanced Accuracy             0.4996929        0.497571        0.6467\n```\n\n\n:::\n:::\n\n\n\n## 6. Kappa\n\nHow do we determine whether a Kappa value represents a good, bad or some other outcome?\n\n<blockquote>\nIn my training, I was taught regard Kappa values as within five \"bins\", ranging from \"not good\" to \"suspiciously good\":\n\n* [0.0,0.2): Unusable\n* [0.2,0.4): Bad\n* [0.4,0.6): Okay\n* [0.6,0.8): Excellent\n* [0.8,1.0): Suspicious, likely overfit.\n</blockquote>\n\n## 7. Improvement\n\nHow can we interpret the confusion matrix, and how can we improve in our predictions?\n\n> For me, confusion between specifical Californian and Oregonian wins both jumps out numerical and is consistent with my own understand of the world - Both California and Oregon share a border on the Pacific coast of the United States, and are likely planting in similar volcanic soil in the temperate climate zones. They likely even experience similar rainfall! To differentiate specifically these two easily confusable wins, I think I should look into dedicated features that specifical capture the essense of the *difference* between California and Oregonian wins.  Secondly, I notes that almost no wins are predicted to be in Marlborough or Casablanca - which isn't too surprising with a $K$ getting pretty close to the number of wines from those regions as a whole! I would need either more data or more advanced numerical techniques to differentiate wines within in regions from the overwhelming popular California and Oregon wines.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}