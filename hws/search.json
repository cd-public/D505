[
  {
    "objectID": "wine_of_pnw_sol.html",
    "href": "wine_of_pnw_sol.html",
    "title": "Wines of the PNW",
    "section": "",
    "text": "Abstract:\nThis is a technical blog post of both an HTML file and .qmd file hosted on GitHub pages."
  },
  {
    "objectID": "wine_of_pnw_sol.html#linear-models",
    "href": "wine_of_pnw_sol.html#linear-models",
    "title": "Wines of the PNW",
    "section": "Linear Models",
    "text": "Linear Models\nFirst run a linear regression model with log of price as the dependent variable and ‘points’ and ‘cherry’ as features (variables).\n\nm1 &lt;- lm(lprice ~ points + cherry, data=wine)\nget_regression_summaries(m1)\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared   mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.305         0.305 0.220 0.469 0.469     5846.       0     2 26584\n\n\nExplanataion:\n\nThe lm function takes a linear model over the sum of the points and the numerical (0 or 1) value of the cherry indicator variable for the wine dataset, then prints and summary.\n\n\nsqrt(mean(m1$residuals^2))\n\n[1] 0.4687657\n\n\n\nAs we predict lprice, this is the measurement in the difference of the log of price from the prediction. This numerical value is highly non-intuitive because it is a post-transform (logarithm). So in general, for around an error of .5, I would expect to be off by \\(1 - (1 / e^{.5}) \\approx .4\\) or 40%.\n\n\nlog(100) - log(60)\n\n[1] 0.5108256"
  },
  {
    "objectID": "wine_of_pnw_sol.html#interaction-models",
    "href": "wine_of_pnw_sol.html#interaction-models",
    "title": "Wines of the PNW",
    "section": "Interaction Models",
    "text": "Interaction Models\nAdd an interaction between ‘points’ and ‘cherry’.\n\nm2 &lt;- lm(lprice ~ points * cherry, data=wine)\nget_regression_table(m2)\n\n# A tibble: 4 × 7\n  term          estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept       -5.66      0.102    -55.4        0   -5.86    -5.46 \n2 points           0.102     0.001     89.0        0    0.1      0.104\n3 cherry          -1.01      0.216     -4.70       0   -1.44    -0.592\n4 points:cherry    0.013     0.002      5.26       0    0.008    0.017\n\n\n\nWithin the arguments of the lm function, ~ and * have specific meanings in accordance with the formulas API, so we note that this does not represent a naive multiplication and is much closer to common conception of a multiple regression - where the impacts of multiple independent variables, including potential interactions between these variables, are used to predict a dependent variable - in this case the log of price.\n\n\nsqrt(mean(m2$residuals^2))\n\n[1] 0.4685223\n\n\n\nWe note the RMSE is larger unaltered by switching to an interaction model, which is consistent with the interaction effect being relatively non-impactful compared to the direct effects of the variables.\n\n\nThe Interaction Variable\n\nWe start with two basic ideas: That wine with higher scores tends to fetch higher prices, and wine that reviewers say has a note of cherry tends to fetch higher prices. We want to answer the question of whether the having a cherry note is more usefully on wines with higher points, or with lower points. We find that in fact the more the points, the more value we get from a cherry note - by a small but meaningful amount. The wines tend to gain about 13% more value for each added point than the would without a cherry note.\n\n\nests &lt;- get_regression_table(m2)$estimate\nests[4] / ests[2]\n\n[1] 0.127451"
  },
  {
    "objectID": "wine_of_pnw_sol.html#applications",
    "href": "wine_of_pnw_sol.html#applications",
    "title": "Wines of the PNW",
    "section": "Applications",
    "text": "Applications\nDetermine which province (Oregon, California, or New York), does the ‘cherry’ feature in the data affect price most?\n\ncherry_impact &lt;- function(state) {\n  df &lt;- wine %&gt;% filter(province == state)\n  m &lt;- lm(lprice ~ cherry, data = df)\n  s &lt;- summary(m)\n  get_regression_table(m)\n}\nmap(c(\"Oregon\", \"California\", \"New York\"), cherry_impact)\n\n[[1]]\n# A tibble: 2 × 7\n  term      estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept    3.40      0.008     430.        0    3.39     3.42 \n2 cherry       0.303     0.016      19.2       0    0.272    0.334\n\n[[2]]\n# A tibble: 2 × 7\n  term      estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept    3.49      0.005     739.        0    3.48     3.50 \n2 cherry       0.177     0.01       18.4       0    0.158    0.196\n\n[[3]]\n# A tibble: 2 × 7\n  term      estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept    3.02      0.009    331.         0    3.00     3.04 \n2 cherry       0.173     0.018      9.77       0    0.138    0.208\n\n\n\nCherry impacts price roughly twice as strongly within Oregon wines as either California or New York wines, which are themselves quite similar."
  },
  {
    "objectID": "wine_of_pnw_sol.html#on-accuracy",
    "href": "wine_of_pnw_sol.html#on-accuracy",
    "title": "Wines of the PNW",
    "section": "On Accuracy",
    "text": "On Accuracy\nImagine a model to distinguish New York wines from those in California and Oregon. After a few days of work, you take some measurements and note: “I’ve achieved 91% accuracy on my model!”\nShould you be impressed? Why or why not?\n\nny &lt;- wine %&gt;% \n        mutate(ny=province==\"New York\") %&gt;% \n        mutate(no=FALSE) %&gt;% \n        mutate(y_hat=ny==no)\nmean(ny$y_hat)\n\n[1] 0.9110743\n\n\n\nAssuming no wines from New York yields 91% accuracy. This model is almost identical in performance to assuming the non-existence of New York and is the negation of impressive."
  },
  {
    "objectID": "wine_of_pnw_sol.html#on-ethics",
    "href": "wine_of_pnw_sol.html#on-ethics",
    "title": "Wines of the PNW",
    "section": "On Ethics",
    "text": "On Ethics\nWhy is understanding this vignette important to use machine learning in an ethical manner?\n\nIt is difficult to understand, for me, to relate ethics, “the philosophical study of moral phenomena”, to correctly calculating numerical values. I suspect ethics would occur at the site of application of these techniques to domain areas, and then using the ethical formulations specific to those domains at application time."
  },
  {
    "objectID": "wine_of_pnw_sol.html#ignorance-is-no-excuse",
    "href": "wine_of_pnw_sol.html#ignorance-is-no-excuse",
    "title": "Wines of the PNW",
    "section": "Ignorance is no excuse",
    "text": "Ignorance is no excuse\nImagine you are working on a model to predict the likelihood that an individual loses their job as the result of the changing federal policy under new presidential administrations. You have a very large dataset with many hundreds of features, but you are worried that including indicators like age, income or gender might pose some ethical problems. When you discuss these concerns with your boss, she tells you to simply drop those features from the model. Does this solve the ethical issue? Why or why not?\n\nNo - the only approach to resolve an ethical problem to is to “do the right thing” - which in this case is to take concrete actions to ensure the best possible outcomes for the individuals in question and nation-state/region as a whole. Modeling is insufficient to achieve a outcome consistent with ethical standards, which requires concrete actions with material implications. For example, one could share the results of a model on background with a reporter to apply political pressure the presidential administration to pressure the government to pursue verifiably sound industrial and economic policy."
  },
  {
    "objectID": "wine_of_pnw.html",
    "href": "wine_of_pnw.html",
    "title": "Wines of the PNW",
    "section": "",
    "text": "Abstract:\nThis is a technical blog post of both an HTML file and .qmd file hosted on GitHub pages."
  },
  {
    "objectID": "wine_of_pnw.html#linear-models",
    "href": "wine_of_pnw.html#linear-models",
    "title": "Wines of the PNW",
    "section": "Linear Models",
    "text": "Linear Models\nFirst run a linear regression model with log of price as the dependent variable and ‘points’ and ‘cherry’ as features (variables).\n\n# TODO: hint: m1 &lt;- lm(lprice ~ points + cherry)\n\nExplanataion:\n\nTODO: write your line-by-line explanation of the code here\n\n\nTODO: report and explain the RMSE"
  },
  {
    "objectID": "wine_of_pnw.html#interaction-models",
    "href": "wine_of_pnw.html#interaction-models",
    "title": "Wines of the PNW",
    "section": "Interaction Models",
    "text": "Interaction Models\nAdd an interaction between ‘points’ and ‘cherry’.\n\n# TODO: hint: Check the slides.\n\n\nTODO: write your line-by-line explanation of the code here\n\n\nTODO: report and explain the RMSE\n\n\nThe Interaction Variable\n\nTODO: interpret the coefficient on the interaction variable. Explain as you would to a non-technical manager."
  },
  {
    "objectID": "wine_of_pnw.html#applications",
    "href": "wine_of_pnw.html#applications",
    "title": "Wines of the PNW",
    "section": "Applications",
    "text": "Applications\nDetermine which province (Oregon, California, or New York), does the ‘cherry’ feature in the data affect price most?\n\n# TODO: \n\n\nTODO: write your line-by-line explanation of the code here, and explain your answer."
  },
  {
    "objectID": "wine_of_pnw.html#on-accuracy",
    "href": "wine_of_pnw.html#on-accuracy",
    "title": "Wines of the PNW",
    "section": "On Accuracy",
    "text": "On Accuracy\nImagine a model to distinguish New York wines from those in California and Oregon. After a few days of work, you take some measurements and note: “I’ve achieved 91% accuracy on my model!”\nShould you be impressed? Why or why not?\n\n# TODO: Use simple descriptive statistics from the data to justify your answer.\n\n\nTODO: describe your reasoning here"
  },
  {
    "objectID": "wine_of_pnw.html#on-ethics",
    "href": "wine_of_pnw.html#on-ethics",
    "title": "Wines of the PNW",
    "section": "On Ethics",
    "text": "On Ethics\nWhy is understanding this vignette important to use machine learning in an ethical manner?\n\nTODO: describe your reasoning here"
  },
  {
    "objectID": "wine_of_pnw.html#ignorance-is-no-excuse",
    "href": "wine_of_pnw.html#ignorance-is-no-excuse",
    "title": "Wines of the PNW",
    "section": "Ignorance is no excuse",
    "text": "Ignorance is no excuse\nImagine you are working on a model to predict the likelihood that an individual loses their job as the result of the changing federal policy under new presidential administrations. You have a very large dataset with many hundreds of features, but you are worried that including indicators like age, income or gender might pose some ethical problems. When you discuss these concerns with your boss, she tells you to simply drop those features from the model. Does this solve the ethical issue? Why or why not?\n\nTODO: describe your reasoning here"
  },
  {
    "objectID": "wine_features_py.html",
    "href": "wine_features_py.html",
    "title": "Wine Features",
    "section": "",
    "text": "Abstract:\nThis is a technical blog post of both an HTML file and .qmd file hosted on GitHub pages.\n\nSetup\n\nChange the author of this QMD file to be yourself and delete this line.\nModify if necessary the below code so that you can successfully load wine.rds then delete this line.\nIn the space provided after the Python chunk, explain what thecode is doing (line by line) then delete this line.\nGet your GitHub Pages ready.\n\nSet Up Python:\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.utils import resample\n\nSet Up R Compatability:\n\nimport pyreadr\n\nStep Up Dataframe:\n\nurl = 'https://cd-public.github.io/D505/dat/'\nrds = 'wine.rds'\npyreadr.download_file(url + rds, rds) \nwine = pyreadr.read_r(rds)[None]      \n\nExplanataion:\n\nTODO: write your line-by-line explanation of the code here\n\n\n\nFeature Engineering\nWe begin by engineering an number of features.\n\nCreate a total of 10 features (including points).\nRemove all rows with a missing value.\nEnsure only log(price) and engineering features are the only columns that remain in the wino dataframe.\n\n\nwine['lprice'] = wine['price'].apply(lambda x: np.log(x))\n\n\n\nSkelarn\nWe now use a train/test split to evaluate the features.\n\nUse the Sklearn library to partition the wino dataframe into an 75/25 split.\nRun a linear regression with bootstrap resampling.\nReport RMSE on the test partition of the data.\n\n\n# TODO: hint: Check the slides.\n\n\n\nVariable selection\nWe now graph the importance of your 10 features.\n\n# TODO: hint: Check the slides."
  },
  {
    "objectID": "knn.html",
    "href": "knn.html",
    "title": "\\(K\\)NN",
    "section": "",
    "text": "Abstract:\nThis is a technical blog post of both an HTML file and .qmd file hosted on GitHub pages."
  },
  {
    "objectID": "knn.html#knn-concepts",
    "href": "knn.html#knn-concepts",
    "title": "\\(K\\)NN",
    "section": "2. \\(K\\)NN Concepts",
    "text": "2. \\(K\\)NN Concepts\n\nTODO: Explain how the choice of K affects the quality of your prediction when using a \\(K\\) Nearest Neighbors algorithm."
  },
  {
    "objectID": "knn.html#feature-engineering",
    "href": "knn.html#feature-engineering",
    "title": "\\(K\\)NN",
    "section": "3. Feature Engineering",
    "text": "3. Feature Engineering\n\nCreate a version of the year column that is a factor (instead of numeric).\nCreate dummy variables that indicate the presence of “cherry”, “chocolate” and “earth” in the description.\n\n\nTake care to handle upper and lower case characters.\n\n\nCreate 3 new features that represent the interaction between time and the cherry, chocolate and earth inidicators.\nRemove the description column from the data.\n\n\n# your code here"
  },
  {
    "objectID": "knn.html#preprocessing",
    "href": "knn.html#preprocessing",
    "title": "\\(K\\)NN",
    "section": "4. Preprocessing",
    "text": "4. Preprocessing\n\nPreprocess the dataframe from the previous code block using BoxCox, centering and scaling of the numeric features\nCreate dummy variables for the year factor column\n\n\n# your code here"
  },
  {
    "objectID": "knn.html#running-knn",
    "href": "knn.html#running-knn",
    "title": "\\(K\\)NN",
    "section": "5. Running \\(K\\)NN",
    "text": "5. Running \\(K\\)NN\n\nSplit the dataframe into an 80/20 training and test set\nUse Caret to run a \\(K\\)NN model that uses our engineered features to predict province\n\n\nuse 5-fold cross validated subsampling\nallow Caret to try 15 different values for \\(K\\)\n\n\nDisplay the confusion matrix on the test data"
  },
  {
    "objectID": "knn.html#kappa",
    "href": "knn.html#kappa",
    "title": "\\(K\\)NN",
    "section": "6. Kappa",
    "text": "6. Kappa\nHow do we determine whether a Kappa value represents a good, bad or some other outcome?\n\nTODO: Explain"
  },
  {
    "objectID": "knn.html#improvement",
    "href": "knn.html#improvement",
    "title": "\\(K\\)NN",
    "section": "7. Improvement",
    "text": "7. Improvement\nHow can we interpret the confusion matrix, and how can we improve in our predictions?\n\nTODO: Explain"
  },
  {
    "objectID": "classify.html",
    "href": "classify.html",
    "title": "Classification",
    "section": "",
    "text": "Abstract:\nThis is a technical blog post of both an HTML file and .qmd file hosted on GitHub pages.\n\n0. Quarto Type-setting\n\nThis document is rendered with Quarto, and configured to embed an images using the embed-resources option in the header.\nIf you wish to use a similar header, here’s is the format specification for this document:\n\nformat: \n  html:\n    embed-resources: true\n\n\n1. Setup\nStep Up Code:\n\nsh &lt;- suppressPackageStartupMessages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(caret)\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nlibrary(naivebayes)\n\nnaivebayes 1.0.0 loaded\nFor more information please visit: \nhttps://majkamichal.github.io/naivebayes/\n\nwine &lt;- readRDS(gzcon(url(\"https://github.com/cd-public/D505/raw/master/dat/pinot.rds\")))\n\n\n\n2. Logistic Concepts\nWhy do we call it Logistic Regression even though we are using the technique for classification?\n\nTODO: Explain.\n\n\n\n3. Modeling\nWe train a logistic regression algorithm to classify a whether a wine comes from Marlborough using:\n\nAn 80-20 train-test split.\nThree features engineered from the description\n5-fold cross validation.\n\nWe report Kappa after using the model to predict provinces in the holdout sample.\n\n# TODO\n\n\n\n4. Weighting\nRerun the above model with a 15 to 1 weight on Marlborough\n\n# TODO\n\n\n\n5. ROC Curves\nWe can display an ROC for the model to explain your model’s quality.\n\n# You can find a tutorial on ROC curves here: https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5\n\n\nTODO: Explain."
  },
  {
    "objectID": "cond.html",
    "href": "cond.html",
    "title": "Conditional Probability",
    "section": "",
    "text": "Abstract:\nThis is a technical blog post of both an HTML file and .qmd file hosted on GitHub pages.\n\n0. Quarto Type-setting\n\nThis document is rendered with Quarto, and configured to embed an images using the embed-resources option in the header.\nIf you wish to use a similar header, here’s is the format specification for this document:\n\nformat: \n  html:\n    embed-resources: true\n\n\n1. Setup\nStep Up Code:\n\nsh &lt;- suppressPackageStartupMessages\nsh(library(tidyverse))\nsh(library(caret))\nwine &lt;- readRDS(gzcon(url(\"https://github.com/cd-public/D505/raw/master/dat/pinot.rds\")))\n\n\n\n2. Conditional Probability\nCalculate the probability that a Pinot comes from Burgundy given it has the word ‘fruit’ in the description.\n\\[\nP({\\rm Burgundy}~|~{\\rm Fruit})\n\\]\n\n# TODO\n\n\n\n3. Naive Bayes Algorithm\nWe train a naive bayes algorithm to classify a wine’s province using: 1. An 80-20 train-test split. 2. Three features engineered from the description 3. 5-fold cross validation.\nWe report Kappa after using the model to predict provinces in the holdout sample.\n\n# TODO\n\n\n\n4. Frequency Differences\nWe find the three words that most distinguish New York Pinots from all other Pinots.\n\n# TODO\n\n\n\n5. Extension\n\nEither do this as a bonus problem, or delete this section.\n\nCalculate the variance of the logged word-frequency distributions for each province."
  },
  {
    "objectID": "wine_features.html",
    "href": "wine_features.html",
    "title": "Wine Features",
    "section": "",
    "text": "Abstract:\nThis is a technical blog post of both an HTML file and .qmd file hosted on GitHub pages.\n\nSetup\n\nChange the author of this RMD file to be yourself and delete this line.\nModify if necessary the below code so that you can successfully load wine.rds then delete this line.\nIn the space provided after the R chunk, explain what thecode is doing (line by line) then delete this line.\nGet your GitHub Pages ready.\n\nStep Up Code:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(caret)\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nlibrary(fastDummies)\nwine &lt;- readRDS(gzcon(url(\"https://github.com/cd-public/D505/raw/master/dat/wine.rds\")))\n\nExplanataion:\n\nTODO: write your line-by-line explanation of the code here\n\n\n\nFeature Engineering\nWe begin by engineering an number of features.\n\nCreate a total of 10 features (including points).\nRemove all rows with a missing value.\nEnsure only log(price) and engineering features are the only columns that remain in the wino dataframe.\n\n\nwino &lt;- wine %&gt;% \n  mutate(lprice=log(price))\n  # engineer features here\n\n\n\nCaret\nWe now use a train/test split to evaluate the features.\n\nUse the Caret library to partition the wino dataframe into an 80/20 split.\nRun a linear regression with bootstrap resampling.\nReport RMSE on the test partition of the data.\n\n\n# TODO: hint: Check the slides.\n\n\n\nVariable selection\nWe now graph the importance of your 10 features.\n\n# TODO: hint: Check the slides."
  },
  {
    "objectID": "wine_features_sol.html",
    "href": "wine_features_sol.html",
    "title": "Wine Features",
    "section": "",
    "text": "Abstract:\nThis is a technical blog post of both an HTML file and .qmd file hosted on GitHub pages.\n\nSetup\nStep Up Code:\n\nsh &lt;- suppressPackageStartupMessages\nsh(library(tidyverse))\nsh(library(caret))\nsh(library(fastDummies))\nwine &lt;- readRDS(gzcon(url(\"https://github.com/cd-public/D505/raw/master/dat/wine.rds\")))\n\nExplanataion:\n\nWe will proceed as follows: - Suppress library load warnings, as we addressed them prior to publication. - Use Tidy data sets via the tidyverse package - Perform classification and regression via the caret package - Engineer features via the fastDummies package - Load in the wine.rds dataframe, hosted publicly on GitHub.\n\n\n\nFeature Engineering\nWe begin by engineering an number of features.\n\nwino &lt;- wine %&gt;% \n  mutate(lprice=log(price), description = tolower(description)) %&gt;%\n  select(lprice, description)\n\n\nCreate a total of 10 features (including points).\n\n\nnotes &lt;- c(\"smoke\", \"spice\", \"pepper\", \"grass\", \"tannic\", \"crisp\", \"acidic\", \"bright\", \"smooth\")\nfor (note in notes) {\n  wino &lt;- wino %&gt;%\n    mutate(!!sym(note) := str_detect(description, note))\n}\nhead(wino)\n\n# A tibble: 6 × 11\n  lprice description  smoke spice pepper grass tannic crisp acidic bright smooth\n   &lt;dbl&gt; &lt;chr&gt;        &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt;  &lt;lgl&gt; &lt;lgl&gt;  &lt;lgl&gt; &lt;lgl&gt;  &lt;lgl&gt;  &lt;lgl&gt; \n1   2.71 this is rip… FALSE FALSE FALSE  FALSE FALSE  FALSE FALSE  FALSE  TRUE  \n2   2.64 tart and sn… FALSE FALSE FALSE  FALSE FALSE  TRUE  FALSE  FALSE  FALSE \n3   2.56 pineapple r… FALSE FALSE FALSE  FALSE FALSE  FALSE FALSE  FALSE  FALSE \n4   4.17 much like t… FALSE FALSE FALSE  FALSE TRUE   FALSE FALSE  FALSE  FALSE \n5   2.71 blackberry … FALSE FALSE FALSE  FALSE FALSE  FALSE FALSE  FALSE  FALSE \n6   2.77 here's a br… FALSE FALSE TRUE   FALSE FALSE  FALSE FALSE  TRUE   FALSE \n\n\n\nRemove all rows with a missing value.\nEnsure only log(price) and engineering features are the only columns that remain in the wino dataframe.\n\n\nwino &lt;- wino %&gt;% select(-description) %&gt;% drop_na(.)\n\n\n\nCaret\nWe now use a train/test split to evaluate the features.\n\nUse the Caret library to partition the wino dataframe into an 80/20 split.\n\n\nset.seed(505)  # For reproducibility\ntrainIndex &lt;- createDataPartition(wino$lprice, p = 0.8, list = FALSE)\ntrainData &lt;- wino[trainIndex, ]\ntestData &lt;- wino[-trainIndex, ]\n\n\nRun a linear regression with bootstrap resampling.\n\n\noptions(warn=-1)\nmodel &lt;- train(lprice ~ ., data = trainData, method = \"lm\", trControl = trainControl(method = \"boot\", number = 5))\noptions(warn=0)\n\n\nReport RMSE on the test partition of the data.\n\n\nsqrt(mean((predict(model, newdata = testData) - testData$lprice)^2))\n\n[1] 0.6475555\n\n\n\n\nVariable selection\nWe now graph the importance of our 10 features.\n\nplot(varImp(model))"
  },
  {
    "objectID": "wine_of_pnw_py.html",
    "href": "wine_of_pnw_py.html",
    "title": "Wines of the PNW",
    "section": "",
    "text": "Abstract:\nThis is a technical blog post of both an HTML file and .qmd file hosted on GitHub pages."
  },
  {
    "objectID": "wine_of_pnw_py.html#linear-models",
    "href": "wine_of_pnw_py.html#linear-models",
    "title": "Wines of the PNW",
    "section": "Linear Models",
    "text": "Linear Models\nFirst run a linear regression model with log of price as the dependent variable and ‘points’ and ‘cherry’ as features (variables).\n# TODO: hint: m1 &lt;- lm(lprice ~ points + cherry)\nExplanataion:\n\nTODO: write your line-by-line explanation of the code here\n\n\nTODO: report and explain the RMSE"
  },
  {
    "objectID": "wine_of_pnw_py.html#interaction-models",
    "href": "wine_of_pnw_py.html#interaction-models",
    "title": "Wines of the PNW",
    "section": "Interaction Models",
    "text": "Interaction Models\nAdd an interaction between ‘points’ and ‘cherry’.\n# TODO: hint: Check the slides.\n\nTODO: write your line-by-line explanation of the code here\n\n\nTODO: report and explain the RMSE\n\n\nThe Interaction Variable\n\nTODO: interpret the coefficient on the interaction variable. Explain as you would to a non-technical manager."
  },
  {
    "objectID": "wine_of_pnw_py.html#applications",
    "href": "wine_of_pnw_py.html#applications",
    "title": "Wines of the PNW",
    "section": "Applications",
    "text": "Applications\nDetermine which province (Oregon, California, or New York), does the ‘cherry’ feature in the data affect price most?\n# TODO: \n\nTODO: write your line-by-line explanation of the code here, and explain your answer."
  },
  {
    "objectID": "wine_of_pnw_py.html#on-accuracy",
    "href": "wine_of_pnw_py.html#on-accuracy",
    "title": "Wines of the PNW",
    "section": "On Accuracy",
    "text": "On Accuracy\nImagine a model to distinguish New York wines from those in California and Oregon. After a few days of work, you take some measurements and note: “I’ve achieved 91% accuracy on my model!”\nShould you be impressed? Why or why not?\n# TODO: Use simple descriptive statistics from the data to justify your answer.\n\nTODO: describe your reasoning here"
  },
  {
    "objectID": "wine_of_pnw_py.html#on-ethics",
    "href": "wine_of_pnw_py.html#on-ethics",
    "title": "Wines of the PNW",
    "section": "On Ethics",
    "text": "On Ethics\nWhy is understanding this vignette important to use machine learning in an ethical manner?\n\nTODO: describe your reasoning here"
  },
  {
    "objectID": "wine_of_pnw_py.html#ignorance-is-no-excuse",
    "href": "wine_of_pnw_py.html#ignorance-is-no-excuse",
    "title": "Wines of the PNW",
    "section": "Ignorance is no excuse",
    "text": "Ignorance is no excuse\nImagine you are working on a model to predict the likelihood that an individual loses their job as the result of the changing federal policy under new presidential administrations. You have a very large dataset with many hundreds of features, but you are worried that including indicators like age, income or gender might pose some ethical problems. When you discuss these concerns with your boss, she tells you to simply drop those features from the model. Does this solve the ethical issue? Why or why not?\n\nTODO: describe your reasoning here"
  },
  {
    "objectID": "knn_sol.html",
    "href": "knn_sol.html",
    "title": "\\(K\\)NN",
    "section": "",
    "text": "Abstract:\nThis is a technical blog post of both an HTML file and .qmd file hosted on GitHub pages."
  },
  {
    "objectID": "knn_sol.html#knn-concepts",
    "href": "knn_sol.html#knn-concepts",
    "title": "\\(K\\)NN",
    "section": "2. \\(K\\)NN Concepts",
    "text": "2. \\(K\\)NN Concepts\n\nGenerally we regard selection of appropriate \\(K\\) as similar to the context of precision and recall, discussed in earlier post. However with \\(K\\)NN we have a slightly different trade-off - between bias, where the model is overgeneralized and begins to lose nuance in niche cases, and variance, where a model captures noise within the data set and extrapolates it to the population. Small \\(K\\) values tend to high variance, and larger \\(K\\) values tend to high bias."
  },
  {
    "objectID": "knn_sol.html#feature-engineering",
    "href": "knn_sol.html#feature-engineering",
    "title": "\\(K\\)NN",
    "section": "3. Feature Engineering",
    "text": "3. Feature Engineering\n\nCreate a version of the year column that is a factor (instead of numeric).\nCreate dummy variables that indicate the presence of “cherry”, “chocolate” and “earth” in the description.\n\n\nTake care to handle upper and lower case characters.\n\n\nCreate 3 new features that represent the interaction between time and the cherry, chocolate and earth inidicators.\nRemove the description column from the data.\n\n\nwine &lt;- wine %&gt;%\n  mutate(fct_year = factor(year)) %&gt;%\n  mutate(description = tolower(description)) %&gt;%\n  mutate(cherry = str_detect(description, \"cherry\"),\n         chocolate = str_detect(description, \"chocolate\"),\n         earth = str_detect(description, \"earth\")) %&gt;%\n  mutate(cherry_year = year*cherry,\n         chocolate_year = year*chocolate,\n         earth_year = year*earth) %&gt;%\n  select(-description)"
  },
  {
    "objectID": "knn_sol.html#preprocessing",
    "href": "knn_sol.html#preprocessing",
    "title": "\\(K\\)NN",
    "section": "4. Preprocessing",
    "text": "4. Preprocessing\n\nPreprocess the dataframe from the previous code block using BoxCox, centering and scaling of the numeric features\nCreate dummy variables for the year factor column\n\n\nwine &lt;- wine %&gt;%\n  preProcess(method = c(\"BoxCox\", \"center\", \"scale\")) %&gt;%\n  predict(wine) %&gt;%\n  dummy_cols(select_columns = \"fct_year\",\n             remove_most_frequent_dummy = TRUE,\n             remove_selected_columns = TRUE)"
  },
  {
    "objectID": "knn_sol.html#running-knn",
    "href": "knn_sol.html#running-knn",
    "title": "\\(K\\)NN",
    "section": "5. Running \\(K\\)NN",
    "text": "5. Running \\(K\\)NN\n\nSplit the dataframe into an 80/20 training and test set\nUse Caret to run a \\(K\\)NN model that uses our engineered features to predict province\n\n\nuse 5-fold cross validated subsampling\nallow Caret to try 15 different values for \\(K\\)\n\n\nDisplay the confusion matrix on the test data\n\n\nsplit &lt;- createDataPartition(wine$province, p = 0.8, list = FALSE)\ntrain &lt;- wine[split, ]\ntest &lt;- wine[-split, ]\nfit &lt;- train(province ~ .,\n             data = train, \n             method = \"knn\",\n             tuneLength = 15,\n             metric = \"Kappa\",\n             trControl = trainControl(method = \"cv\", number = 5))\nconfusionMatrix(predict(fit, test),factor(test$province))\n\nConfusion Matrix and Statistics\n\n                   Reference\nPrediction          Burgundy California Casablanca_Valley Marlborough New_York\n  Burgundy               109         23                 3           6        0\n  California              59        675                13          17       15\n  Casablanca_Valley        0          1                 0           0        0\n  Marlborough              0          0                 0           0        0\n  New_York                 0          1                 1           0        2\n  Oregon                  70         91                 9          22        9\n                   Reference\nPrediction          Oregon\n  Burgundy              26\n  California           256\n  Casablanca_Valley      0\n  Marlborough            1\n  New_York               3\n  Oregon               261\n\nOverall Statistics\n                                          \n               Accuracy : 0.6258          \n                 95% CI : (0.6021, 0.6491)\n    No Information Rate : 0.4728          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.3794          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: Burgundy Class: California Class: Casablanca_Valley\nSensitivity                  0.45798            0.8534                0.0000000\nSpecificity                  0.95958            0.5918                0.9993928\nPos Pred Value               0.65269            0.6522                0.0000000\nNeg Pred Value               0.91434            0.8182                0.9844498\nPrevalence                   0.14226            0.4728                0.0155409\nDetection Rate               0.06515            0.4035                0.0000000\nDetection Prevalence         0.09982            0.6186                0.0005977\nBalanced Accuracy            0.70878            0.7226                0.4996964\n                     Class: Marlborough Class: New_York Class: Oregon\nSensitivity                   0.0000000        0.076923        0.4771\nSpecificity                   0.9993857        0.996964        0.8215\nPos Pred Value                0.0000000        0.285714        0.5649\nNeg Pred Value                0.9730861        0.985594        0.7638\nPrevalence                    0.0268978        0.015541        0.3270\nDetection Rate                0.0000000        0.001195        0.1560\nDetection Prevalence          0.0005977        0.004184        0.2762\nBalanced Accuracy             0.4996929        0.536944        0.6493"
  },
  {
    "objectID": "knn_sol.html#kappa",
    "href": "knn_sol.html#kappa",
    "title": "\\(K\\)NN",
    "section": "6. Kappa",
    "text": "6. Kappa\nHow do we determine whether a Kappa value represents a good, bad or some other outcome?\n\nIn my training, I was taught regard Kappa values as within five “bins”, ranging from “not good” to “suspiciously good”:\n\n[0.0,0.2): Unusable\n[0.2,0.4): Bad\n[0.4,0.6): Okay\n[0.6,0.8): Excellent\n[0.8,1.0): Suspicious, likely overfit."
  },
  {
    "objectID": "knn_sol.html#improvement",
    "href": "knn_sol.html#improvement",
    "title": "\\(K\\)NN",
    "section": "7. Improvement",
    "text": "7. Improvement\nHow can we interpret the confusion matrix, and how can we improve in our predictions?\n\nFor me, confusion between specifical Californian and Oregonian wins both jumps out numerical and is consistent with my own understand of the world - Both California and Oregon share a border on the Pacific coast of the United States, and are likely planting in similar volcanic soil in the temperate climate zones. They likely even experience similar rainfall! To differentiate specifically these two easily confusable wins, I think I should look into dedicated features that specifical capture the essense of the difference between California and Oregonian wins. Secondly, I notes that almost no wins are predicted to be in Marlborough or Casablanca - which isn’t too surprising with a \\(K\\) getting pretty close to the number of wines from those regions as a whole! I would need either more data or more advanced numerical techniques to differentiate wines within in regions from the overwhelming popular California and Oregon wines."
  },
  {
    "objectID": "prac_rubric.html",
    "href": "prac_rubric.html",
    "title": "Characterizing Colleges",
    "section": "",
    "text": "Multiple Regression\n\nThe student:\n\nMinimally computes the RMSE of more than one model\nDiscusses how to RMSE relates to relates to the data set in human readable language\nDiscusses one claim that may be made about the data based on these multiple RMSEs.\nSupports any numerical claims made with code.\n\n\n\n\nFeature Engineering\n\nThe student:\n\nMinimally adds 6 new features.\nRemoves all non-engineered features.\nCalculates an RMSE and compares it to the earlier RMSEs.\nComments on independence of features or achieves independence through engineering.\n\n\n\n\nNaive Classification\n\nThe student:\n\nMinimally provides \\(K\\)-NN and/or Naive Bayes specialized features.\nReports Kappa values for more than one method.\nProvides a narrative for the difference in Kappa values.\n\n\n\n\nImproved Classification\n\nThe student:\n\nCalculates relative frequencies of some categories.\nDerives weights from these categories.\nTrains and new model with novel Kappa value.\nProvides a narrative for the difference in Kappa values.\n\n\n\n\nEthics\n\nThe student:\n\nWrites at a college level.\nDoes not contradict code samples."
  },
  {
    "objectID": "prac_sol.html",
    "href": "prac_sol.html",
    "title": "Characterizing Colleges",
    "section": "",
    "text": "Setup\n\nSetup\n\n\nsh &lt;- suppressPackageStartupMessages\nsh(library(tidyverse))\nsh(library(caret))\nsh(library(class))\nsh(library(ISLR)) # for the \"College\" dataframe\n\n\n\nDataframe\n\nWe use the College dataframe.\n\n\nhead(College)\n\n                             Private Apps Accept Enroll Top10perc Top25perc\nAbilene Christian University     Yes 1660   1232    721        23        52\nAdelphi University               Yes 2186   1924    512        16        29\nAdrian College                   Yes 1428   1097    336        22        50\nAgnes Scott College              Yes  417    349    137        60        89\nAlaska Pacific University        Yes  193    146     55        16        44\nAlbertson College                Yes  587    479    158        38        62\n                             F.Undergrad P.Undergrad Outstate Room.Board Books\nAbilene Christian University        2885         537     7440       3300   450\nAdelphi University                  2683        1227    12280       6450   750\nAdrian College                      1036          99    11250       3750   400\nAgnes Scott College                  510          63    12960       5450   450\nAlaska Pacific University            249         869     7560       4120   800\nAlbertson College                    678          41    13500       3335   500\n                             Personal PhD Terminal S.F.Ratio perc.alumni Expend\nAbilene Christian University     2200  70       78      18.1          12   7041\nAdelphi University               1500  29       30      12.2          16  10527\nAdrian College                   1165  53       66      12.9          30   8735\nAgnes Scott College               875  92       97       7.7          37  19016\nAlaska Pacific University        1500  76       72      11.9           2  10922\nAlbertson College                 675  67       73       9.4          11   9727\n                             Grad.Rate\nAbilene Christian University        60\nAdelphi University                  56\nAdrian College                      54\nAgnes Scott College                 59\nAlaska Pacific University           15\nAlbertson College                   55\n\n\n\nStates the ISLR textbook:\n\n\n\n\nName\nDescription\n\n\n\n\nPrivate\nPublic/private indicator\n\n\nApps\nNumber of applications received\n\n\nAccept\nNumber of applicants accepted\n\n\nEnroll\nNumber of new students enrolled\n\n\nTop10perc\nNew students from top 10 % of high school class\n\n\nTop25perc\nNew students from top 25 % of high school class\n\n\nF.Undergrad\nNumber of full-time undergraduates\n\n\nP.Undergrad\nNumber of part-time undergraduates\n\n\nOutstate\nOut-of-state tuition\n\n\nRoom.Board\nRoom and board costs\n\n\nBooks\nEstimated book costs\n\n\nPersonal\nEstimated personal spending\n\n\nPhD\nPercent of faculty with Ph.D.’s\n\n\nTerminal\nPercent of faculty with terminal degree\n\n\nS.F.Ratio\nStudent/faculty ratio\n\n\nperc.alumni\nPercent of alumni who donate\n\n\nExpend\nInstructional expenditure per student\n\n\nGrad.Rate\nGraduation rate\n\n\n\n\n\nMultiple Regression\n\nRun a linear regression model with Grad.Rate as the dependent variable and PhD and Expend as features (variables).\n\nRegard PhD and Expend as two forms of investment in education - in training for instructors, and in resources for students.\n\nCompute and comment on the RMSE.\n\n\nm1 = lm(Grad.Rate ~ PhD, College)\nm2 = lm(Grad.Rate ~ Expend, College)\nm3 = lm(Grad.Rate ~ PhD + Expend, College)\nm4 = lm(Grad.Rate ~ PhD * Expend, College)\nm5 = lm(Grad.Rate ~ ., College)\n\nget_rmse &lt;- function(m) {\n    pred &lt;- predict(m, newdata = College)\n    sqrt(mean((College$Grad.Rate - pred)^2))\n}\n\nunlist(lapply(list(m1, m2, m3, m4, m5), get_rmse))\n\n[1] 16.34849 15.80482 15.59084 15.57864 12.59685\n\n\n\nWhile neither PhD attainment of teaching faculty nor the institutional expenditure per student are extraordinarily accurate - an error of ~16 with regards to a percentage intuitively feels high - nothing in the data set necessarily naively predicts graduation rate so well, and the interaction of these features predicts better than either feature individually or even both features without an interaction term. So, I expect both educational attainment of faculty and expenditure per student are important parts of delivering a high quality education, and each improves the other’s value, though I would need to separately ensure the coefficients are positive to validate this claim:\n\n\nm4\n\n\nCall:\nlm(formula = Grad.Rate ~ PhD * Expend, data = College)\n\nCoefficients:\n(Intercept)          PhD       Expend   PhD:Expend  \n  4.899e+01    1.002e-01    2.757e-04    8.829e-06  \n\n\n\n\nFeature Engineering\n\nCreate 10 total features. Consider:\n\nAttributes of the student body.\n\nFor example, an acceptance rate, or a percentages of students in other categories vs. accepted/enrolled.\n\nCosts of the university.\nSome other category, such as related to success, alumni, or faculty.\n\nRemove all rows with a missing value.\nEnsure only Grad.Rate and the engineered features remain.\nCompute and comment on the RMSE.\n\n\ndf_all &lt;- College %&gt;%\n            mutate(AcceptRate=Accept/Apps) %&gt;%\n            mutate(EnrollRate=Enroll/Accept) %&gt;%\n            mutate(Top10Rate=Top10perc/Enroll) %&gt;%\n            mutate(Top10Rate=Top25perc/Enroll) %&gt;%\n            mutate(Cost=Outstate+Room.Board+Books+Personal) %&gt;%\n            mutate(MS=Terminal-PhD) %&gt;% \n            mutate(AppToAlum = AcceptRate * EnrollRate * perc.alumni)\n        \ndf_feat &lt;- df_all %&gt;%\n             select(-Private,-Apps,-Accept,-Enroll,-Top10perc,-Top25perc,-F.Undergrad,-P.Undergrad,-Outstate,-Room.Board) %&gt;%\n             select(-Books,-Personal,-PhD,-Terminal,-S.F.Ratio,-perc.alumni,-Expend)\n\n\nsqrt(mean((df_all$Grad.Rate - predict(lm(formula = Grad.Rate ~ ., data = df_all), newdata = df_all))^2))\n\n[1] 12.36756\n\nsqrt(mean((df_feat$Grad.Rate - predict(lm(formula = Grad.Rate ~ ., data = df_feat), newdata = df_feat))^2))\n\n[1] 13.4235\n\n\n\nAdding my novel features did marginally improve predictive power, but removing the initial, provided features, worsened performances versus just using the original data set. I should be more intentional and systematic about: (1) including all elements of the original data frame which may be relevant in my engineered features, and (2) ensuring independence between retained features to ensure that the assumptions of linear models are satisfied.\n\n\n\nClassification Methods\n\nUse either of \\(K\\)-NN or Naive Bayes to predict whether a college is Private.\nExplain your choice of technique.\nReport on your Kappa value.\n\n\ncontrol = trainControl(method = \"cv\", number = 5)\n\n\nWe try a \\(K\\)-NN over a few features.\n\n\ndf_knn = df_all %&gt;% \n           select(Private, AcceptRate, PhD, Cost, Top10perc)\n\nsplit &lt;- createDataPartition(df_knn$Private, p = 0.8, list = FALSE)\ntrain_knn &lt;- df_knn[split, ]\ntest_knn &lt;- df_knn[-split, ]\n\nfit_knn = train(Private ~ .,\n                data = train_knn, \n                method = \"knn\",\n                tuneLength = 15,\n                metric = \"Kappa\",\n                trControl = control)\n\nconfusionMatrix(predict(fit_knn, test_knn),factor(test_knn$Private))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No   27   8\n       Yes  15 105\n                                          \n               Accuracy : 0.8516          \n                 95% CI : (0.7857, 0.9036)\n    No Information Rate : 0.729           \n    P-Value [Acc &gt; NIR] : 0.0002073       \n                                          \n                  Kappa : 0.6037          \n                                          \n Mcnemar's Test P-Value : 0.2109029       \n                                          \n            Sensitivity : 0.6429          \n            Specificity : 0.9292          \n         Pos Pred Value : 0.7714          \n         Neg Pred Value : 0.8750          \n             Prevalence : 0.2710          \n         Detection Rate : 0.1742          \n   Detection Prevalence : 0.2258          \n      Balanced Accuracy : 0.7860          \n                                          \n       'Positive' Class : No              \n                                          \n\n\n\nWe bin a few features and try Naive.\n\n\ndf_nb = df_knn %&gt;% \n          mutate(HighAccept=AcceptRate &gt; mean(df_knn$AcceptRate)) %&gt;%\n          mutate(HighPhD=PhD &gt; mean(df_knn$PhD)) %&gt;%\n          mutate(HighCost=Cost &gt; mean(df_knn$Cost)) %&gt;%\n          mutate(HighTop10=Top10perc &gt; mean(df_knn$Top10perc)) %&gt;%\n          select(-AcceptRate, -PhD, -Cost, -Top10perc)\n\nsplit &lt;- createDataPartition(df_nb$Private, p = 0.8, list = FALSE)\ntrain_nb &lt;- df_nb[split, ]\ntest_nb &lt;- df_nb[-split, ]\n\nfit_nb = train(Private ~ .,\n               data = train_nb, \n               method = \"naive_bayes\",\n               tuneLength = 15,\n               metric = \"Kappa\",\n               trControl = control)\n\nconfusionMatrix(predict(fit_nb, test_nb),factor(test_nb$Private))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No   24   9\n       Yes  18 104\n                                         \n               Accuracy : 0.8258         \n                 95% CI : (0.7568, 0.882)\n    No Information Rate : 0.729          \n    P-Value [Acc &gt; NIR] : 0.00325        \n                                         \n                  Kappa : 0.5273         \n                                         \n Mcnemar's Test P-Value : 0.12366        \n                                         \n            Sensitivity : 0.5714         \n            Specificity : 0.9204         \n         Pos Pred Value : 0.7273         \n         Neg Pred Value : 0.8525         \n             Prevalence : 0.2710         \n         Detection Rate : 0.1548         \n   Detection Prevalence : 0.2129         \n      Balanced Accuracy : 0.7459         \n                                         \n       'Positive' Class : No             \n                                         \n\n\n\nI expect public schools, due to the forms of regulatory oversight, they experience, to be clustered fairly neatly around certain admission, instruction preparation, and acceptance metrics, with of course a few exceptions for e.g. the “Public Ivies” like UC Berkley and UT Austin. This is reflected in drastically more accurate Kappa values for Naive Bayes, which are reflective, to me, of public universities as a rule being on the same side of the means across various metrics as other public universities.\n\n\n\nClassification Techniques\n\nPredict whether a college is Private.\nUse model weights.\nDisplay and comment on an ROC curve.\n\n\ncounts &lt;- table(df_knn$Private)\ncount_y &lt;- counts[\"Yes\"]\ncount_n &lt;- counts[\"No\"]\nweigh_y &lt;- max(count_y,count_n)/count_y\nweigh_n &lt;- max(count_y,count_n)/count_n\n\nc(count_y,count_n,weigh_y,weigh_n)\n\n       Yes         No        Yes         No \n565.000000 212.000000   1.000000   2.665094 \n\n\n\ntrain_knn &lt;- train_knn %&gt;% \n               mutate(weight=ifelse(Private==\"Yes\", weigh_y, weigh_n))\n\nfit_weights = train(Private ~ .,\n                    data = train_knn %&gt;% select(-weight), \n                    method = \"naive_bayes\",\n                    tuneLength = 15,\n                    metric = \"Kappa\",\n                    trControl = control,\n                    weights = train_knn$weight)\n\nconfusionMatrix(predict(fit_weights, test_knn),factor(test_knn$Private))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No   32   6\n       Yes  10 107\n                                          \n               Accuracy : 0.8968          \n                 95% CI : (0.8378, 0.9398)\n    No Information Rate : 0.729           \n    P-Value [Acc &gt; NIR] : 2.395e-07       \n                                          \n                  Kappa : 0.7307          \n                                          \n Mcnemar's Test P-Value : 0.4533          \n                                          \n            Sensitivity : 0.7619          \n            Specificity : 0.9469          \n         Pos Pred Value : 0.8421          \n         Neg Pred Value : 0.9145          \n             Prevalence : 0.2710          \n         Detection Rate : 0.2065          \n   Detection Prevalence : 0.2452          \n      Balanced Accuracy : 0.8544          \n                                          \n       'Positive' Class : No              \n                                          \n\n\n\nIn this case, where we had relatively high predictive power to find both private and non-private colleges or universities without weights, adding weights actually disrupted this process and reduced our Kappa. Using weights is not effective in all cases - there are many private colleges and universities, and maybe many different kinds of colleges and universities we find through nearest neighbors or naive bayes, and it is unhelpful to apply weights to public colleges and universities which we already identified relatively effective. It may be worthwhile, however, to add something like a “college or university” feature or something of this nature to further differentiate types of institutions.\n\n\n\nEthics\n\nBased on your analysis, comment on the for-profit privatization of education, perhaps through the framework advanced by this article:\n\n\nIn mid-May 2018, The New York Times reported that under DeVos, the size of the team investigating abuses and fraud by for-profit colleges was reduced from about twelve members under the Obama administration to three, with their task also being scaled back to “processing student loan forgiveness applications and looking at smaller compliance cases”.\n\n\nDiscuss the civic reposibilities of data scientists for:\n\nBig Data and Human-Centered Computing\nDemocratic Institutions\nEducation and Educational Policy\n\nProvide at least one statistical measure for each, such as a RMSE, Kappa value, or ROC curve.\n\n\nBig Data and Human-Centered Computing\n\nWe note that private institutions make up 73% of institutions while educating 43% of students. From our earlier work, we note that private/public predicts graduation rate, and these institutions may be weakly differentiated with regards to measures like acceptance rate, instuctor educational attainment, and cost. A data driven approach to policy may want to investigate closely whether private schools, which enjoy non-profit status and federal loan assistance, are serving students better, or worse, given their lower level of regulatory oversight.\n\n\nlist(\n    sum(filter(College, Private == \"Yes\")$Enroll)/sum(College$Enroll),\n    count(filter(College, Private == \"Yes\"))/count(College)\n)\n\n[[1]]\n[1] 0.4260023\n\n[[2]]\n          n\n1 0.7271557\n\n\n\nWe note that public institutions have dramatically lower cost, including for out-of-state students, in aggregate, possible due to their obligation to serve the broader public through the democratic process. A strong claim, which is partially but not fully supported here, is that democratic pressures dramatically expand access and cut costs, but this at least appears true for the specific niche of higher education.\n\n\n\nDemocratic Institutions\n\nlist(\n    mean(filter(df_knn, Private == \"Yes\")$Cost),\n    mean(filter(df_knn, Private == \"No\")$Cost)\n)\n\n[[1]]\n[1] 18149.78\n\n[[2]]\n[1] 12793.01\n\n\n\n\nEducation and Educational Policy\n\nWe note that private universities achieve dramatically lower student-to-faculty ratios, roughly in line with their higher costs…\n\n\nlist(\n    mean(filter(College, Private == \"Yes\")$S.F.Ratio),\n    mean(filter(College, Private == \"No\")$S.F.Ratio)\n)\n\n[[1]]\n[1] 12.94549\n\n[[2]]\n[1] 17.13915\n\n\n\nYet we do not see lower student-to-faculty ratios necessarily corresponding to a higher graduation rate.\n\n\nlm(Grad.Rate ~ S.F.Ratio, College)\n\n\nCall:\nlm(formula = Grad.Rate ~ S.F.Ratio, data = College)\n\nCoefficients:\n(Intercept)    S.F.Ratio  \n     84.217       -1.331  \n\n\n\n\nClosing Thoughts\n\nAs data scientists with a private school affiliation, we should advocate strongly for democratic process within our institution and our region."
  },
  {
    "objectID": "prac.html",
    "href": "prac.html",
    "title": "Characterizing Colleges",
    "section": "",
    "text": "Setup\n\nSetup\n\n\nsh &lt;- suppressPackageStartupMessages\nsh(library(tidyverse))\nsh(library(caret))\nsh(library(class))\nsh(library(ISLR)) # for the \"College\" dataframe\n\n\n\nDataframe\n\nWe use the College dataframe.\n\n\nhead(College)\n\n                             Private Apps Accept Enroll Top10perc Top25perc\nAbilene Christian University     Yes 1660   1232    721        23        52\nAdelphi University               Yes 2186   1924    512        16        29\nAdrian College                   Yes 1428   1097    336        22        50\nAgnes Scott College              Yes  417    349    137        60        89\nAlaska Pacific University        Yes  193    146     55        16        44\nAlbertson College                Yes  587    479    158        38        62\n                             F.Undergrad P.Undergrad Outstate Room.Board Books\nAbilene Christian University        2885         537     7440       3300   450\nAdelphi University                  2683        1227    12280       6450   750\nAdrian College                      1036          99    11250       3750   400\nAgnes Scott College                  510          63    12960       5450   450\nAlaska Pacific University            249         869     7560       4120   800\nAlbertson College                    678          41    13500       3335   500\n                             Personal PhD Terminal S.F.Ratio perc.alumni Expend\nAbilene Christian University     2200  70       78      18.1          12   7041\nAdelphi University               1500  29       30      12.2          16  10527\nAdrian College                   1165  53       66      12.9          30   8735\nAgnes Scott College               875  92       97       7.7          37  19016\nAlaska Pacific University        1500  76       72      11.9           2  10922\nAlbertson College                 675  67       73       9.4          11   9727\n                             Grad.Rate\nAbilene Christian University        60\nAdelphi University                  56\nAdrian College                      54\nAgnes Scott College                 59\nAlaska Pacific University           15\nAlbertson College                   55\n\n\n\nStates the ISLR textbook:\n\n\n\n\nName\nDescription\n\n\n\n\nPrivate\nPublic/private indicator\n\n\nApps\nNumber of applications received\n\n\nAccept\nNumber of applicants accepted\n\n\nEnroll\nNumber of new students enrolled\n\n\nTop10perc\nNew students from top 10 % of high school class\n\n\nTop25perc\nNew students from top 25 % of high school class\n\n\nF.Undergrad\nNumber of full-time undergraduates\n\n\nP.Undergrad\nNumber of part-time undergraduates\n\n\nOutstate\nOut-of-state tuition\n\n\nRoom.Board\nRoom and board costs\n\n\nBooks\nEstimated book costs\n\n\nPersonal\nEstimated personal spending\n\n\nPhD\nPercent of faculty with Ph.D.’s\n\n\nTerminal\nPercent of faculty with terminal degree\n\n\nS.F.Ratio\nStudent/faculty ratio\n\n\nperc.alumni\nPercent of alumni who donate\n\n\nExpend\nInstructional expenditure per student\n\n\nGrad.Rate\nGraduation rate\n\n\n\n\n\nMultiple Regression\n\nRun a linear regression model with Grad.Rate as the dependent variable and PhD and Expend as features (variables).\n\nRegard PhD and Expend as two forms of investment in education - in training for instructors, and in resources for students.\n\nCompute and comment on the RMSE.\n\n\n# Your code here\n\n\nTODO: Explain\n\n\n\nFeature Engineering\n\nCreate 6+ total features. Consider:\n\nAttributes of the student body.\n\nFor example, an acceptance rate, or a percentages of students in other categories vs. accepted/enrolled.\n\nCosts of the university.\nSome other category, such as related to success, alumni, or faculty.\n\nRemove all rows with a missing value.\nEnsure only Grad.Rate and the engineered features remain.\nCompute and comment on the RMSE.\n\n\n# Your code here\n\n\nTODO: Explain\n\n\n\nNaive Classification\n\nUse either of \\(K\\)-NN or Naive Bayes to predict whether a college is Private.\nExplain your choice of technique.\nReport on your Kappa value.\n\n\n# Your code here\n\n\nTODO: Explain\n\n\n\nImproved Classification\n\nPredict whether a college is Private.\nUse model weights.\nDisplay and comment on an ROC curve.\n\n\n# Your code here\n\n\nTODO: Explain\n\n\n\nEthics\n\nBased on your analysis, comment on the for-profit privatization of education, perhaps through the framework advanced by this article:\n\n\nIn mid-May 2018, The New York Times reported that under DeVos, the size of the team investigating abuses and fraud by for-profit colleges was reduced from about twelve members under the Obama administration to three, with their task also being scaled back to “processing student loan forgiveness applications and looking at smaller compliance cases”.\n\n\nDiscuss the civic reposibilities of data scientists for:\n\nBig Data and Human-Centered Computing\nDemocratic Institutions\nEducation and Educational Policy\n\nProvide at least one statistical measure for each, such as a RMSE, Kappa value, or ROC curve.\n\n\nBig Data and Human-Centered Computing\n\nTODO: Big Data and Human-Centered Computing\n\n\n# Your code here\n\n\n\nDemocratic Institutions\n\nTODO: Democratic Institutions\n\n\n# Your code here\n\n\n\nEducation and Educational Policy\n\nTODO: Education and Educational Policy\n\n\n# Your code here"
  },
  {
    "objectID": "cond_sols.html",
    "href": "cond_sols.html",
    "title": "Conditional Probability",
    "section": "",
    "text": "Abstract:\n\nThis is a technical blog post of both an HTML file and .qmd file hosted on GitHub pages.\n\n\n0. Quarto Type-setting\n\nThis document is rendered with Quarto, and configured to embed an images using the embed-resources option in the header.\nIf you wish to use a similar header, here’s is the format specification for this document:\n\nformat: \n  html:\n    embed-resources: true\n\n\n1. Setup\n\nsh &lt;- suppressPackageStartupMessages\nsh(library(tidyverse))\nsh(library(caret))\nsh(library(naivebayes)) # bae caught me naivin'\nsh(library(tidytext))\nwine &lt;- readRDS(gzcon(url(\"https://github.com/cd-public/D505/raw/master/dat/pinot.rds\")))\n\n\n\n2. Conditional Probability\n\nCalculate the probability that a Pinot comes from Burgundy given it has the word ‘fruit’ in the description.\n\nTake \\(A\\) to be the probability that a Pinot was grown in Burgundy.\nTake \\(B\\) to be the probability that Pinot has the word ‘fruit’ in it’s description.\n\n\n\\[\nP(A|B)\n\\]\n\nnrow(filter(wine,province==\"Burgundy\" & str_detect(description,\"fruit\")))/nrow(filter(wine, str_detect(description,\"fruit\")))\n\n[1] 0.2196038\n\n\n\n\n3. Naive Bayes Algorithm\n\nWe train a naive bayes algorithm to classify a wine’s province using:\n\n\nAn 80-20 train-test split.\nThree features engineered from the description\n5-fold cross validation.\n\n\nWe report Kappa after using the model to predict provinces in the holdout sample.\n\n\nwino = wine %&gt;% \n  mutate(cherry = str_detect(description,\"cherry\")) %&gt;% \n  mutate(chocolate = str_detect(description,\"chocolate\")) %&gt;%\n  mutate(earth = str_detect(description,\"earth\")) %&gt;%\n  select(-description)\n\nwine_index &lt;- createDataPartition(wino$province, p = 0.80, list = FALSE)\ntrain &lt;- wino[ wine_index, ]\ntest &lt;- wino[-wine_index, ]\n\nfit &lt;- train(province ~ .,\n             data = train, \n             method = \"naive_bayes\",\n             metric = \"Kappa\",\n             trControl = trainControl(method = \"cv\", number = 5))\n\nconfusionMatrix(predict(fit, test),factor(test$province))\n\nConfusion Matrix and Statistics\n\n                   Reference\nPrediction          Burgundy California Casablanca_Valley Marlborough New_York\n  Burgundy               171         77                 8          13        5\n  California              64        703                16          24       15\n  Casablanca_Valley        0          0                 0           0        0\n  Marlborough              0          0                 0           0        0\n  New_York                 0          0                 0           0        2\n  Oregon                   3         11                 2           8        4\n                   Reference\nPrediction          Oregon\n  Burgundy             146\n  California           305\n  Casablanca_Valley      0\n  Marlborough            0\n  New_York               0\n  Oregon                96\n\nOverall Statistics\n                                          \n               Accuracy : 0.581           \n                 95% CI : (0.5569, 0.6048)\n    No Information Rate : 0.4728          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.3258          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: Burgundy Class: California Class: Casablanca_Valley\nSensitivity                   0.7185            0.8887                  0.00000\nSpecificity                   0.8265            0.5193                  1.00000\nPos Pred Value                0.4071            0.6238                      NaN\nNeg Pred Value                0.9465            0.8388                  0.98446\nPrevalence                    0.1423            0.4728                  0.01554\nDetection Rate                0.1022            0.4202                  0.00000\nDetection Prevalence          0.2510            0.6736                  0.00000\nBalanced Accuracy             0.7725            0.7040                  0.50000\n                     Class: Marlborough Class: New_York Class: Oregon\nSensitivity                      0.0000        0.076923       0.17550\nSpecificity                      1.0000        1.000000       0.97513\nPos Pred Value                      NaN        1.000000       0.77419\nNeg Pred Value                   0.9731        0.985637       0.70884\nPrevalence                       0.0269        0.015541       0.32696\nDetection Rate                   0.0000        0.001195       0.05738\nDetection Prevalence             0.0000        0.001195       0.07412\nBalanced Accuracy                0.5000        0.538462       0.57532\n\n\n\n\n4. Frequency Differences\n\nWe find the three words that most distinguish New York Pinots from all other Pinots.\n\n\nCalculate relative word count.\n\n\nwc &lt;- function(df, omits) {\n  count &lt;- nrow(df)\n  df %&gt;%\n    unnest_tokens(word, description) %&gt;% anti_join(stop_words) %&gt;%\n    filter(!(word %in% omits)) %&gt;% \n    group_by(word) %&gt;% mutate(total=n()) %&gt;% count() %&gt;%\n    ungroup() %&gt;% mutate(n=n/count)\n}\n\n\nMake corresponding dataframes.\n\n\nomits = c(\"pinot\", \"noir\", \"wine\")\nwc_ny &lt;- wc(wine %&gt;% filter(province==\"New_York\") %&gt;% select(description), omits)\n\nJoining with `by = join_by(word)`\n\nwc_no &lt;- wc(wine %&gt;% filter(province!=\"New_York\") %&gt;% select(description), omits)\n\nJoining with `by = join_by(word)`\n\n\n\nCalculate difference in relative frequencies.\n\n\ndiff &lt;- wc_ny %&gt;%\n    inner_join(wc_no, by = \"word\", suffix = c(\"_ny\", \"_no\")) %&gt;%\n    mutate(diff = n_ny - n_no) %&gt;%\n    arrange(desc(abs(diff)))\n    \ndiff %&gt;% head(3)\n\n# A tibble: 3 × 4\n  word     n_ny  n_no  diff\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 cherry  0.916 0.431 0.485\n2 tannins 0.580 0.234 0.346\n3 palate  0.565 0.239 0.326\n\n\n\nPlot it.\n\n\nsh(library(plotly))\n\nplot_ly(diff %&gt;% top_n(25, diff), \n        x = ~n_ny, y = ~n_no, z = ~diff, text = ~word, \n        type = 'scatter3d', mode = 'markers+text', \n        marker = list(size = 5, color = ~diff, showscale = TRUE),\n        textposition = 'top right') %&gt;%\n    layout(title = \"3D Scatterplot of Word Frequencies\",\n           scene = list(\n               xaxis = list(title = \"Frequency in New York Pinots\"),\n               yaxis = list(title = \"Frequency in Other Pinots\"),\n               zaxis = list(title = \"Difference in Frequency\")\n           ))"
  }
]