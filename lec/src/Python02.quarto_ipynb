{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Feature Engineering in Python\"\n",
        "subtitle: \"Applied Machine Learning\"\n",
        "author: \"Calvin\"\n",
        "  \n",
        "jupyter: python3\n",
        "\n",
        "execute:\n",
        "    echo: true\n",
        "    cache: true\n",
        "    freeze: true  # never re-render during project render\n",
        "---\n",
        "\n",
        "\n",
        "## Agenda\n",
        "\n",
        "1.  Feature Engineering\n",
        "2.  Variable Selection\n",
        "\n",
        "## Import\n",
        "\n",
        "-   Python base data stack"
      ],
      "id": "d1e446f1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "id": "1c981445",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   R compatibility"
      ],
      "id": "30463f17"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pyreadr"
      ],
      "id": "5772fab3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ML Library"
      ],
      "id": "1e479cd8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import RepeatedKFold, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.ensemble import RandomForestRegressor"
      ],
      "id": "ba99b3af",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "-   `pyreadr` to read in an R dataset.\n"
      ],
      "id": "21d7d6f0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "url = 'https://cd-public.github.io/D505/dat/'\n",
        "rds = 'wine.rds'\n",
        "pyreadr.download_file(url + rds, rds) \n",
        "wine = pyreadr.read_r(rds)[None]      \n",
        "wine.dropna(subset=['points','price'])"
      ],
      "id": "f7cadf0f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering\n",
        "\n",
        "## Categorical vs. Continuous\n",
        "\n",
        "-   What is a categorical variable?\n",
        "-   What is a continuous variable?\n",
        "-   Why visualize at the data before modeling it?\n",
        "\n",
        "## Categorical Example 1\n"
      ],
      "id": "af30fec1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "wine['roger'] = wine['taster_name'] == \"Roger Voss\"\n",
        "wine['pinot_gris'] = wine['variety'] == \"Pinot Gris\"\n",
        "means = wine.groupby(['roger','pinot_gris'])['points'].mean().reset_index()\n",
        "sns.lineplot(means,x='pinot_gris',y='points',hue='roger')"
      ],
      "id": "2410a450",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Categorical Example 2\n"
      ],
      "id": "520edcd3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "filtered = wine[wine['province']=='Oregon']\n",
        "sns.lineplot(filtered.groupby('year')['price'].mean())"
      ],
      "id": "68b77707",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise\n",
        "\n",
        "1.  Group by winery and year, Find:\n",
        "    -   The average score, and \n",
        "    -   Number of reviews.\n",
        "2.  Find year-on-year change in score by winery.\n",
        "\n",
        "## Year-on-Year Change Example\n"
      ],
      "id": "e1a5d278"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "wino = wine.groupby(['winery', 'year']).agg(\n",
        "    avg_score=('points', 'mean'),\n",
        "    num_reviews=('id', 'nunique')\n",
        ")\n",
        "wino = wino.sort_values(by=['winery', 'year'])\n",
        "wino['score_change'] = wino.groupby('winery')['avg_score'].diff()\n",
        "wino = wino.dropna(subset=['score_change'])\n",
        "wino.head()"
      ],
      "id": "c7b9a137",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dummy Variables\n",
        "\n",
        "-   **What are Dummy Variables?**: \n",
        "    -   Represent categories as 0s and 1s for models.\n",
        "-   **Why Use Dummy Variables?**: \n",
        "    -   Handle categorical data in numerical algorithms.\n",
        "-   **Avoid Dummy Trap**: \n",
        "    -   Drop one column to prevent multicollinearity.\n",
        "\n",
        "## Many vs Few Dummies\n",
        "\n",
        "-   **Few Dummies**: \n",
        "    -   Simplifies models, risks losing fine-grained patterns.\n",
        "-   **Many Dummies**: \n",
        "    -   Captures detailed trends, increases model complexity.\n",
        "-   **Key Decision**: \n",
        "    -   Balance interpretability and predictive power.\n",
        "\n",
        "## \"fastDummies\" Package\n",
        "\n",
        "-   **Purpose**: \n",
        "    -   Quickly create dummy variables in R datasets.\n",
        "-   **Key Functions**: \n",
        "    -   `dummy_cols()` adds dummy columns efficiently.\n",
        "-   **Features**: \n",
        "    -   Handles multiple columns and missing data flexibly.\n",
        "\n",
        "## Few Dummies\n"
      ],
      "id": "4e8c75dc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "wino = wine[['taster_name']]\n",
        "wino = pd.get_dummies(wino, columns=['taster_name'])\n",
        "wino = wino.iloc[:, :4]\n",
        "wino.head()"
      ],
      "id": "a6c958fe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Many Dummies\n",
        "\n",
        "- Can use `siuba` for `fct_lump` or write custom:\n"
      ],
      "id": "35efdf8d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "wino = wine[['variety']]\n",
        "wino = pd.get_dummies(wino, columns=['variety'])\n",
        "wino.head()"
      ],
      "id": "a77f5228",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Other types of engineered categorical features...\n",
        "\n",
        "-   Words or phrases in text\n",
        "-   A given time period\n",
        "-   An arbitrary numerical cut-off\n",
        "-   Demographic variables\n",
        "\n",
        "## What about numerical features?\n"
      ],
      "id": "434c6667"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sns.histplot(wine['price'], bins=30, kde=False)"
      ],
      "id": "8d50af7d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Take the natural log\n"
      ],
      "id": "69977046"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "wine['log_price'] = np.log(wine['price'])\n",
        "sns.histplot(wine['log_price'], bins=30, kde=False)"
      ],
      "id": "cde15b6f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Standardizing\n",
        "\n",
        "-   Create a common scale across variables.\n",
        "    -   Mean-centering $x-\\bar{x}$\n",
        "    -   Scaling: $x/std(x)$\n",
        "\n",
        "-   Helps reduce bias when interactions are included. \n",
        "    -   (i.e. eliminates variance inflation).\n",
        "\n",
        "## Other transformations.\n",
        "\n",
        "-   I use logs \\> 95% of the time, standarizing \\~40%.\n",
        "-   There are [many other transformations](http://www.feat.engineering/numeric-one-to-many.html):\n",
        "    -   YoY, QoQ, etc. (absolute and percent)\n",
        "    -   log\n",
        "    -   polynomial transforms\n",
        "    -   lags!\n",
        "\n",
        "## Standardize\n",
        "\n",
        "- R was: `list(normalized = ~(scale(.) %>% as.vector))`\n",
        "- Py is either the calculation or include `scipy`\n",
        "\n",
        "\n",
        "```{r Normalize}\n",
        "wine['std_pts'] = (wine['points'] - wine['points'].mean()) / wine['points'].std()\n",
        "\n",
        "print(wine[['points', 'std_pts']].head())\n",
        "```\n",
        "\n",
        "\n",
        "- There's also built-ins.\n",
        "\n",
        "```{.python}\n",
        "# or use scipy\n",
        "from scipy import stats \n",
        "wine['points'] = stats.zscore(wine['points']) \n",
        "```\n",
        "\n",
        "## Philosophy\n",
        "\n",
        "\n",
        "```{dot Philosophy}\n",
        "//| echo: false\n",
        "digraph G {\n",
        "    \n",
        "    bgcolor=\"#101010\";\n",
        "\n",
        "    node [\n",
        "        fontcolor = \"#e0e0e0\",\n",
        "        color = \"#e0e0e0\",\n",
        "    ]\n",
        "\n",
        "    edge [\n",
        "        color = \"#e0e0e0\",\n",
        "        fontcolor = \"#e0e0e0\"\n",
        "    ]\n",
        "    node [shape=circle];\n",
        "    A [label=\"All Data\"];\n",
        "\n",
        "    node [shape=pentagon];\n",
        "    B [label=\"Training\"];\n",
        "    C [label=\"Testing\"];\n",
        "\n",
        "    node [shape=rectangle];\n",
        "    D [label=\"Resample 1\"];\n",
        "    E [label=\"Resample 2\"];\n",
        "    F [label=\"Resample B\"];\n",
        "\n",
        "    node [shape=ellipse];\n",
        "    G [label=\"Analysis\"];\n",
        "    H [label=\"Assessment\"];\n",
        "    I [label=\"Analysis\"];\n",
        "    J [label=\"Assessment\"];\n",
        "    K [label=\"Analysis\"];\n",
        "    L [label=\"Assessment\"];\n",
        "\n",
        "    A -> B;\n",
        "    A -> C;\n",
        "    B -> D;\n",
        "    B -> E;\n",
        "    B -> F;\n",
        "    D -> G;\n",
        "    D -> H;\n",
        "    E -> I;\n",
        "    E -> J;\n",
        "    F -> K;\n",
        "    F -> L;\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "## Types of resampling\n",
        "\n",
        "-   [V-fold Cross-Validation](http://www.feat.engineering/resampling.html#cv)\n",
        "    -   Divides data into $k$ folds, trains on $k−1$ folds, validates on the remaining fold, for all folds.\n",
        "-   [Monte Carlo Cross-Validation](http://www.feat.engineering/resampling.html#monte-carlo-cross-validation)\n",
        "    -   Randomly splits data into training and validation sets multiple times, averaging results for evaluation.\n",
        "-   [The Bootstrap](http://www.feat.engineering/resampling.html#the-bootstrap)\n",
        "    -   Uses resampling with replacement to estimate model accuracy and variability.\n",
        "\n",
        "## Setup the Dataframe\n",
        "\n",
        "-   Follow [this link](https://topepo.github.io/caret) for the full documentation on caret.\n"
      ],
      "id": "33dd20a8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "wino = wine.copy()\n",
        "wino['fr'] = (wino['country'] == \"France\")\n",
        "wino['cab'] = wino['variety'].str.contains(\"Cabernet\")\n",
        "wino['lprice'] = np.log(wino['price'])\n",
        "wino = wino.dropna(subset=['fr', 'cab'])\n",
        "wino = wino[['lprice', 'points', 'fr', 'cab']]"
      ],
      "id": "e2275491",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Off hand, I would've standarized points as well, but\n",
        "-   We're following Jameson's code...\n",
        "    -   ...who *understands the data better*.\n",
        "\n",
        "## Split Samples\n",
        "\n",
        "- Single line train/test split with sklearn.\n"
      ],
      "id": "31e6d681"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "wino_tr, wino_te = train_test_split(wino)"
      ],
      "id": "52228845",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the model\n",
        "\n",
        "-   Configure `train` to cross validate\n"
      ],
      "id": "514f07da"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = LinearRegression()\n",
        "cv = RepeatedKFold(n_splits=5, n_repeats=3)\n",
        "scores = cross_val_score(model,\n",
        "                         wino_tr.drop(columns=['lprice']), \n",
        "                         wino_tr['lprice'], \n",
        "                         cv=cv, \n",
        "                         scoring='neg_mean_squared_error')\n",
        "model.fit(wino_tr.drop(columns=['lprice']),wino_tr['lprice'])"
      ],
      "id": "66b3e207",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train vs. test\n",
        "\n",
        "::::: columns\n",
        "\n",
        "::: {.column width=\"50%\"}\n"
      ],
      "id": "34ed3ef7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y_pred = model.predict(wino_tr.drop(columns=['lprice']))\n",
        "mean_squared_error(wino_tr['lprice'], y_pred) ** .5"
      ],
      "id": "437a70d1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n"
      ],
      "id": "104802ba"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y_pred = model.predict(wino_te.drop(columns=['lprice']))\n",
        "mean_squared_error(wino_te['lprice'], y_pred) ** .5"
      ],
      "id": "1720b711",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        ":::::\n",
        "\n",
        "# Feature selection\n",
        "\n",
        "## Stepwise selection\n",
        "\n",
        "-   **What is Stepwise Selection?**: Iterative method to add or remove predictors in a model based on statistical criteria.\n",
        "-   **Types**: Forward selection starts with no predictors; backward elimination starts with all predictors; stepwise combines both.\n",
        "-   **Goal**: Identify a model with strong predictive power and minimal overfitting.\n",
        "\n",
        "## Stepwise selection is bad\n",
        "\n",
        "Harrell (2015) provides a comprehensive indictment of the method that can be encapsulated by the statement:\n",
        "\n",
        "> **“… if this procedure had just been proposed as a statistical method, it would most likely be rejected because it violates every principle of statistical estimation and hypothesis testing.”**\n",
        "\n",
        "  Reference: [Harrell, F. 2015. Regression Modeling Strategies. Springer.](https://link.springer.com/book/10.1007/978-3-319-19425-7https://link.springer.com/book/10.1007/978-3-319-19425-7)\n",
        "\n",
        "## Engineer 9 features\n"
      ],
      "id": "7374e37e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "wino = wine.copy()\n",
        "top_countries = wino['country'].value_counts().nlargest(4).index\n",
        "top_varieties = wino['variety'].value_counts().nlargest(4).index\n",
        "wino['country'] = np.where(wino['country'].isin(top_countries), wino['country'], 'Other')\n",
        "wino['variety'] = np.where(wino['variety'].isin(top_varieties), wino['variety'], 'Other')\n",
        "wino['lprice'] = np.log(wino['price'])\n",
        "wino = wino[['lprice', 'points', 'country', 'variety']].dropna()\n",
        "\n",
        "wino = pd.get_dummies(wino, columns=['country', 'variety'], drop_first=True)"
      ],
      "id": "cc68d344",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Add Dummy Columns\n",
        "- Careful - a destructive update to `wino`!"
      ],
      "id": "9cfe9d04"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "wino = wino.loc[:, ~wino.columns.str.contains('Other')]"
      ],
      "id": "054a3716",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Factor"
      ],
      "id": "6af9c594"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def do_model(df, y_name):\n",
        "    df_tr, df_te = train_test_split(wino)\n",
        "    model = LinearRegression()\n",
        "    scores = cross_val_score(model,\n",
        "                            df_tr.drop(columns=[y_name]), \n",
        "                            df_tr[y_name], \n",
        "                            cv=RepeatedKFold(n_splits=5, n_repeats=3),\n",
        "                            scoring='neg_mean_squared_error')\n",
        "    model.fit(df_tr.drop(columns=[y_name]),df_tr[y_name])\n",
        "    y_pred = model.predict(df_tr.drop(columns=['lprice']))\n",
        "    tr = mean_squared_error(wino_tr['lprice'], model.predict(df_tr.drop(columns=['lprice']))) ** .5\n",
        "    te = mean_squared_error(wino_te['lprice'], model.predict(df_te.drop(columns=['lprice']))) ** .5\n",
        "    return model, tr, te"
      ],
      "id": "4ba8f4bd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic Model\n"
      ],
      "id": "b252da93"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model, tr, te = do_model(wino, 'lprice')\n",
        "tr, te"
      ],
      "id": "85c3e66c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Variable Importance\n",
        "-   Importance depends on model used...\n"
      ],
      "id": "eafb5d5e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.bar(wino.columns.drop('lprice'), model.coef_)\n",
        "_ = plt.xticks(rotation=90)"
      ],
      "id": "e5a68a7e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Variable Importance\n",
        "\n",
        "-   Each (linear model) coefficient has a standard error, \n",
        "    -   Measures certainty of coefficient given data.\n",
        "-   For the t-statistic, \n",
        "    -   Confidence that the coefficient is different from 0\n",
        "    -   We divide the coefficient by the standard error.\n",
        "-   If \"small\" error relative to coefficient\n",
        "    -   Then \"big\" t-statistic & high feature importance!\n",
        "-   What about coefficient as variable importance?\n",
        "\n",
        "## [Recursive Feature Elimination](https://topepo.github.io/caret/recursive-feature-elimination.html) {.smaller}\n",
        "\n",
        "1. Tune/train the model on the training set using all predictors.\n",
        "2. Calculate model performance.\n",
        "3. Calculate variable importance or rankings.\n",
        "4. **for** each subset size $S_i$, i = 1...S **do**\n",
        "    1. Keep the $S_i$ most important variables.\n",
        "    2. [Optional] Pre-process the data.\n",
        "    3. Tune/train the model on the training set using $S_i$ predictors.\n",
        "    4. Calculate model performance.\n",
        "    5. [Optional] Recalculate the rankings for each predictor.\n",
        "5. **end**\n",
        "6. Calculate the performance profile over the $S_i$.\n",
        "7. Determine the appropriate number of predictors.\n",
        "8. Use the model corresponding to the optimal $S_i$.\n",
        "\n",
        "## Size Drop\n",
        "-   It did not seem like 2024 `r` could handle 90k wine samples.\n",
        "-   Python was faster on the 90k then R on 1k.\n",
        "\n",
        "```{r 1k}\n",
        "wino <- wino[sample(nrow(wino), 1000), ]\n",
        "```\n",
        "\n",
        "\n",
        "## Partition Again\n",
        "-   Partition"
      ],
      "id": "3b966263"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "wino_tr, wino_te = train_test_split(wino)"
      ],
      "id": "fd1f8393",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Caret RFE\n"
      ],
      "id": "51d6e1ce"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = RandomForestRegressor()\n",
        "rfe = RFE(estimator=model, n_features_to_select=3)\n",
        "rfe.fit(wino_tr.drop(columns=['lprice']), wino_tr['lprice'])\n",
        "wino_tr.drop(columns=['lprice']).columns, rfe.ranking_"
      ],
      "id": "09412a12",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practical Workflow\n",
        "\n",
        "\n",
        "```{dot Practice}\n",
        "//| echo: false\n",
        "digraph feature_engineering_pipeline {\n",
        "    \n",
        "    bgcolor=\"#101010\";\n",
        "\n",
        "    node [\n",
        "        fontcolor = \"#e0e0e0\",\n",
        "        color = \"#e0e0e0\",\n",
        "    ]\n",
        "\n",
        "    edge [\n",
        "        color = \"#e0e0e0\",\n",
        "        fontcolor = \"#e0e0e0\"\n",
        "    ]\n",
        "    node [shape=box];\n",
        "    \"Raw Data\" -> \"Lots of Features\" [label=\"Feature Engineering\"];\n",
        "    \"Lots of Features\" -> \"Candidate Features\" [label=\"Feature Selection\"];\n",
        "    \"Candidate Features\" -> \"Shortlist Features\" [label=\"Expert Input\"];\n",
        "    \"Shortlist Features\" -> \"Finalist Models\" [label=\"DS Judgement\"];\n",
        "    \"Finalist Models\" -> \"Production\" [label=\"Business Unit\"];\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "## Key Terms\n",
        "\n",
        "::::: columns\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "-   Feature Engineering\n",
        "-   Categorical Feature\n",
        "-   Continuous Feature\n",
        "-   Dummy\n",
        "-   Interaction\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "-   Sklearn\n",
        "-   Model\n",
        "-   Resampling\n",
        "-   Train vs. Test Data\n",
        "-   Variable Importance\n",
        ":::\n",
        "\n",
        ":::::"
      ],
      "id": "acf7e2fc"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\cd-desk\\AppData\\Local\\Programs\\Python\\Python312\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}