{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a31eb456",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Deep Leaning\"\n",
    "author: \"Hendrik > Calvin\"\n",
    "  \n",
    "execute:\n",
    "    echo: true\n",
    "    cache: true\n",
    "    freeze: true  # never re-render during project render\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68601a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b6ae18",
   "metadata": {},
   "source": [
    "# Neural Networks Basics\n",
    "\n",
    "- **Definition:** A neural network is a computational model inspired by the structure and functioning of the human brain. It consists of interconnected nodes (neurons) organized in layers.\n",
    "  \n",
    "- **Basic Architecture:**\n",
    "  - Input Layer: Receives input data features.\n",
    "  - Hidden Layers: Intermediate layers that perform computations.\n",
    "  - Output Layer: Produces the final output or prediction.\n",
    "\n",
    "- **Forward Pass and Activation Functions:**\n",
    "  - Forward Pass: Input data flows through the network, and computations are performed layer by layer until the output is generated.\n",
    "  - Activation Functions: Non-linear functions applied to the weighted sum of inputs to introduce non-linearity and enable the network to learn complex patterns.\n",
    "  - Automatic feature engineering: we imagine that all the sophisticated feature engineering we are used to doing by hand happen automatically in the hidden layers.\n",
    "\n",
    "\n",
    "<a title=\"Paskari at the English-language Wikipedia, CC BY-SA 3.0 &lt;http://creativecommons.org/licenses/by-sa/3.0/&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Feed_forward_neural_net.gif\"><img width=\"512\" alt=\"Feed forward neural net\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/54/Feed_forward_neural_net.gif?20240322215401\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9226fa4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aae77e35",
   "metadata": {},
   "source": [
    "# Functional Form of a Neuron\n",
    "\n",
    "- **Functional Form:**\n",
    "  - Input: $ x = (x_1, x_2, ..., x_n) $ - Input features or values.\n",
    "  - Weighted Sum: $ z = \\sum_{i=1}^{n} w_i \\cdot x_i + b $ - Linear combination of inputs with weights and bias.\n",
    "  - Activation Function: $ a = f(z) $ - Non-linear function applied to the weighted sum.\n",
    "\n",
    "<a title=\"Funcs, CC0, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Artificial_neuron_structure.svg\"><img style=\"background-color:white\" width=\"512\" alt=\"Artificial neuron structure\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c6/Artificial_neuron_structure.svg/512px-Artificial_neuron_structure.svg.png?20240531082700\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dc8eee",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "\n",
    "- **Activation Functions:**\n",
    "  - **Sigmoid Function:**\n",
    "    - $ \\sigma(z) = \\frac{1}{1 + e^{-z}} $\n",
    "    - S-shaped curve mapping input to a range between 0 and 1. Used in binary classification tasks.\n",
    "\n",
    "  - **Softmax Function:**\n",
    "    - $ \\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{N} e^{z_j}} $\n",
    "    - Outputs a probability distribution over multiple classes. Used in multi-class classification tasks.\n",
    "\n",
    "  - **ReLU (Rectified Linear Unit):**\n",
    "    - $ \\text{ReLU}(z) = \\max(0, z) $\n",
    "    - Outputs the input if it's positive, otherwise, outputs zero. Helps in overcoming the vanishing gradient problem.\n",
    "\n",
    "\n",
    "  - **Other Activation Functions:**\n",
    "    - **Tanh:** Hyperbolic tangent function, mapping input to a range between -1 and 1.\n",
    "    - **Leaky ReLU:** Variation of ReLU that allows a small gradient for negative inputs, addressing the dying ReLU problem.\n",
    "    - Many other activation functions exist, each with different properties and use cases.\n",
    "\n",
    "\n",
    "# Functional Form of a Neuron\n",
    "\n",
    "- **Functional Form:**\n",
    "  - Input: $ x = (x_1, x_2, ..., x_n) $ - Input features or values.\n",
    "  - Weighted Sum: $ z = \\sum_{i=1}^{n} w_i \\cdot x_i + b $ - Linear combination of inputs with weights and bias.\n",
    "  - Activation Function: $ a = f(z) $ - Non-linear function applied to the weighted sum.\n",
    "\n",
    "<a title=\"BrunelloN, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Example_of_a_deep_neural_network.png\"><img width=\"512\" alt=\"An example of the structure of a deep neural network.\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/2f/Example_of_a_deep_neural_network.png/512px-Example_of_a_deep_neural_network.png?20210813150620\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a9e83d",
   "metadata": {},
   "source": [
    "## Why Python?\n",
    "\n",
    "- [https://www.indeed.com/viewjob?jk=38667955752f8d57&from=shareddesktop_copy](https://www.indeed.com/viewjob?jk=38667955752f8d57&from=shareddesktop_copy)\n",
    "- [https://www.indeed.com/viewjob?jk=d876a09728e8a21c&from=shareddesktop_copy](https://www.indeed.com/viewjob?jk=d876a09728e8a21c&from=shareddesktop_copy)\n",
    "- [https://www.indeed.com/viewjob?jk=3eaf03f7179b791c&from=shareddesktop_copy](https://www.indeed.com/viewjob?jk=3eaf03f7179b791c&from=shareddesktop_copy)\n",
    "- [https://www.indeed.com/viewjob?jk=8cc913a6e60fdb12&from=shareddesktop_copy](https://www.indeed.com/viewjob?jk=8cc913a6e60fdb12&from=shareddesktop_copy)\n",
    "- [https://www.indeed.com/viewjob?jk=6be9e463d5db8fe0&from=shareddesktop_copy](https://www.indeed.com/viewjob?jk=6be9e463d5db8fe0&from=shareddesktop_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a37fc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is an example neural network in PyTorch\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        print(out.shape)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09017a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f60a64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpy2\n",
    "import rpy2.rinterface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3210d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cd-desk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\rpy2\\robjects\\packages.py:367: UserWarning: The symbol 'quartz' is not in this R namespace/package.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c025c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In addition: Warning messages:\n",
       "1: package 'torch' was built under R version 4.4.3 \n",
       "2: i torch failed to start, restart your R session to try again.\n",
       "i You might need to reinstall torch using `install_torch()`\n",
       "x C:\\Users\\cd-desk\\AppData\\Local\\R\\win-library\\4.4\\torch/lib\\lantern.dll - The\n",
       "  specified procedure could not be found.\n",
       "Caused by error in `cpp_lantern_init()`:\n",
       "! C:\\Users\\cd-desk\\AppData\\Local\\R\\win-library\\4.4\\torch/lib\\lantern.dll - The specified procedure could not be found.\r \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R\n",
    "library(torch)\n",
    "\n",
    "# Define the neural network class\n",
    "SimpleNN <- nn_module(\n",
    "  initialize = function(input_size, hidden_size, output_size) {\n",
    "    self$fc1 <- nn_linear(input_size, hidden_size)\n",
    "    self$relu <- nn_relu()\n",
    "    self$fc2 <- nn_linear(hidden_size, output_size)\n",
    "    self$sigmoid <- nn_sigmoid()\n",
    "  },\n",
    "  \n",
    "  forward = function(x) {\n",
    "    out <- self$fc1(x)\n",
    "    out <- self$relu(out)\n",
    "    print(dim(out))\n",
    "    out <- self$fc2(out)\n",
    "    out <- self$sigmoid(out)\n",
    "    out\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fa391b",
   "metadata": {},
   "source": [
    "# Motivation for Using Neural Networks\n",
    "\n",
    "- **Universal Approximation Theorem (UAT):**\n",
    "  - The UAT states that a feed-forward neural network with a single hidden layer and a non-linear activation function can approximate any continuous function to arbitrary accuracy given enough neurons in the hidden layer.\n",
    "  - This theorem highlights the expressive power of neural networks in capturing complex relationships and functions.\n",
    "\n",
    "- **Key Points:**\n",
    "  - Neural networks with non-linear activation functions can learn and represent highly nonlinear and intricate mappings between inputs and outputs.\n",
    "  - The flexibility and adaptability of neural networks make them suitable for a wide range of tasks, including regression and classification.\n",
    "  - The number of neurons in the hidden layer and the choice of activation function play crucial roles in the network's capacity to approximate complex functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02a58eb",
   "metadata": {},
   "source": [
    "# Training Neural Networks\n",
    "\n",
    "- **Training Process Overview:**\n",
    "  - Initialize all parameter values to small random numbers.\n",
    "  - Forward Pass:\n",
    "    - Input data is passed through the network, and computations are performed layer by layer.\n",
    "    - Activation functions introduce non-linearity into the model.\n",
    "  \n",
    "  - Loss Calculation:\n",
    "    - The output of the network is compared to the target values using a loss function (more or less error).\n",
    "    - Common loss functions include Mean Squared Error (MSE), Cross Entropy Loss, etc.\n",
    "  \n",
    "  - Backward Pass (Gradient Descent):\n",
    "    - Gradients of the loss function with respect to the model parameters are computed using backpropagation.\n",
    "    - Optimizers update the model parameters (weights and biases) to minimize the loss.\n",
    "\n",
    "  - Update Weights and Biases:\n",
    "    - Optimizers like SGD, Adam, RMSProp, etc., adjust the model parameters based on computed gradients and learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cde5f4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([24, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([24, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([24, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([24, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([24, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([24, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([24, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([24, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([24, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([24, 10])\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create TensorDataset and DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "model = SimpleNN(input_size=4, hidden_size=10, output_size=3)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Iterate over the batches in the training DataLoader\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()  # Zero the gradients from previous step\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acfb9b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 10])\n",
      "Predicted labels: [1 2 1 1 1 2 1 1 1 1 1 2 2 2 2 1 1 1 1 1 2 1 2 1 1 1 1 1 2 2]\n",
      "Reference labels: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
      "~Random guessing: [0 1 0 2 0 2 0 2 2 2 0 1 0 2 1 1 0 0 0 2 2 2 1 0 0 1 0 2 1 1]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    test_outputs = model(X_test)\n",
    "    _, predicted = torch.max(test_outputs, 1)  # Get the class with the highest score\n",
    "\n",
    "# Print the predictions\n",
    "rand = np.random.choice(np.unique(y_test.numpy()), size=len(y_test), replace=True)\n",
    "print(\"Predicted labels:\", predicted.numpy())\n",
    "print(\"Reference labels:\", y_test.numpy())\n",
    "print(\"~Random guessing:\", rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c18d4e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(9), np.int64(6), 30)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(predicted.numpy() == y_test.numpy()), sum(rand == y_test.numpy()), len(predicted) # Count correct predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e7023a",
   "metadata": {},
   "source": [
    "# Practical Applications of Neural Networks\n",
    "\n",
    "- **Image Classification:**\n",
    "  - Identifying objects, scenes, or patterns within images.\n",
    "  - Applications in healthcare, autonomous vehicles, security, etc.\n",
    "\n",
    "- **Natural Language Processing (NLP):**\n",
    "  - Text analysis, sentiment analysis, language translation, chatbots, etc.\n",
    "  - Used in social media, customer support, content generation, etc.\n",
    "\n",
    "- **Medical Diagnosis:**\n",
    "  - Disease diagnosis, medical imaging analysis, patient monitoring, drug discovery, etc.\n",
    "  - Improving healthcare outcomes and decision-making.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34c606a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08b15733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input features (temperature in Celsius)\n",
    "t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0]\n",
    "x = torch.tensor(t_c).view(-1, 1)  # Reshape to a 2D tensor with 11 rows and 1 column\n",
    "\n",
    "# Target values (temperature in Fahrenheit)\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "y = torch.tensor(t_u).view(-1, 1)  # Reshape to a 2D tensor with 11 rows and 1 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d92af5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(t_c, t_u);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cffb9186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data normalization\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "x_normalized = scaler_x.fit_transform(x.float())\n",
    "y_normalized = scaler_y.fit_transform(y.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d8a028c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.lin_coeffs = nn.Linear(input_size, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin_coeffs(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define a simple linear regression model\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)  # One input feature, one output\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85539e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the linear regression model, loss function, and optimizer\n",
    "model = LinearNet(1,1)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfbc1cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.106748580932617\n",
      "OrderedDict({'lin_coeffs.weight': tensor([[-0.4010]]), 'lin_coeffs.bias': tensor([0.3991])})\n",
      "Epoch 101, Loss: 1.4241530895233154\n",
      "OrderedDict({'lin_coeffs.weight': tensor([[-0.1503]]), 'lin_coeffs.bias': tensor([0.3267])})\n",
      "Epoch 201, Loss: 0.9667789936065674\n",
      "OrderedDict({'lin_coeffs.weight': tensor([[0.0549]]), 'lin_coeffs.bias': tensor([0.2674])})\n",
      "Epoch 301, Loss: 0.6603147387504578\n",
      "OrderedDict({'lin_coeffs.weight': tensor([[0.2229]]), 'lin_coeffs.bias': tensor([0.2189])})\n",
      "Epoch 401, Loss: 0.4549678564071655\n",
      "OrderedDict({'lin_coeffs.weight': tensor([[0.3604]]), 'lin_coeffs.bias': tensor([0.1792])})\n",
      "Epoch 501, Loss: 0.3173748254776001\n",
      "OrderedDict({'lin_coeffs.weight': tensor([[0.4730]]), 'lin_coeffs.bias': tensor([0.1467])})\n",
      "Epoch 601, Loss: 0.22518032789230347\n",
      "OrderedDict({'lin_coeffs.weight': tensor([[0.5651]]), 'lin_coeffs.bias': tensor([0.1201])})\n",
      "Epoch 701, Loss: 0.16340528428554535\n",
      "OrderedDict({'lin_coeffs.weight': tensor([[0.6406]]), 'lin_coeffs.bias': tensor([0.0983])})\n",
      "Epoch 801, Loss: 0.1220129206776619\n",
      "OrderedDict({'lin_coeffs.weight': tensor([[0.7023]]), 'lin_coeffs.bias': tensor([0.0804])})\n",
      "Epoch 901, Loss: 0.09427792578935623\n",
      "OrderedDict({'lin_coeffs.weight': tensor([[0.7528]]), 'lin_coeffs.bias': tensor([0.0659])})\n",
      "Final Model Parameters: OrderedDict({'lin_coeffs.weight': tensor([[0.7942]]), 'lin_coeffs.bias': tensor([0.0539])})\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass: compute predicted y by passing x to the model\n",
    "    y_pred = model(torch.tensor(x_normalized, dtype=torch.float32))\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, torch.tensor(y_normalized, dtype=torch.float32))\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n",
    "        print(model.state_dict())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# After training, print the final model parameters\n",
    "print(f'Final Model Parameters: {model.state_dict()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "093efd1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16.04861968])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_y.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "195f04cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.225981999999998"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8.78*0.9369"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743730d9",
   "metadata": {},
   "source": [
    "### Penguins\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a98d0333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96bbcd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = sns.load_dataset(\"penguins\")\n",
    "\n",
    "penguins = penguins.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0ceaa80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.3</td>\n",
       "      <td>20.6</td>\n",
       "      <td>190.0</td>\n",
       "      <td>3650.0</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
       "0  Adelie  Torgersen            39.1           18.7              181.0   \n",
       "1  Adelie  Torgersen            39.5           17.4              186.0   \n",
       "2  Adelie  Torgersen            40.3           18.0              195.0   \n",
       "4  Adelie  Torgersen            36.7           19.3              193.0   \n",
       "5  Adelie  Torgersen            39.3           20.6              190.0   \n",
       "\n",
       "   body_mass_g     sex  \n",
       "0       3750.0    Male  \n",
       "1       3800.0  Female  \n",
       "2       3250.0  Female  \n",
       "4       3450.0  Female  \n",
       "5       3650.0    Male  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penguins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7534dd58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(333, 7)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penguins.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a213a96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "penguins['species_encoded'] = label_encoder.fit_transform(penguins['species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "892526f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PenguinDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.X = data[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']].values\n",
    "        self.y = data['species_encoded'].values # DONT FORGET .VALUES\n",
    "        self.n_samples = len(data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.X[index], dtype=torch.float32), torch.tensor(self.y[index], dtype=torch.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd208d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(penguins, test_size=0.2, random_state=97301)\n",
    "\n",
    "train_dataset = PenguinDataset(train_data)\n",
    "test_dataset = PenguinDataset(test_data)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a262195e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  36.7000,   19.3000,  193.0000, 3450.0000]), tensor(0))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitem__(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "02635d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e734c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 4  # Number of features\n",
    "hidden_size = 64  # Size of the hidden layer\n",
    "num_classes = len(label_encoder.classes_)\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40492de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "023f5b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNN(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d6219272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 256.6689\n",
      "Epoch 2/10, Loss: 56.5208\n",
      "Epoch 3/10, Loss: 20.3136\n",
      "Epoch 4/10, Loss: 10.1350\n",
      "Epoch 5/10, Loss: 9.8483\n",
      "Epoch 6/10, Loss: 11.7395\n",
      "Epoch 7/10, Loss: 6.6724\n",
      "Epoch 8/10, Loss: 5.6617\n",
      "Epoch 9/10, Loss: 6.3352\n",
      "Epoch 10/10, Loss: 6.1882\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3f570bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 43.28%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on the test set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# might be worth picking this apart line by line...\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Accuracy on test set: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae4e00f",
   "metadata": {},
   "source": [
    "Well that was awful. Before we fix it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6cc26cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  35.5000,   16.2000,  195.0000, 3350.0000],\n",
      "        [  34.0000,   17.1000,  185.0000, 3400.0000],\n",
      "        [  49.2000,   15.2000,  221.0000, 6300.0000]])\n",
      "tensor([[ 636.6671,  634.6541,  637.5727],\n",
      "        [ 645.9810,  643.4651,  645.5684],\n",
      "        [1192.1962, 1182.4634, 1181.7388]])\n",
      "tensor([2, 0, 0])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tensor([[  35.5000,   16.2000,  195.0000, 3350.0000],\n",
      "        [  34.0000,   17.1000,  185.0000, 3400.0000],\n",
      "        [  49.2000,   15.2000,  221.0000, 6300.0000]])\n",
      "tensor([[ 636.6671,  634.6541,  637.5727],\n",
      "        [ 645.9810,  643.4651,  645.5684],\n",
      "        [1192.1962, 1182.4634, 1181.7388]])\n",
      "tensor([2, 0, 0])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tensor([[  35.5000,   16.2000,  195.0000, 3350.0000],\n",
      "        [  34.0000,   17.1000,  185.0000, 3400.0000],\n",
      "        [  49.2000,   15.2000,  221.0000, 6300.0000]])\n",
      "tensor([[ 636.6671,  634.6541,  637.5727],\n",
      "        [ 645.9810,  643.4651,  645.5684],\n",
      "        [1192.1962, 1182.4634, 1181.7388]])\n",
      "tensor([2, 0, 0])\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_batches_to_iterate = 3  # Specify the number of batches you want to iterate through\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(test_loader):\n",
    "        if batch_idx >= num_batches_to_iterate:\n",
    "            break\n",
    "        outputs = model(inputs)\n",
    "        print(inputs)\n",
    "        print(outputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        print(predicted)\n",
    "        print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7093a9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(penguins[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']])\n",
    "penguins[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']] = scaled_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6bc6cd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(penguins, test_size=0.2, random_state=12345)\n",
    "\n",
    "train_dataset = PenguinDataset(train_data)\n",
    "test_dataset = PenguinDataset(test_data)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "model = SimpleNN(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "abd1ba88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.0034\n",
      "Epoch 2/10, Loss: 0.7833\n",
      "Epoch 3/10, Loss: 0.6128\n",
      "Epoch 4/10, Loss: 0.4949\n",
      "Epoch 5/10, Loss: 0.3995\n",
      "Epoch 6/10, Loss: 0.3347\n",
      "Epoch 7/10, Loss: 0.2778\n",
      "Epoch 8/10, Loss: 0.2354\n",
      "Epoch 9/10, Loss: 0.1982\n",
      "Epoch 10/10, Loss: 0.1719\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "05745909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 97.01%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on the test set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Accuracy on test set: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c810cf",
   "metadata": {},
   "source": [
    "How striking!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712e12bf",
   "metadata": {},
   "source": [
    "### Exercise: the titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2ba7b259",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv\"\n",
    "titanic_df = pd.read_csv(url)\n",
    "\n",
    "# Drop rows with missing values for simplicity\n",
    "titanic_df = titanic_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b0bb8531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Siblings/Spouses Aboard</th>\n",
       "      <th>Parents/Children Aboard</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Mr. Owen Harris Braund</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mrs. John Bradley (Florence Briggs Thayer) Cum...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Miss. Laina Heikkinen</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mrs. Jacques Heath (Lily May Peel) Futrelle</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Mr. William Henry Allen</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass                                               Name  \\\n",
       "0         0       3                             Mr. Owen Harris Braund   \n",
       "1         1       1  Mrs. John Bradley (Florence Briggs Thayer) Cum...   \n",
       "2         1       3                              Miss. Laina Heikkinen   \n",
       "3         1       1        Mrs. Jacques Heath (Lily May Peel) Futrelle   \n",
       "4         0       3                            Mr. William Henry Allen   \n",
       "\n",
       "      Sex   Age  Siblings/Spouses Aboard  Parents/Children Aboard     Fare  \n",
       "0    male  22.0                        1                        0   7.2500  \n",
       "1  female  38.0                        1                        0  71.2833  \n",
       "2  female  26.0                        0                        0   7.9250  \n",
       "3  female  35.0                        1                        0  53.1000  \n",
       "4    male  35.0                        0                        0   8.0500  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864ed94f",
   "metadata": {},
   "source": [
    "#### Your exercise:\n",
    "\n",
    "Create a neural network as above to model survival on the titanic dataset. There are several ways to do this. Here are some suggestions:\n",
    "* change the size of the output layer (a simple probability, so 1)\n",
    "* change the output of the final hidden layer to be a probability using nn.Sigmoid()\n",
    "* change the loss criterion to be nn.BCELoss()\n",
    "\n",
    "Please note that you can do this all differently: use 2 outputs (one per class), omit sigmoid and keep the same loss function, but the difference might be instructive.\n",
    "\n",
    "* also: explore variations of the model architecture (multiple hidden layers? hidden layer size? etc.) and see how far you can push the model performance!\n",
    "\n",
    "I encourage you to print out lots of intermediate things (your tensors? what is in your data loader? model parameters and performance? etc.)! I learned a lot doing it and I bet you will too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90046b4f",
   "metadata": {},
   "source": [
    "#### One solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7dd2b30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant features and target\n",
    "features = ['Pclass', 'Age', 'Siblings/Spouses Aboard', 'Parents/Children Aboard', 'Fare']\n",
    "target = 'Survived'\n",
    "\n",
    "# Standard scale the features\n",
    "scaler = StandardScaler()\n",
    "titanic_df[features] = scaler.fit_transform(titanic_df[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "24c1a13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom PyTorch dataset\n",
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.X = data[features].values\n",
    "        self.y = data[target].values\n",
    "        self.n_samples = len(data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.X[index], dtype=torch.float32), torch.tensor(self.y[index], dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_data, test_data = train_test_split(titanic_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create PyTorch datasets and dataloaders\n",
    "train_dataset = TitanicDataset(train_data)\n",
    "test_dataset = TitanicDataset(test_data)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2f0557fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.3654, -0.1043, -0.4759, -0.4750, -0.3880]), tensor(1.))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitem__(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1402adb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple neural network with one hidden layer\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bafa15c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "input_size = len(features)  # Number of features\n",
    "hidden_size = 64  # Size of the hidden layer\n",
    "output_size = 1  # Output size (binary classification for survival)\n",
    "\n",
    "model = SimpleNN(input_size, hidden_size, output_size)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "49ca015c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.6559\n",
      "Epoch 2/10, Loss: 0.6542\n",
      "Epoch 3/10, Loss: 0.6504\n",
      "Epoch 4/10, Loss: 0.6496\n",
      "Epoch 5/10, Loss: 0.6456\n",
      "Epoch 6/10, Loss: 0.6431\n",
      "Epoch 7/10, Loss: 0.6423\n",
      "Epoch 8/10, Loss: 0.6421\n",
      "Epoch 9/10, Loss: 0.6369\n",
      "Epoch 10/10, Loss: 0.6389\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        #print(outputs)\n",
    "        #print(targets)\n",
    "        #print(outputs.squeeze())\n",
    "        #print(outputs.shape)\n",
    "        #print(outputs.squeeze().shape)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a6122eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 64.04%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        predicted = torch.round(outputs)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets.unsqueeze(1)).sum().item()  # Ensure targets are 2D\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Accuracy on test set: {accuracy:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
