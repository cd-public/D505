---
title: "Logistic Regression"
subtitle: "Applied Machine Learning"
author: "Jameson > Calvin"
  
execute:
    echo: true
    cache: true
    freeze: true  # never re-render during project render
---

## Agenda

## Agenda

0. Model Announcement
1. Mathematics of logistic regression
2. Implementation with Caret
4. ROC Curves
5. Model Weights
6. Regularization (penalized logit)

# First Model Due 3/10

##  Publish

0. Form groups of size 1 to 4, create
1. An annotated `.*md` file, and 
2. The .rds/.pickle/.parquet file that it generates, that
3. Contains *only* the features you want in the model.

Under version control, on GitHub.

##  Constraints

I will run:
1. The specify $K$NN or Naive Bayes model, (say in the .*md)
2. With: `province ~ .` on your data frame (or the whole data frame in `scikit`)
3. With repeated 5-fold cross validation
4. With the same index for partitioning training and test sets for every group.
5. On whatever is turned in before class.
6. Bragging rights for highest Kappa

## Optional

- Separate categories for `.py` and `.r`
- The `.py` tooling is stronger but you are getting less training on it.
- Let me know in advance if you intend to Python, or not.

## Context

- The "final exam" is that during the last class you will present your model results as though you are speaking to the
managers of a large winery.
- It should be presented from a Quarto presentation.
- You must present via the in-room "teaching machine" computer, not your own physical device, to ensure that you are comfortable distributing your findings.

## 10 minutes

- Chat about forming groups
- Don't need to lock in now if not everyone is here etc.
- 

# Logistic Regression

## Algorithm

- Assume a linear relationship between the log odds and a set of predictor variables.

$$
log(\frac{p}{1-p})=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}
$$

With a bit of algebra you can get the probabilities as...

$p=\frac{1}{1+e^{-(\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2})}}$

Why do we call this regression instead of classification?

# Implementation with Caret

## Libraries Setup
```{r libs}
sh <- suppressPackageStartupMessages
sh(library(tidyverse))
sh(library(caret))
sh(library(tidytext))
sh(library(tidytext))
sh(library(SnowballC)) # new?
sh(library(pROC))      # new?
data(stop_words)
sh(library(thematic))
theme_set(theme_dark())
thematic_rmd(bg = "#111", fg = "#eee", accent = "#eee")
```

## Dataframe

```{r fg}
wine <- readRDS(gzcon(url("https://cd-public.github.io/D505/dat/pinot.rds")))
names(wine)[names(wine) == 'id'] = 'id'
```


## create a function to extract words with totals > j

```{r wordfunc}
wine_words <- function(df, j = 1000, stem=F) { 

  words <- df %>%
    unnest_tokens(word, description) %>%
    anti_join(stop_words) %>% # get rid of stop words
    filter(!(word %in% c("wine","pinot","vineyard")))
  
  if(stem){
    words <- words %>% mutate(word = wordStem(word))
  }
  
  words <- words %>% 
    count(id, word) %>% 
    group_by(id) %>% mutate(exists = (n>0)) %>% ungroup %>% 
    group_by(word) %>% 
    mutate(total = sum(n)) %>% 
    filter(total > j) %>% 
    pivot_wider(id_cols = id, names_from = word, values_from = exists, values_fill = list(exists=0)) %>% 
    right_join(select(df,id,province)) %>% 
    drop_na() %>% 
    select(-id)
}
```

## Look at the data

```{r w100}
wino <- wine_words(wine, 500)

wino %>% 
  head(10) %>% 
  select(1:5,province)
```

## Split the data 

```{r tt100}
wine_index <- createDataPartition(wino$province, p = 0.80, list = FALSE)
train <- wino[ wine_index, ]
test <- wino[-wine_index, ]
table(train$province)
```

## A basic model

```{r getfit}
control = trainControl(method = "cv", number = 5)
get_fit <- function(df) {
  train(province ~ .,
        data = df, 
        trControl = control,
        method = "multinom",
        maxit = 5) # speed it up - default 100
}
fit <- get_fit(train)
```

## Check Kappa

```{r}
fit
```

## Probability

- See top coefficients

```{r getodds}
get_odds <- function(fit) {
  as.data.frame(t(exp(coef(fit$finalModel))))   %>%
  rownames_to_column(var = "name") %>%
  pivot_longer(-name, names_to = "class", values_to = "odds") %>%
  arrange(desc(odds)) %>%
  head()
}
get_odds(fit)
```

## Confusion Matrix

```{r getmatrix}
get_matrix <- function(fit, df) {
  pred <- factor(predict(fit, newdata = df))
  confusionMatrix(pred,factor(df$province))
}
get_matrix(fit,test)
```

Not bad. But what if we decrease the number of words used?

## Using more words

```{r w500}
wino <- wine_words(wine, 1000)
wine_index <- createDataPartition(wino$province, p = 0.80, list = FALSE)
train <- wino[ wine_index, ]
test <- wino[-wine_index, ]

fit <- get_fit(train)
```

## Check Kappa

```{r}
fit
```


## Confusion Matrix

```{r 500matrix}
get_matrix(fit,test)
```

## Using stems

```{r stem}
wino <- wine_words(wine, 1000, T)
wine_index <- createDataPartition(wino$province, p = 0.80, list = FALSE)
train <- wino[ wine_index, ]
test <- wino[-wine_index, ]

fit <- get_fit(train)
get_odds(fit)
```

## Check Kappa

```{r}
fit
```

## Confusion Matrix

```{r stematrix}
get_matrix(fit,test)
```

Even better!

# ROC Curve evaluation

**Note:**

$Sensitivity = \frac{truePos}{allPos} = TruePosRate$

$Specificity = \frac{trueNeg}{allNeg} = TrueNegRate = 1 - FalsePosRate$


```{.r}
myRoc <- roc(test$province, prob)
plot(myRoc)
auc(myRoc)
```

```{.r}
labels <- test$province[order(prob, decreasing=TRUE)]
roc_df <- data.frame(TruePosRate=cumsum(labels)/sum(labels), FalsePosRate=cumsum(!labels)/sum(!labels), labels)

roc_df %>% 
  slice(110:120) 
```


```{.r}
roc_df %>% 
  ggplot(aes(FalsePosRate,TruePosRate))+
  geom_point()
```

## Exercise

1. Gather into your prediction teams.
2. Choose a Pinot province other than Oregon or California
3. Use logistic regression to find the words/terms that increase the odds of choosing that province the most


# Weigted Penalty

```{.r}
wine = read_rds("../resources/pinot.rds") %>% 
  mutate(province = as.numeric(province=="New_York")) %>% 
  rowid_to_column("id") %>% 
  select(-taster_name)

wino <- wine_words(wine, j=500, stem = T)
table(wino$province)
```

## Basic model

```{.r}
fit <- train(province ~ .,
             data = wino, 
             trControl = control,
             method = "glm",
             family = "binomial")

prob <- predict(fit, newdata=test)
pred <- ifelse(prob > 0.5, 1, 0)

confusionMatrix(factor(pred),factor(test$province))

```


## Create some weights

```{.r}
weight_train <- train %>% 
  mutate(weights=if_else(province==1,20,1))

fit <- train(province ~ .,
             data = train, 
             trControl = trainControl(method = "cv", number = 5),
             method = "glm",
             family = "binomial",
             weights = weight_train$weights)

prob <- predict(fit, newdata=test)
pred <- ifelse(prob > 0.5, 1, 0)

confusionMatrix(factor(pred),factor(test$province))

```
## Top features

```{.r}
#show the odds ratios for top coefficients
odds <- exp(coef(fit$finalModel))
data.frame(name = names(odds), odds = odds) %>%  
  mutate(probability=odds/(1+odds)) %>% 
  arrange(desc(odds)) %>% 
  head(10)
```


# Regularization

![](images/Regularization.png)

Often also called 'penalized' because it adds a penalizing term to the loss function.

$\min_{f}\sum_{i=1}^{n}V(f(x_{i}),y_{i})+\lambda R(f)$

The amount of the penalty is fine-tuned using a constant called lambda. When lambda = 0, no penalty is enforced. The best lambda can be found by finding a value that minimizes prediction error after cross validating the model with different values.

```{.r}
library(glmnet)
```


## Lasso Regression

$\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij}    \right) ^ 2 + \lambda \sum_{j=1}^{p} |\beta_j|$

Lasso stands for Least Absolute Shrinkage and Selection Operator. It shrinks the regression coefficients toward zero by penalizing the regression model with a penalty term called L1-norm, which is the sum of the absolute coefficients.

In the case of lasso regression, the penalty has the effect of forcing some of the coefficient estimates, with a minor contribution to the model, to be exactly equal to zero. This means that, lasso can be also seen as an alternative to the subset selection methods for performing variable selection in order to reduce the complexity of the model.

## Ridge Regression

$\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij}    \right) ^ 2 + \lambda \sum_{j=1}^{p} \beta_j^2$

Ridge regression shrinks the regression coefficients, so that variables, with minor contribution to the outcome, have their coefficients close to zero.

The shrinkage of the coefficients is achieved by penalizing the regression model with a penalty term called **L2-norm**, which is the sum of the squared coefficients. 


## Elastic Net

$\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij}  \right) ^ 2 + \alpha\lambda \sum_{j=1}^{p} |\beta_j|+(1-\alpha)\lambda \sum_{j=1}^{p} \beta_j^2$

Generally, lasso might perform better in a situation where some of the predictors have large coefficients, and the remaining predictors have very small coefficients.

Ridge regression will perform better when the outcome is a function of many predictors, all with coefficients of roughly equal size (James et al. 2014).


```{.r}
wine = read_rds("../resources/pinot.rds") %>% 
  mutate(province = as.numeric(province=="Burgundy")) %>% 
  rowid_to_column("id") %>% 
  select(-taster_name)

wino <- wine_words(wine, j=500, stem = T)

set.seed(504)
wino <- wine_words(wine)
wine_index <- createDataPartition(wino$province, p = 0.80, list = FALSE)
train <- wino[ wine_index, ]
test <- wino[-wine_index, ]
fit <- train(
  province ~., 
  data = train,
  method = "glmnet",
  trControl = control,
  tuneLength = 10
)

# Best tuning parameter
fit$bestTune
# to specify the best lambda
exp(coef(fit$finalModel, fit$bestTune$lambda))
```

## Confusion Matrix

```{.r}
prob <- predict(fit, newdata=test)
pred <- ifelse(prob > 0.5, 1, 0)

confusionMatrix(factor(pred),factor(test$province))
```

## ROC Curve

```{.r}
myRoc <- roc(test$province, prob)
plot(myRoc)
auc(myRoc)
```
# Vocabulary

- Logistic Regression
- Odds
- Sensitivity
- Specificity
- ROC
- AUC

# Extra reading

https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62
https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5

# References

James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2014. An Introduction to Statistical Learning: With Applications in R. Springer Publishing Company, Incorporated.

http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/153-penalized-regression-essentials-ridge-lasso-elastic-net/

https://en.wikipedia.org/wiki/Regularization_(mathematics)

