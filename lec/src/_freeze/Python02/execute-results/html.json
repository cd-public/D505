{
  "hash": "abc812de9e34c4bdd2349bd00c2aa061",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Feature Engineering in Python\"\nsubtitle: \"Applied Machine Learning\"\nauthor: \"Calvin\"\n  \njupyter: python3\n\nexecute:\n    echo: true\n    cache: true\n    freeze: true  # never re-render during project render\n---\n\n\n## Agenda\n\n1.  Feature Engineering\n2.  Variable Selection\n\n## Import\n\n-   Python base data stack\n\n::: {#4c8dfb4d .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n```\n:::\n\n\n-   R compatibility\n\n::: {#2d8c86b4 .cell execution_count=3}\n``` {.python .cell-code}\nimport pyreadr\n```\n:::\n\n\n## ML Library\n```python scikit\n# Common to include parts of, not all of, sklearn\n```\n\n## Setup\n\n-   `pyreadr` to read in an R dataset.\n\n::: {#ba7105ed .cell execution_count=4}\n``` {.python .cell-code}\nurl = 'https://cd-public.github.io/courses/rmls25/dat/'\nrds = 'wine.rds'\npyreadr.download_file(url + rds, rds) \nwine = pyreadr.read_r(rds)[None]      \nwine.dropna(subset=['points','price'])\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>country</th>\n      <th>description</th>\n      <th>designation</th>\n      <th>points</th>\n      <th>price</th>\n      <th>province</th>\n      <th>region_1</th>\n      <th>region_2</th>\n      <th>taster_name</th>\n      <th>taster_twitter_handle</th>\n      <th>title</th>\n      <th>variety</th>\n      <th>winery</th>\n      <th>year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>Portugal</td>\n      <td>This is ripe and fruity, a wine that is smooth...</td>\n      <td>Avidagos</td>\n      <td>87.0</td>\n      <td>15.0</td>\n      <td>Douro</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Roger Voss</td>\n      <td>@vossroger</td>\n      <td>Quinta dos Avidagos 2011 Avidagos Red (Douro)</td>\n      <td>Portuguese Red</td>\n      <td>Quinta dos Avidagos</td>\n      <td>2011.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.0</td>\n      <td>US</td>\n      <td>Tart and snappy, the flavors of lime flesh and...</td>\n      <td>NaN</td>\n      <td>87.0</td>\n      <td>14.0</td>\n      <td>Oregon</td>\n      <td>Willamette Valley</td>\n      <td>Willamette Valley</td>\n      <td>Paul Gregutt</td>\n      <td>@paulgwine</td>\n      <td>Rainstorm 2013 Pinot Gris (Willamette Valley)</td>\n      <td>Pinot Gris</td>\n      <td>Rainstorm</td>\n      <td>2013.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.0</td>\n      <td>US</td>\n      <td>Pineapple rind, lemon pith and orange blossom ...</td>\n      <td>Reserve Late Harvest</td>\n      <td>87.0</td>\n      <td>13.0</td>\n      <td>Michigan</td>\n      <td>Lake Michigan Shore</td>\n      <td>NaN</td>\n      <td>Alexander Peartree</td>\n      <td>NaN</td>\n      <td>St. Julian 2013 Reserve Late Harvest Riesling ...</td>\n      <td>Riesling</td>\n      <td>St. Julian</td>\n      <td>2013.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.0</td>\n      <td>US</td>\n      <td>Much like the regular bottling from 2012, this...</td>\n      <td>Vintner's Reserve Wild Child Block</td>\n      <td>87.0</td>\n      <td>65.0</td>\n      <td>Oregon</td>\n      <td>Willamette Valley</td>\n      <td>Willamette Valley</td>\n      <td>Paul Gregutt</td>\n      <td>@paulgwine</td>\n      <td>Sweet Cheeks 2012 Vintner's Reserve Wild Child...</td>\n      <td>Pinot Noir</td>\n      <td>Sweet Cheeks</td>\n      <td>2012.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>Spain</td>\n      <td>Blackberry and raspberry aromas show a typical...</td>\n      <td>Ars In Vitro</td>\n      <td>87.0</td>\n      <td>15.0</td>\n      <td>Northern Spain</td>\n      <td>Navarra</td>\n      <td>NaN</td>\n      <td>Michael Schachner</td>\n      <td>@wineschach</td>\n      <td>Tandem 2011 Ars In Vitro Tempranillo-Merlot (N...</td>\n      <td>Tempranillo-Merlot</td>\n      <td>Tandem</td>\n      <td>2011.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>89551</th>\n      <td>129966.0</td>\n      <td>Germany</td>\n      <td>Notes of honeysuckle and cantaloupe sweeten th...</td>\n      <td>Brauneberger Juffer-Sonnenuhr Spätlese</td>\n      <td>90.0</td>\n      <td>28.0</td>\n      <td>Mosel</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Anna Lee C. Iijima</td>\n      <td>NaN</td>\n      <td>Dr. H. Thanisch (Erben Müller-Burggraef) 2013 ...</td>\n      <td>Riesling</td>\n      <td>Dr. H. Thanisch (Erben Müller-Burggraef)</td>\n      <td>2013.0</td>\n    </tr>\n    <tr>\n      <th>89552</th>\n      <td>129967.0</td>\n      <td>US</td>\n      <td>Citation is given as much as a decade of bottl...</td>\n      <td>NaN</td>\n      <td>90.0</td>\n      <td>75.0</td>\n      <td>Oregon</td>\n      <td>Oregon</td>\n      <td>Oregon Other</td>\n      <td>Paul Gregutt</td>\n      <td>@paulgwine</td>\n      <td>Citation 2004 Pinot Noir (Oregon)</td>\n      <td>Pinot Noir</td>\n      <td>Citation</td>\n      <td>2004.0</td>\n    </tr>\n    <tr>\n      <th>89553</th>\n      <td>129968.0</td>\n      <td>France</td>\n      <td>Well-drained gravel soil gives this wine its c...</td>\n      <td>Kritt</td>\n      <td>90.0</td>\n      <td>30.0</td>\n      <td>Alsace</td>\n      <td>Alsace</td>\n      <td>NaN</td>\n      <td>Roger Voss</td>\n      <td>@vossroger</td>\n      <td>Domaine Gresser 2013 Kritt Gewurztraminer (Als...</td>\n      <td>Gewürztraminer</td>\n      <td>Domaine Gresser</td>\n      <td>2013.0</td>\n    </tr>\n    <tr>\n      <th>89554</th>\n      <td>129969.0</td>\n      <td>France</td>\n      <td>A dry style of Pinot Gris, this is crisp with ...</td>\n      <td>NaN</td>\n      <td>90.0</td>\n      <td>32.0</td>\n      <td>Alsace</td>\n      <td>Alsace</td>\n      <td>NaN</td>\n      <td>Roger Voss</td>\n      <td>@vossroger</td>\n      <td>Domaine Marcel Deiss 2012 Pinot Gris (Alsace)</td>\n      <td>Pinot Gris</td>\n      <td>Domaine Marcel Deiss</td>\n      <td>2012.0</td>\n    </tr>\n    <tr>\n      <th>89555</th>\n      <td>129970.0</td>\n      <td>France</td>\n      <td>Big, rich and off-dry, this is powered by inte...</td>\n      <td>Lieu-dit Harth Cuvée Caroline</td>\n      <td>90.0</td>\n      <td>21.0</td>\n      <td>Alsace</td>\n      <td>Alsace</td>\n      <td>NaN</td>\n      <td>Roger Voss</td>\n      <td>@vossroger</td>\n      <td>Domaine Schoffit 2012 Lieu-dit Harth Cuvée Car...</td>\n      <td>Gewürztraminer</td>\n      <td>Domaine Schoffit</td>\n      <td>2012.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>89556 rows × 15 columns</p>\n</div>\n```\n:::\n:::\n\n\n# Feature Engineering\n\n## Group Exercise (30m)\n\n1.  Identify 3 \"interesting\" features of the wine dataset\n2.  **Bonus** \n    -   &exist; Critic \"Roger Voss\"\n    -   &exist; a wine varietal(s) \"Voss\" seems to dislike\n    -   Find said varietal.\n\n## Categorical vs. Continuous\n\n-   What is a categorical variable?\n-   What is a continuous variable?\n-   Why visualize at the data before modeling it?\n\n## Categorical Example 1\n\n::: {#742d262f .cell execution_count=5}\n``` {.python .cell-code}\nwine['roger'] = wine['taster_name'] == \"Roger Voss\"\nwine['pinot_gris'] = wine['variety'] == \"Pinot Gris\"\nmeans = wine.groupby(['roger','pinot_gris'])['points'].mean().reset_index()\nsns.lineplot(means,x='pinot_gris',y='points',hue='roger')\n```\n\n::: {.cell-output .cell-output-display}\n![](Python02_files/figure-revealjs/cell-5-output-1.png){width=838 height=434}\n:::\n:::\n\n\n## Categorical Example 2\n\n::: {#713ab1f7 .cell execution_count=6}\n``` {.python .cell-code}\nfiltered = wine[wine['province']=='Oregon']\nsns.lineplot(filtered.groupby('year')['price'].mean())\n```\n\n::: {.cell-output .cell-output-display}\n![](Python02_files/figure-revealjs/cell-6-output-1.png){width=808 height=429}\n:::\n:::\n\n\n## YoY Exercise\n\n1.  Group by winery and year, Find:\n    -   The average score, and \n    -   Number of reviews.\n2.  Find year-on-year change in score by winery.\n\n## Year-on-Year Change Example\n\n\n```{r YoY}\nwine %>%\n  group_by(winery, year) %>%\n  summarize(avg_score = mean(points), num_reviews = n_distinct(id)) %>%\n  select(year, winery, num_reviews, avg_score) %>%\n  arrange(winery, year) %>%\n  mutate(score_change = avg_score - lag(avg_score)) %>%\n  drop_na(score_change) %>%\n  summarize(mean(score_change))\n```\n\n\n## Dummy Variables\n\n-   **What are Dummy Variables?**: \n    -   Represent categories as 0s and 1s for models.\\\n-   **Why Use Dummy Variables?**: \n    -   Handle categorical data in numerical algorithms.\\\n-   **Avoid Dummy Trap**: \n    -   Drop one column to prevent multicollinearity.\n\n## Many vs Few Dummies\n\n-   **Few Dummies**: \n    -   Simplifies models, risks losing fine-grained patterns.\n-   **Many Dummies**: \n    -   Captures detailed trends, increases model complexity.\n-   **Key Decision**: \n    -   Balance interpretability and predictive power.\n\n## \"fastDummies\" Package\n\n-   **Purpose**: \n    -   Quickly create dummy variables in R datasets.\n-   **Key Functions**: \n    -   `dummy_cols()` adds dummy columns efficiently.\n-   **Features**: \n    -   Handles multiple columns and missing data flexibly.\n\n## Few Dummies\n\n\n```{r Few Dummies}\nwine %>%\n  select(taster_name) %>%\n  dummy_cols() %>% # library(fastDummies)\n  select(1:4) %>%\n  head()\n```\n\n\n## Many Dummies\n\n\n```{r Many Dummies}\nwine %>%\n  select(variety) %>%\n  mutate(variety = fct_lump(variety, 4)) %>%\n  dummy_cols() %>%\n  head()\n```\n\n\n## Other types of engineered categorical features...\n\n-   Words or phrases in text\n-   A given time period\n-   An arbitrary numerical cut-off\n-   Demographic variables\n\n## What about numerical features?\n\n\n```{r Price gg}\nwine %>%\n  ggplot(aes(price)) +\n  geom_histogram()\n```\n\n\n## Take the natural log\n\n\n```{r Log Price gg}\nwine %>%\n  ggplot(aes(log(price))) +\n  geom_histogram()\n```\n\n\n## Standardizing\n\n-   Create a common scale across variables.\n    -   Mean-centering $x-\\bar{x}$\n    -   Scaling: $x/std(x)$\n\n-   Helps reduce bias when interactions are included. \n    -   (i.e. eliminates variance inflation).\n\n## Other transformations.\n\n-   I use logs \\> 95% of the time, standarizing \\~40%.\n-   There are [many other transformations](http://www.feat.engineering/numeric-one-to-many.html):\n    -   YoY, QoQ, etc. (absolute and percent)\n    -   log\n    -   polynomial transforms\n    -   lags!\n\n## Standardize\n\n-   `list(normalized = ~(scale(.) %>% as.vector))`: :\n    -   `scale(.)`: Standardizes the \"points\" column.\n    -   `%>% as.vector`: Converts back to a vector.\n\n\n```{r Normalize}\nwine %>% mutate_at(\"points\", list(standardized = ~ (scale(.) %>% as.vector())))\n```\n\n\n## Interaction effects\n\n[This chapter](http://www.feat.engineering/detecting-interaction-effects.html) has a good overview of interactions.\n\n-   Start with domain knowledge.\n-   Use visualizations.\n-   3-way interactions exist, but are rare.\n    -   If you suspect a 3-way, also suspect your suspicions.\n    -   Complexity increases exponentially in \"ways\".\n    -   These are notoriously hard to explain.\n\n## Dinner (and virtual high fives)\n\n![](images/comic2.gif)\n\n# Le Bibliothèques _caret_\n\n## Philosophy\n\n\n```{dot Philosophy}\n//| echo: false\ndigraph G {\n    \n    bgcolor=\"#101010\";\n\n    node [\n        fontcolor = \"#e0e0e0\",\n        color = \"#e0e0e0\",\n    ]\n\n    edge [\n        color = \"#e0e0e0\",\n        fontcolor = \"#e0e0e0\"\n    ]\n    node [shape=circle];\n    A [label=\"All Data\"];\n\n    node [shape=pentagon];\n    B [label=\"Training\"];\n    C [label=\"Testing\"];\n\n    node [shape=rectangle];\n    D [label=\"Resample 1\"];\n    E [label=\"Resample 2\"];\n    F [label=\"Resample B\"];\n\n    node [shape=ellipse];\n    G [label=\"Analysis\"];\n    H [label=\"Assessment\"];\n    I [label=\"Analysis\"];\n    J [label=\"Assessment\"];\n    K [label=\"Analysis\"];\n    L [label=\"Assessment\"];\n\n    A -> B;\n    A -> C;\n    B -> D;\n    B -> E;\n    B -> F;\n    D -> G;\n    D -> H;\n    E -> I;\n    E -> J;\n    F -> K;\n    F -> L;\n}\n```\n\n\n## Types of resampling\n\n-   [V-fold Cross-Validation](http://www.feat.engineering/resampling.html#cv)\n    -   Divides data into $k$ folds, trains on $k−1$ folds, validates on the remaining fold, for all folds.\n-   [Monte Carlo Cross-Validation](http://www.feat.engineering/resampling.html#monte-carlo-cross-validation)\n    -   Randomly splits data into training and validation sets multiple times, averaging results for evaluation.\n-   [The Bootstrap](http://www.feat.engineering/resampling.html#the-bootstrap)\n    -   Uses resampling with replacement to estimate model accuracy and variability.\n\n## Setup the Dataframe\n\n-   Follow [this link](https://topepo.github.io/caret) for the full documentation on caret.\n\n\n```{r Engineering}\nwino <- wine %>% # 3 engineered features\n  mutate(fr = (country == \"France\")) %>%\n  mutate(cab = str_detect(variety, \"Cabernet\")) %>%\n  mutate(lprice = log(price)) %>%\n  drop_na(fr, cab) %>%\n  select(lprice, points, fr, cab)\n```\n\n\n-   Off hand, I would've standarized points as well, but\n-   We're following Jameson's code...\n    -   ...who *understands the data better*.\n\n## Split Samples\n\n\n```{r Split}\nwine_index <- createDataPartition(wino$lprice, p = 0.8, list = FALSE)\nwino_tr <- wino[wine_index, ]\nwino_te <- wino[-wine_index, ]\nsummary(wino_tr)\n```\n\n\n## Train the model\n\n-   Configure `train` to cross validate\n\n\n```{r Train Model}\nm1 <- train(lprice ~ .,\n  data = wino_tr,\n  method = \"lm\",\n  trControl = trainControl(method = \"repeatedcv\", number = 5, repeats = 3)\n)\nm1\n```\n\n\n## RMSE outputs\n\n\n```{r RMSE}\nprint(m1$resample)\n```\n\n\n## Train vs. test\n\n::::: columns\n\n::: {.column width=\"50%\"}\n\n\n```{r Train}\nm1\n```\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n```{r Test}\npostResample(pred = predict(m1, wino_te), obs = wino_te$lprice)\n```\n\n\n:::\n\n:::::\n\n## Group Exercise (30+ minutes)\n\n1.  Create 5-10 new features (in addition to points)\n2.  Create training and test data\n3.  For each, train a linear model for log(price)\n4.  Report RMSE on test set and cross-validated score.\n5.  (Re-)Engineer new(ish) features to lower the RMSE.\n\n# Feature selection\n\n## Stepwise selection\n\n-   **What is Stepwise Selection?**: Iterative method to add or remove predictors in a model based on statistical criteria.\\\n-   **Types**: Forward selection starts with no predictors; backward elimination starts with all predictors; stepwise combines both.\\\n-   **Goal**: Identify a model with strong predictive power and minimal overfitting.\n\n## Stepwise selection is bad\n\nHarrell (2015) provides a comprehensive indictment of the method that can be encapsulated by the statement:\n\n> **“… if this procedure had just been proposed as a statistical method, it would most likely be rejected because it violates every principle of statistical estimation and hypothesis testing.”**\n\n  Reference: [Harrell, F. 2015. Regression Modeling Strategies. Springer.](https://link.springer.com/book/10.1007/978-3-319-19425-7https://link.springer.com/book/10.1007/978-3-319-19425-7)\n\n## Engineer 9 features\n\n\n```{r Mutates}\nwino <- wine %>%\n  mutate(country = fct_lump(country, 4)) %>%    # 1:4,\n  mutate(variety = fct_lump(variety, 4)) %>%    # 5:8,\n  mutate(lprice = log(price)) %>%               #   9\n  select(lprice, points, country, variety) %>%\n  drop_na(.)\nhead(wino)\n```\n\n\n## Add Dummy Columns\n- Careful - a destructive update to `wino`!\n\n```{r Format}\nrenamer <- function(s) {\n  s %>% tolower() %>% str_replace(\"-| \", \"_\")\n}\n\nwino <- wino %>%\n  dummy_cols(remove_selected_columns = TRUE) %>%\n  rename_with(.fn = renamer) %>%\n  select(-ends_with(\"other\"))\nhead(wino)\n```\n\n\n## Basic Model\n-   Partition\n\n```{r Basic}\nwine_index <- createDataPartition(wino$lprice, p = 0.8, list = FALSE)\nwino_tr <- wino[wine_index, ]\nwino_te <- wino[-wine_index, ]\n```\n\n- We would model the same way, so let's take aside.\n\n## Aside: Factoring\n- Same modelling command\n```\nmx <- train(lprice ~ .,\n  data = wino_tr,\n  method = \"lm\",\n  trControl = trainControl(method = \"repeatedcv\", number = 5, repeats = 3)\n)\n```\n-   I should factor this into a function.\n\n```{r Factor}\ndo_training <- function(df, formula) {\n  train(formula,\n    data = df,\n    method = \"lm\",\n    trControl = trainControl(method = \"repeatedcv\", number = 5, repeats = 3)\n  )\n}\n```\n\n\n## Train vs. test\n\n::::: columns\n\n::: {.column width=\"50%\"}\n\n\n```{r Train 2}\nm2 <- do_training(\n  wino_tr, lprice ~ .\n)\nm2\n```\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n```{r Test 2}\npostResample(\n  pred = predict(m2, wino_te),\n  obs = wino_te$lprice\n)\n```\n\n:::\n\n:::::\n\n\n## Variable Importance\n-   Importance depends on model used...\n\n\n```{r Importance}\nplot(varImp(m2, scale = TRUE))\n```\n\n\n## Variable Importance\n\n-   Each (linear model) coefficient has a standard error, \n    -   Measures certainty of coefficient given data.\n-   For the t-statistic, \n    -   Confidence that the coefficient is different from 0\n    -   We divide the coefficient by the standard error.\n-   If \"small\" error relative to coefficient\n    -   Then \"big\" t-statistic & high feature importance!\n-   What about coefficient as variable importance?\n\n## [Recursive Feature Elimination](https://topepo.github.io/caret/recursive-feature-elimination.html) {.smaller}\n\n1. Tune/train the model on the training set using all predictors.\n2. Calculate model performance.\n3. Calculate variable importance or rankings.\n4. **for** each subset size $S_i$, i = 1...S **do**\n    1. Keep the $S_i$ most important variables.\n    2. [Optional] Pre-process the data.\n    3. Tune/train the model on the training set using $S_i$ predictors.\n    4. Calculate model performance.\n    5. [Optional] Recalculate the rankings for each predictor.\n5. **end**\n6. Calculate the performance profile over the $S_i$.\n7. Determine the appropriate number of predictors.\n8. Use the model corresponding to the optimal $S_i$.\n\n## Size Drop\n-   It did not seem like 2024 `r` could handle 90k wine samples.\n-   We drop to the 1k data set for this demonstration.\n-   There are ways to address performance.\n    -   I say: Out-of-scope.\n\n```{r 1k}\nwino <- wino[sample(nrow(wino), 1000), ]\n```\n\n\n## Partition Again\n-   Partition\n\n```{r Repartition}\nwine_index <- createDataPartition(wino$lprice, p = 0.8, list = FALSE)\nwino_tr <- wino[wine_index, ]\nwino_te <- wino[-wine_index, ]\n```\n\n\n## Caret RFE\n\n\n```{r RFE}\ncontrol <- rfeControl(functions = rfFuncs, method = \"cv\", number = 2)\n# run the RFE algorithm\nresults <- rfe(select(wino_tr, -lprice), wino_tr$lprice, sizes = c(1:3), rfeControl = control)\n# summarize the results\nprint(results)\n```\n\n\n## See Results\n\n```{r See Results}\npredictors(results)\n```\n\n\n## Plot Results\n\n\n```{r Plot Results}\nplot(results)\n```\n\n\n## Practical Workflow\n\n\n```{dot Practice}\n//| echo: false\ndigraph feature_engineering_pipeline {\n    \n    bgcolor=\"#101010\";\n\n    node [\n        fontcolor = \"#e0e0e0\",\n        color = \"#e0e0e0\",\n    ]\n\n    edge [\n        color = \"#e0e0e0\",\n        fontcolor = \"#e0e0e0\"\n    ]\n    node [shape=box];\n    \"Raw Data\" -> \"Lots of Features\" [label=\"Feature Engineering\"];\n    \"Lots of Features\" -> \"Candidate Features\" [label=\"Feature Selection\"];\n    \"Candidate Features\" -> \"Shortlist Features\" [label=\"Expert Input\"];\n    \"Shortlist Features\" -> \"Finalist Models\" [label=\"DS Judgement\"];\n    \"Finalist Models\" -> \"Production\" [label=\"Business Unit\"];\n}\n```\n\n\n## Key Terms\n\n::::: columns\n\n::: {.column width=\"50%\"}\n-   Feature Engineering\n-   Categorical Feature\n-   Continuous Feature\n-   Dummy\n-   Interaction\n:::\n\n::: {.column width=\"50%\"}\n-   Caret\n-   Model\n-   Resampling\n-   Train vs. Test Data\n-   Variable Importance\n:::\n\n:::::\n\n\n# Bonus Slides:<br> Linear Regression\n\n## 5 Assumptions of Linear Regression\n\n-   Linear regressions have a well-developed statistical theory.\n\n-   This brings perks like confidence intervals on predictions.\n\n-   It also has \"costs\" in that assumptions need to be satisfied.\n\n## The Five\n\n1. Linearity\n2. Constant variance\n3. Normality\n4. Imperfect multicollinearity\n5. Exogeneity\n\n## 1.  Linearity \n\n-   **The dependent variable is a linear combination of the features.**\n\n-   This is less of big deal than it might seem! \n\n-   If y is actually quadratic in x, then y is linear in x\\^2! \n    -   That's feature engineering.\n\n## 2.  Constant variance\n\n-   Or *homoscedasticity*\n\n-   **The variance of the errors do not depend on the values of the features.**\n\n-   Don't make bigger prediction errors for some values of x than for others.\n\n## 3.  Normality \n\n-   **The errors should be independent and normally distributed.**\n\n-   A scatter plot of target variable value and residual (model error) should look like white noise.\n\n## 4.  Lack of perfect multicollinearity \n\n-   **None predictors should be a perfect linear combination of others.**\n\n-   This can happen if you over-engineer features\n    -   This is uncommon. \n    -   You'll see an error that your coefficient matrix is singular or something.\n\n## 5.  Exogeneity \n\n-   **Model errors should be independent of the values of the features.**\n\n-   In particular, errors should have mean zero. \n\n-   It's always good to look at a histogram of your residuals (see also normality).\n\n## First Test\n\n-   Determine whether the errors are normally distributed, like Shapiro-Wilk (also, plot them).\n\n![](images/residuals.png)\n\n## 5 Assumptions of Linear Regression: testing\n\n-   Second I would always look at fitted value vs. residual to check homoscedasticity.\n\n![](images/heterosceda.png)\n\n![](images/homosceda.png)\n\n-   For more, see for example https://people.duke.edu/\\~rnau/testing.htm\n\n",
    "supporting": [
      "Python02_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}