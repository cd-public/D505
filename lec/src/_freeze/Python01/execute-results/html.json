{
  "hash": "55fed42bcb953d03ef1637ff34cfadfe",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Machine Learning in Python\"\nsubtitle: \"Applied Machine Learning\"\nauthor: \"Calvin\"\n  \njupyter: python3\n\nexecute:\n    echo: true\n    cache: true\n    freeze: true  # never re-render during project render\n---\n\n\n## Prerequisite\n\n- I assume a Python/VS Code/Quarto workflow.\n    - Review [this document](Python00.html)\n- I assume familiarity with Machine Learning in R\n    - Review [these slides](Session01.html)\n\n\n## Agenda\n\n1.  Python Overview\n2.  Review of Regression\n3.  Classification\n4.  Basic Feature Engineering\n\n## Quarto\n\n-   I switch from a R backend to a Python backend.\n-   I add the following below my title in my .qmd header:\n    -   `jupyter: python3`\n\n```yml         \ntitle: \"Machine Learning in Python\"\nsubtitle: \"Applied Machine Learning\"\nauthor: \"Calvin\"\n  \njupyter: python3\n```\n\n## Pip\n\n-   In Python, we can typically install packages via `pip`\n-   It is more typical to use `pip` at commandline.\n\n``` bash\npython -m pip install sampleproject\n```\n\n-   Here is a 'clean' way to do so from within the Python\n\n::: {#77ae9743 .cell execution_count=2}\n``` {.python .cell-code}\nimport subprocess  # A base package we need to install other packages\nimport sys         # A base package we need to install other packages\ninstall = lambda package : subprocess.check_call([sys.executable, \n                                                  '-m', \n                                                  'pip', \n                                                  'install', \n                                                  package])\n```\n:::\n\n\n## Packages\n\n-   I'll build a list of packages then install them via a loop.\n    -   Some (numpy, matplotlib) required for Quarto.\n\n::: {#80f989bc .cell execution_count=3}\n``` {.python .cell-code}\npython_data_stack = [\"numpy\", \"pandas\", \"matplotlib\", \"seaborn\"]\nr_data_stack = ['pyreadr', 'statsmodels']\npackages = python_data_stack + r_data_stack + [\"scikit-learn\"]\n\n_ = [install(package) for package in packages]\n```\n:::\n\n\n-   I use `_ =` to discard the result of the process.\n    -   This ignores errors - remove to debug.\n\n## Import\n\n-   Python packages use `import` rather than `library()`\n-   Python base data stack\n\n::: {#c45197d4 .cell execution_count=4}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n```\n:::\n\n\n-   R & statistics\n\n::: {#1d2108fa .cell execution_count=5}\n``` {.python .cell-code}\nimport pyreadr\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n```\n:::\n\n\n## ML Library\n-   There are 3.5 Python ML libraries that matter\n    -   Scikit-learn, mainline ML\n    -   Torch, deep learning\n    -   Tensorflow, deep learning\n    -   PySpark MLlib, MLOps\n\n::: {#da3c4df8 .cell execution_count=6}\n``` {.python .cell-code}\n# Common to include parts of, not all of, sklearn\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n```\n:::\n\n\n## Many packages\n-   There's a lot of imports here, we could cut back but...\n-   We would typical use only `sklearn` not `stats_models`\n    -   SM=statisical learning, SK=machine learning\n    -   Use SM for equivalence with R.\n-   Typically use the module name as a function's prefix.\n    -   This resolves name collisions, like `dplyr` and `stats`\n\n    ``` r\n    ✖ dplyr::filter() masks stats::filter()\n    ✖ dplyr::lag()    masks stats::lag()\n    ```\n\n## Setup\n\n-   We will use `pyreadr` to read in an R dataset.\n    -   This is atypical but not difficult.\n\n::: {#bf98a59c .cell execution_count=7}\n``` {.python .cell-code}\nurl = 'https://cd-public.github.io/courses/rmls25/dat/'\nrds = 'wine.rds'\npyreadr.download_file(url + rds, rds) \nwine = pyreadr.read_r(rds)[None]      \nwine.dropna(subset=['points','price'])\nstr(wine.info()) # string for slide formatting\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 89556 entries, 0 to 89555\nData columns (total 15 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   id                     89556 non-null  float64\n 1   country                89503 non-null  object \n 2   description            89556 non-null  object \n 3   designation            64356 non-null  object \n 4   points                 89556 non-null  float64\n 5   price                  89556 non-null  float64\n 6   province               89503 non-null  object \n 7   region_1               71712 non-null  object \n 8   region_2               33081 non-null  object \n 9   taster_name            89556 non-null  object \n 10  taster_twitter_handle  84953 non-null  object \n 11  title                  89556 non-null  object \n 12  variety                89556 non-null  object \n 13  winery                 89556 non-null  object \n 14  year                   89556 non-null  float64\ndtypes: float64(4), object(11)\nmemory usage: 10.2+ MB\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n'None'\n```\n:::\n:::\n\n\n# Review of Regression\n\n## Single Variable\n\n-   Pick the poshest province.\n\n::: {#eeae1ad2 .cell execution_count=8}\n``` {.python .cell-code}\nwine['bordeaux'] = wine['province'] == 'Bordeaux'\nwine.head(2)\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>country</th>\n      <th>description</th>\n      <th>designation</th>\n      <th>points</th>\n      <th>price</th>\n      <th>province</th>\n      <th>region_1</th>\n      <th>region_2</th>\n      <th>taster_name</th>\n      <th>taster_twitter_handle</th>\n      <th>title</th>\n      <th>variety</th>\n      <th>winery</th>\n      <th>year</th>\n      <th>bordeaux</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>Portugal</td>\n      <td>This is ripe and fruity, a wine that is smooth...</td>\n      <td>Avidagos</td>\n      <td>87.0</td>\n      <td>15.0</td>\n      <td>Douro</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Roger Voss</td>\n      <td>@vossroger</td>\n      <td>Quinta dos Avidagos 2011 Avidagos Red (Douro)</td>\n      <td>Portuguese Red</td>\n      <td>Quinta dos Avidagos</td>\n      <td>2011.0</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.0</td>\n      <td>US</td>\n      <td>Tart and snappy, the flavors of lime flesh and...</td>\n      <td>NaN</td>\n      <td>87.0</td>\n      <td>14.0</td>\n      <td>Oregon</td>\n      <td>Willamette Valley</td>\n      <td>Willamette Valley</td>\n      <td>Paul Gregutt</td>\n      <td>@paulgwine</td>\n      <td>Rainstorm 2013 Pinot Gris (Willamette Valley)</td>\n      <td>Pinot Gris</td>\n      <td>Rainstorm</td>\n      <td>2013.0</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Regress\n\n-   Take a quick regression model over the wine.\n\n::: {#e480d6a6 .cell execution_count=9}\n``` {.python .cell-code}\nm1 = sm.OLS(wine['points'], wine['price']).fit()\nm1.summary()\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>         <td>points</td>      <th>  R-squared (uncentered):</th>       <td>   0.414</td>  \n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared (uncentered):</th>  <td>   0.414</td>  \n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>           <td>6.331e+04</td> \n</tr>\n<tr>\n  <th>Date:</th>             <td>Fri, 27 Dec 2024</td> <th>  Prob (F-statistic):</th>            <td>  0.00</td>   \n</tr>\n<tr>\n  <th>Time:</th>                 <td>01:13:50</td>     <th>  Log-Likelihood:    </th>          <td>-5.0481e+05</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td> 89556</td>      <th>  AIC:               </th>           <td>1.010e+06</td> \n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td> 89555</td>      <th>  BIC:               </th>           <td>1.010e+06</td> \n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>               <td> </td>     \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>               <td> </td>     \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>price</th> <td>    1.0169</td> <td>    0.004</td> <td>  251.623</td> <td> 0.000</td> <td>    1.009</td> <td>    1.025</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>199847.674</td> <th>  Durbin-Watson:     </th>    <td>   0.598</td>   \n</tr>\n<tr>\n  <th>Prob(Omnibus):</th>   <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>3451086699.000</td>\n</tr>\n<tr>\n  <th>Skew:</th>            <td>-20.492</td>  <th>  Prob(JB):          </th>    <td>    0.00</td>   \n</tr>\n<tr>\n  <th>Kurtosis:</th>        <td>963.819</td>  <th>  Cond. No.          </th>    <td>    1.00</td>   \n</tr>\n</table><br/><br/>Notes:<br/>[1] R² is computed without centering (uncentered) since the model does not contain a constant.<br/>[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\n## Sklearn\n\n-   I'm much fonder of `sklearn` for this sort of thing.\n\n::: {#091fe4ed .cell execution_count=10}\n``` {.python .cell-code}\ns1 = LinearRegression().fit(wine[['points']],wine['price'])\nf'Intercept: {s1.intercept_}, Coefficients: {s1.coef_}'\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n'Intercept: -489.2508038352233, Coefficients: [5.92019824]'\n```\n:::\n:::\n\n\n## Let's draw it\n\n:::: {.columns}\n\n::: {.column width='50%'}\n-   Python ggplot is `plotnine`\n-   I prefer Seaborn (`sns`)\n-   Also: `matplotlib`\n-   Also: `plotly`\n:::\n\n::: {.column width='50%'}\n\n::: {#44942438 .cell execution_count=11}\n``` {.python .cell-code}\nsns.lmplot(x='points', y='price', \n           data=wine, scatter=True, \n           x_bins=range(80,101))\n```\n\n::: {.cell-output .cell-output-display}\n![](Python01_files/figure-revealjs/cell-11-output-1.png){width=470 height=470}\n:::\n:::\n\n\n:::\n\n::::\n\n## Multiple regression\n\n-   We can literally use the exact same formula in `smf`\n-   I am unaccustomed to doing multiple regressions without interaction.\n    -   Moving on.\n\n::: {#27474c2a .cell execution_count=12}\n``` {.python .cell-code}\nm2 = smf.ols('price ~ points + bordeaux', data=wine).fit()\nm2.summary()\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th>  <td>   0.165</td>  \n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.165</td>  \n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   8857.</td>  \n</tr>\n<tr>\n  <th>Date:</th>             <td>Fri, 27 Dec 2024</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n</tr>\n<tr>\n  <th>Time:</th>                 <td>01:13:56</td>     <th>  Log-Likelihood:    </th> <td>-4.5673e+05</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td> 89556</td>      <th>  AIC:               </th>  <td>9.135e+05</td> \n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td> 89553</td>      <th>  BIC:               </th>  <td>9.135e+05</td> \n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>      <td> </td>     \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Intercept</th>        <td> -491.8296</td> <td>    3.970</td> <td> -123.897</td> <td> 0.000</td> <td> -499.610</td> <td> -484.049</td>\n</tr>\n<tr>\n  <th>bordeaux[T.True]</th> <td>    8.7090</td> <td>    0.661</td> <td>   13.182</td> <td> 0.000</td> <td>    7.414</td> <td>   10.004</td>\n</tr>\n<tr>\n  <th>points</th>           <td>    5.9451</td> <td>    0.045</td> <td>  132.878</td> <td> 0.000</td> <td>    5.857</td> <td>    6.033</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>209113.996</td> <th>  Durbin-Watson:     </th>    <td>   1.653</td>   \n</tr>\n<tr>\n  <th>Prob(Omnibus):</th>   <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>5148121806.319</td>\n</tr>\n<tr>\n  <th>Skew:</th>            <td>22.820</td>   <th>  Prob(JB):          </th>    <td>    0.00</td>   \n</tr>\n<tr>\n  <th>Kurtosis:</th>       <td>1176.693</td>  <th>  Cond. No.          </th>    <td>2.66e+03</td>   \n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.66e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems.\n```\n:::\n:::\n\n\n## How about with an interaction?\n\n::: {#6e27719b .cell execution_count=13}\n``` {.python .cell-code}\nm3 = smf.ols('price ~ points * bordeaux', data=wine).fit()\nm3.summary()\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```{=html}\n<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th>  <td>   0.177</td>  \n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.177</td>  \n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   6421.</td>  \n</tr>\n<tr>\n  <th>Date:</th>             <td>Fri, 27 Dec 2024</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n</tr>\n<tr>\n  <th>Time:</th>                 <td>01:13:56</td>     <th>  Log-Likelihood:    </th> <td>-4.5609e+05</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td> 89556</td>      <th>  AIC:               </th>  <td>9.122e+05</td> \n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td> 89552</td>      <th>  BIC:               </th>  <td>9.122e+05</td> \n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>      <td> </td>     \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n             <td></td>                <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Intercept</th>               <td> -460.8522</td> <td>    4.034</td> <td> -114.233</td> <td> 0.000</td> <td> -468.759</td> <td> -452.945</td>\n</tr>\n<tr>\n  <th>bordeaux[T.True]</th>        <td> -665.8277</td> <td>   18.763</td> <td>  -35.486</td> <td> 0.000</td> <td> -702.604</td> <td> -629.052</td>\n</tr>\n<tr>\n  <th>points</th>                  <td>    5.5958</td> <td>    0.045</td> <td>  123.063</td> <td> 0.000</td> <td>    5.507</td> <td>    5.685</td>\n</tr>\n<tr>\n  <th>points:bordeaux[T.True]</th> <td>    7.6585</td> <td>    0.213</td> <td>   35.972</td> <td> 0.000</td> <td>    7.241</td> <td>    8.076</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>207559.719</td> <th>  Durbin-Watson:     </th>    <td>   1.666</td>   \n</tr>\n<tr>\n  <th>Prob(Omnibus):</th>   <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>5054712547.121</td>\n</tr>\n<tr>\n  <th>Skew:</th>            <td>22.390</td>   <th>  Prob(JB):          </th>    <td>    0.00</td>   \n</tr>\n<tr>\n  <th>Kurtosis:</th>       <td>1166.014</td>  <th>  Cond. No.          </th>    <td>1.27e+04</td>   \n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.27e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems.\n```\n:::\n:::\n\n\n## Sklearinteraction\n\n-   `sklearn` defaults to multiple regression.\n\n::: {#1d727b2f .cell execution_count=14}\n``` {.python .cell-code}\nwine['points_bordeaux'] = wine['points'] * wine['bordeaux']\n\ns3 = LinearRegression().fit(wine[['points','bordeaux']],\n                            wine['price'])\nf'Intercept: {s3.intercept_}, Coefficients: {s3.coef_}'\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\n'Intercept: -491.829643567971, Coefficients: [5.9451492  8.70900299]'\n```\n:::\n:::\n\n\n## Let's draw it\n\n::: {#d418daa6 .cell execution_count=15}\n``` {.python .cell-code}\nsns.lmplot(x='points', y='price', \n           data=wine, scatter=True, \n           x_bins=range(80,101), hue='bordeaux')\n```\n\n::: {.cell-output .cell-output-display}\n![](Python01_files/figure-revealjs/cell-15-output-1.png){width=546 height=470}\n:::\n:::\n\n\n# Moving to an ML framework\n\n## Split sample using Sklearn\n-   Vs R, with a proliferation of libraries, Python ML is concentrated.\n-   There are a few main libraries, `Sklearn` the first and most popular\n-   So modelling and partitioning are all in `sklearn`, vs `stats` and `caret`\n\n::: {#cbd21b72 .cell execution_count=16}\n``` {.python .cell-code}\nrandom_seed = 505\ntrain, test = train_test_split(wine, random_state=random_seed)\nprint(test.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             id   country                                        description  \\\n65856   95817.0        US  The appellation continues to impress with this...   \n71192  103509.0   Romania  This easy-drinking red wine has aromas of cher...   \n29519   43024.0  Portugal  A blend of Tinta Roriz and the local Jaen, thi...   \n23597   34478.0        US  This is the producer's annual unoaked version ...   \n42843   62706.0   Germany  Delicate quince and apple aromas are fringed w...   \n\n                     designation  points  price         province  \\\n65856              Frank Johnson    90.0   14.0       California   \n71192                  Dreambird    86.0    7.0  Viile Timisului   \n29519       Beyra Colheita Tinto    87.0   12.0   Beira Interior   \n23597  Acero Don Miguel Vineyard    88.0   29.0       California   \n42843                        NaN    87.0   13.0            Mosel   \n\n                   region_1 region_2         taster_name  \\\n65856      Dry Creek Valley   Sonoma      Virginie Boone   \n71192                   NaN      NaN        Jeff Jenssen   \n29519                   NaN      NaN          Roger Voss   \n23597  Russian River Valley   Sonoma      Virginie Boone   \n42843                   NaN      NaN  Anna Lee C. Iijima   \n\n      taster_twitter_handle  \\\n65856               @vboone   \n71192        @worldwineguys   \n29519            @vossroger   \n23597               @vboone   \n42843                   NaN   \n\n                                                   title          variety  \\\n65856  Wines Gone Wild 2014 Frank Johnson Sauvignon B...  Sauvignon Blanc   \n71192  Cramele Recas 2014 Dreambird Merlot (Viile Tim...           Merlot   \n29519  Rui Roboredo Madeira 2014 Beyra Colheita Tinto...   Portuguese Red   \n23597  Marimar Estate 2014 Acero Don Miguel Vineyard ...       Chardonnay   \n42843              Ulrich Langguth 2012 Riesling (Mosel)         Riesling   \n\n                     winery    year  bordeaux  points_bordeaux  \n65856       Wines Gone Wild  2014.0     False              0.0  \n71192         Cramele Recas  2014.0     False              0.0  \n29519  Rui Roboredo Madeira  2014.0     False              0.0  \n23597        Marimar Estate  2014.0     False              0.0  \n42843       Ulrich Langguth  2012.0     False              0.0  \n```\n:::\n:::\n\n\n## Compare RMSE across models\n\n-   Retrain the models on the training set only\n\n::: {#2ea85b8f .cell execution_count=17}\n``` {.python .cell-code}\nformulas = ['price ~ points', \n            'price ~ points + bordeaux', \n            'price ~ points * bordeaux']\nms = [smf.ols(f, data=train).fit() for f in formulas]\n```\n:::\n\n\n-   Test them all under the same conditions.\n\n::: {#451bea2a .cell execution_count=18}\n``` {.python .cell-code}\ndef rmse(m):\n  residuals_sq = (test['price'] - m.predict(test)) ** 2\n  mse = np.mean(residuals_sq)\n  return mse ** .5\n\n[rmse(m) for m in ms]\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\n[np.float64(38.253198876742374),\n np.float64(38.208086530739),\n np.float64(37.80098647775707)]\n```\n:::\n:::\n\n\n## Exercise Problems\n\n-   *Bonus++. Pick a non-Bordeaux category.*\n    -   We write a quick reserve-checker and apply it.\n    -   Basically Python wasn't told all the data was textual\n        -   Use `str` and `lower`\n    -   Then use Pythonic `in` with a list of names\n-   `apply` is like `mutate`, in a way.\n\n::: {#cfcda512 .cell execution_count=19}\n``` {.python .cell-code}\nis_reserve = lambda x: str(x).lower() in ['reserve','reserva','riserva']\nwine['reserve'] = wine['designation'].apply(is_reserve)\n```\n:::\n\n\n## via Seaborn\n\n::: {#c4e8af69 .cell execution_count=20}\n``` {.python .cell-code}\nsns.lmplot(wine, x='points', y='price', x_bins=range(80,101), hue='reserve',\n           x_estimator=np.mean, order=2) # polynomail regression, why not\n```\n\n::: {.cell-output .cell-output-display}\n![](Python01_files/figure-revealjs/cell-20-output-1.png){width=548 height=470}\n:::\n:::\n\n\n# Classification and Ethics\n\n## The math of it...\n\n-   This problem used a R library to get data.\n-   That R library is open source, \n    -   It has reasonable documentation, and\n    -   It's GitHub is linked from documentation.\n-   I spent 30 seconds on a search engine to find it.\n\n## Partition our Data\n-   The `.rda` file is at this url:\n    -   [https://github.com/rafalab/dslabs/blob/master/data/heights.rda](https://github.com/rafalab/dslabs/blob/master/data/heights.rda){style=\"font-size:smaller\"}\n    -   Change `blob` to `raw` to [download directly](https://github.com/rafalab/dslabs/raw/master/data/heights.rda).\n\n::: {#5f70baf4 .cell execution_count=21}\n``` {.python .cell-code}\nurl = 'https://github.com/rafalab/dslabs/raw/master/data/'\nrds = 'heights.rda'\npyreadr.download_file(url + rds, rds) \nheights = pyreadr.read_r(rds)['heights']      \nrandom_seed = 505\ntrain, test = train_test_split(heights, random_state=random_seed)\n```\n:::\n\n\nNote: this vignette is adapted from [this book](https://rafalab.github.io/dsbook/introduction-to-machine-learning.html)\n\n## Guessing\n-   Let’s start by developing the simplest possible machine algorithm: guessing the outcome.\n\n::: {#ee34d70d .cell execution_count=22}\n``` {.python .cell-code}\ny_hat = np.random.choice(['Male', 'Female'], len(test))\n```\n:::\n\n\nRecall:\n\n>[Y hat (written ŷ ) is the predicted value of y (the dependent variable) in a regression equation. It can also be considered to be the average value of the response variable.](https://www.statisticshowto.com/y-hat-definition/)\n\n## Accuracy\n-   The overall accuracy is simply defined as the overall proportion that is predicted correctly:\n\n::: {#a1545014 .cell execution_count=23}\n``` {.python .cell-code}\nnp.mean(y_hat == test['sex'])\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\nnp.float64(0.49809885931558934)\n```\n:::\n:::\n\n\n-   What would we have expected the accuracy to be?\n    -   What much would we have expected accuracy to deviate from that expectionation?\n\n## Let's do better...\n\n::: {#42155306 .cell execution_count=24}\n``` {.python .cell-code}\nhs = heights.groupby('sex')['height']\nhs.std(), hs.mean()\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```\n(sex\n Female    3.760656\n Male      3.611024\n Name: height, dtype: float64,\n sex\n Female    64.939424\n Male      69.314755\n Name: height, dtype: float64)\n```\n:::\n:::\n\n\n## A simple predictive model\n\n-   Idea: Predict `'Male'` if observation is within 2 standard deviations\n\n::: {#90371d31 .cell execution_count=25}\n``` {.python .cell-code}\nmale_mean_less_2sd = hs.mean()['Male'] - 2*hs.std()['Male']\n\ny_hat = heights['height'].apply(lambda x : 'Male' if x > male_mean_less_2sd else 'Female')\n\nprint(male_mean_less_2sd, np.mean(heights['sex'] == y_hat))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n62.09270759210095 0.7933333333333333\n```\n:::\n:::\n\n\n-   The accuracy goes up from \\~0.50 to about \\~0.80!!\n\n## Let's optimize\n\n::: {#0b55679c .cell execution_count=26}\n``` {.python .cell-code}\ncutoff = list(range(61,71))\n\ndef get_accuracy(y):\n  f = lambda x : 'Male' if x > y else 'Female'\n  y_hat = heights['height'].apply(f)\n  return np.mean(heights['sex'] == y_hat)\n\naccuracy = list(map(get_accuracy, cutoff))\n\naccuracy\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```\n[np.float64(0.7819047619047619),\n np.float64(0.7933333333333333),\n np.float64(0.8085714285714286),\n np.float64(0.8266666666666667),\n np.float64(0.8266666666666667),\n np.float64(0.8019047619047619),\n np.float64(0.7371428571428571),\n np.float64(0.6819047619047619),\n np.float64(0.5914285714285714),\n np.float64(0.5085714285714286)]\n```\n:::\n:::\n\n\n## Optimal Cutoff\n\n::: {#a8f4f5e2 .cell execution_count=27}\n``` {.python .cell-code}\nbest_cutoff = cutoff[np.argmax(accuracy)]\n_ = plt.plot(cutoff, accuracy), print('Optimal cutoff is', best_cutoff)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimal cutoff is 64\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Python01_files/figure-revealjs/cell-27-output-2.png){width=802 height=411}\n:::\n:::\n\n\n-   Should we be cutting at an integer?\n\n## Apply & Evaluate\n\n::: {#d378b323 .cell execution_count=28}\n``` {.python .cell-code}\ntest['y_hat'] = test['height'].apply(lambda x : 'Male' if x > best_cutoff else 'Female')\nprint('Accuracy is', np.mean(test['sex'] == test['y_hat']))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy is 0.8365019011406845\n```\n:::\n:::\n\n\n## Confusion matrix\n\n::: {#60438092 .cell execution_count=29}\n``` {.python .cell-code}\nConfusionMatrixDisplay.from_predictions(test['sex'], test['y_hat'])\n```\n\n::: {.cell-output .cell-output-display}\n![](Python01_files/figure-revealjs/cell-29-output-1.png){width=555 height=429}\n:::\n:::\n\n\n*Took 7 lines / 350+ characters of handwritten ggplot*\n\n## Accuracy by sex\n\n::: {#cd5cfbcd .cell execution_count=30}\n``` {.python .cell-code}\ntest['acc'] = test['sex'] == test['y_hat']\ntest.groupby('sex')['acc'].mean()\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n```\nsex\nFemale    0.423077\nMale      0.938389\nName: acc, dtype: float64\n```\n:::\n:::\n\n\nIt's raining men.\n\n## Debrief\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {#f5198ce0 .cell execution_count=31}\n``` {.python .cell-code}\n_ = sns.boxplot(heights, x='sex',y='height')\n```\n\n::: {.cell-output .cell-output-display}\n![](Python01_files/figure-revealjs/cell-31-output-1.png){width=808 height=429}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {#d6f6a10b .cell execution_count=32}\n``` {.python .cell-code}\n_ = plt.pie(heights['sex'].value_counts(), labels=['♀','♂'])\n```\n\n::: {.cell-output .cell-output-display}\n![](Python01_files/figure-revealjs/cell-32-output-1.png){width=389 height=389}\n:::\n:::\n\n\n:::\n\n::::\n\n# Machine Learning in Python\n\n",
    "supporting": [
      "Python01_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}