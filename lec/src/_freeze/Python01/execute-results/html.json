{
  "hash": "452635357438df7fcf6712f48cd154e0",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Machine Learning in Python\"\nsubtitle: \"Applied Machine Learning\"\nauthor: \"Calvin\"\n  \njupyter: python3\n\nexecute:\n    echo: true\n    freeze: true  # never re-render during project render\n---\n\n\n\n## Agenda\n\n1. Python Overview\n2. Review of Regression\n3. Classification\n4. Basic Feature Engineering\n\n## Quarto\n\n-   I switch from a R backend to a Python backend.\n-   I add the following below my title in my .qmd header:\n```\njupyter: python3\n```\n\n\n\n## Pip\n\n-   In Python, we can typically install packages via `pip`\n-   It is much typical to use `pip` at commandline.\n```bash\npython -m pip install sampleproject\n```\n-   Here is a \"clean\" way to do so from within the Python\n\n::: {#6175f439 .cell execution_count=1}\n``` {.python .cell-code}\nimport subprocess  # A base package we need to install other packages\nimport sys         # A base package we need to install other packages\ninstall = lambda package : subprocess.check_call([sys.executable, \n                                                  \"-m\", \n                                                  \"pip\", \n                                                  \"install\", \n                                                  package])\n```\n:::\n\n\n## Packages\n\n-   I'll build a list of packages at install them via a loop.\n\n::: {#92413d2b .cell execution_count=2}\n``` {.python .cell-code}\npackages = [\"pyreadr\"]\n\n_ = [install(package) for package in packages]\n```\n:::\n\n\n-   I use `_ =` to discard the result of the installation process.\n    -   Stores cell result in a variable.\n    -   I ignore the variable.\n\n## Import\n-   Python packages use `import` rather than `library()`\n\n::: {#46d24d7f .cell execution_count=3}\n``` {.python .cell-code}\nimport pyreadr\n```\n:::\n\n\n-   Typically use the module name as a function's prefix.\n    -   This resolves name collisions, like `dplyr` and `stats`\n```r\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n\n## Setup\n\n- We will work with a `wine` dataset that is enormous.\n\n::: {#e37cb98f .cell execution_count=4}\n``` {.python .cell-code}\nurl = \"https://cd-public.github.io/courses/rmls25/dat/\"\nrds = \"wine.rds\" # or \"w_1k.rds\"\npyreadr.download_file(url + rds, rds)\nwine = pyreadr.read_r(rds)[None]\nwine\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>country</th>\n      <th>description</th>\n      <th>designation</th>\n      <th>points</th>\n      <th>price</th>\n      <th>province</th>\n      <th>region_1</th>\n      <th>region_2</th>\n      <th>taster_name</th>\n      <th>taster_twitter_handle</th>\n      <th>title</th>\n      <th>variety</th>\n      <th>winery</th>\n      <th>year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>Portugal</td>\n      <td>This is ripe and fruity, a wine that is smooth...</td>\n      <td>Avidagos</td>\n      <td>87.0</td>\n      <td>15.0</td>\n      <td>Douro</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Roger Voss</td>\n      <td>@vossroger</td>\n      <td>Quinta dos Avidagos 2011 Avidagos Red (Douro)</td>\n      <td>Portuguese Red</td>\n      <td>Quinta dos Avidagos</td>\n      <td>2011.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.0</td>\n      <td>US</td>\n      <td>Tart and snappy, the flavors of lime flesh and...</td>\n      <td>NaN</td>\n      <td>87.0</td>\n      <td>14.0</td>\n      <td>Oregon</td>\n      <td>Willamette Valley</td>\n      <td>Willamette Valley</td>\n      <td>Paul Gregutt</td>\n      <td>@paulgwine</td>\n      <td>Rainstorm 2013 Pinot Gris (Willamette Valley)</td>\n      <td>Pinot Gris</td>\n      <td>Rainstorm</td>\n      <td>2013.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.0</td>\n      <td>US</td>\n      <td>Pineapple rind, lemon pith and orange blossom ...</td>\n      <td>Reserve Late Harvest</td>\n      <td>87.0</td>\n      <td>13.0</td>\n      <td>Michigan</td>\n      <td>Lake Michigan Shore</td>\n      <td>NaN</td>\n      <td>Alexander Peartree</td>\n      <td>NaN</td>\n      <td>St. Julian 2013 Reserve Late Harvest Riesling ...</td>\n      <td>Riesling</td>\n      <td>St. Julian</td>\n      <td>2013.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.0</td>\n      <td>US</td>\n      <td>Much like the regular bottling from 2012, this...</td>\n      <td>Vintner's Reserve Wild Child Block</td>\n      <td>87.0</td>\n      <td>65.0</td>\n      <td>Oregon</td>\n      <td>Willamette Valley</td>\n      <td>Willamette Valley</td>\n      <td>Paul Gregutt</td>\n      <td>@paulgwine</td>\n      <td>Sweet Cheeks 2012 Vintner's Reserve Wild Child...</td>\n      <td>Pinot Noir</td>\n      <td>Sweet Cheeks</td>\n      <td>2012.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>Spain</td>\n      <td>Blackberry and raspberry aromas show a typical...</td>\n      <td>Ars In Vitro</td>\n      <td>87.0</td>\n      <td>15.0</td>\n      <td>Northern Spain</td>\n      <td>Navarra</td>\n      <td>NaN</td>\n      <td>Michael Schachner</td>\n      <td>@wineschach</td>\n      <td>Tandem 2011 Ars In Vitro Tempranillo-Merlot (N...</td>\n      <td>Tempranillo-Merlot</td>\n      <td>Tandem</td>\n      <td>2011.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>89551</th>\n      <td>129966.0</td>\n      <td>Germany</td>\n      <td>Notes of honeysuckle and cantaloupe sweeten th...</td>\n      <td>Brauneberger Juffer-Sonnenuhr Spätlese</td>\n      <td>90.0</td>\n      <td>28.0</td>\n      <td>Mosel</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Anna Lee C. Iijima</td>\n      <td>NaN</td>\n      <td>Dr. H. Thanisch (Erben Müller-Burggraef) 2013 ...</td>\n      <td>Riesling</td>\n      <td>Dr. H. Thanisch (Erben Müller-Burggraef)</td>\n      <td>2013.0</td>\n    </tr>\n    <tr>\n      <th>89552</th>\n      <td>129967.0</td>\n      <td>US</td>\n      <td>Citation is given as much as a decade of bottl...</td>\n      <td>NaN</td>\n      <td>90.0</td>\n      <td>75.0</td>\n      <td>Oregon</td>\n      <td>Oregon</td>\n      <td>Oregon Other</td>\n      <td>Paul Gregutt</td>\n      <td>@paulgwine</td>\n      <td>Citation 2004 Pinot Noir (Oregon)</td>\n      <td>Pinot Noir</td>\n      <td>Citation</td>\n      <td>2004.0</td>\n    </tr>\n    <tr>\n      <th>89553</th>\n      <td>129968.0</td>\n      <td>France</td>\n      <td>Well-drained gravel soil gives this wine its c...</td>\n      <td>Kritt</td>\n      <td>90.0</td>\n      <td>30.0</td>\n      <td>Alsace</td>\n      <td>Alsace</td>\n      <td>NaN</td>\n      <td>Roger Voss</td>\n      <td>@vossroger</td>\n      <td>Domaine Gresser 2013 Kritt Gewurztraminer (Als...</td>\n      <td>Gewürztraminer</td>\n      <td>Domaine Gresser</td>\n      <td>2013.0</td>\n    </tr>\n    <tr>\n      <th>89554</th>\n      <td>129969.0</td>\n      <td>France</td>\n      <td>A dry style of Pinot Gris, this is crisp with ...</td>\n      <td>NaN</td>\n      <td>90.0</td>\n      <td>32.0</td>\n      <td>Alsace</td>\n      <td>Alsace</td>\n      <td>NaN</td>\n      <td>Roger Voss</td>\n      <td>@vossroger</td>\n      <td>Domaine Marcel Deiss 2012 Pinot Gris (Alsace)</td>\n      <td>Pinot Gris</td>\n      <td>Domaine Marcel Deiss</td>\n      <td>2012.0</td>\n    </tr>\n    <tr>\n      <th>89555</th>\n      <td>129970.0</td>\n      <td>France</td>\n      <td>Big, rich and off-dry, this is powered by inte...</td>\n      <td>Lieu-dit Harth Cuvée Caroline</td>\n      <td>90.0</td>\n      <td>21.0</td>\n      <td>Alsace</td>\n      <td>Alsace</td>\n      <td>NaN</td>\n      <td>Roger Voss</td>\n      <td>@vossroger</td>\n      <td>Domaine Schoffit 2012 Lieu-dit Harth Cuvée Car...</td>\n      <td>Gewürztraminer</td>\n      <td>Domaine Schoffit</td>\n      <td>2012.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>89556 rows × 15 columns</p>\n</div>\n```\n:::\n:::\n\n\n# Review of Regression\n\n## Single Variable\n\n- Pick the poshest province.\n\n```{r Single Variable}\nwine <- wine %>%\n  mutate(bordeaux = (province == \"Bordeaux\"))\nwine <- wine %>% drop_na(bordeaux)\ntop_n(wine, 10, bordeaux)\n```\n\n\n## Regress\n- Take a quick regression model over the wine.\n\n```{r Regress}\nm1 <- lm(price ~ points, data = wine)\nget_regression_table(m1)\n```\n\n## Let's draw it\n\n\n```{r Lets draw it 1}\n#| echo: false\nwine %>%\n  mutate(m1 = predict(m1)) %>%\n  ggplot() +\n  geom_smooth(aes(points, price)) +\n  geom_line(aes(points, m1), size = 2, color = \"orange\") +\n  labs(\n    title = \"Regression of Price on Points\",\n    x = \"Points\", y = \"Price\"\n  )\n```\n\n\n## Multiple regression\n\n\n```{r Multiple regression}\nm2 <- lm(price ~ points + bordeaux, data = wine)\nget_regression_table(m2)\n```\n\n\n## Let's draw it\n\n\n```{r Lets draw it 2}\n#| echo: false\nwine %>%\n  mutate(m2 = predict(m2)) %>%\n  ggplot() +\n  geom_smooth(aes(points, price)) +\n  geom_line(aes(x = points, y = m2, color = bordeaux), size = 2) +\n  labs(\n    title = \"Multiple Regression of Price on Points and Bordeaux\",\n    x = \"Points\", y = \"Price\"\n  )\n```\n\n\n## How about with an interaction?\n\n\n```{r Interaction}\nm3 <- lm(price ~ points * bordeaux, data = wine)\nget_regression_table(m3)\n```\n\n\n## Let's draw it\n\n```{r Lets draw it 3}\n#| echo: false\nwine %>%\n  mutate(m3 = predict(m3)) %>%\n  ggplot() +\n  geom_smooth(aes(points, price)) +\n  geom_line(aes(x = points, y = m3, color = bordeaux), size = 2) +\n  labs(\n    title = \"Interaction Model: Price on Points and Bordeaux\",\n    x = \"Points\", y = \"Price\"\n  )\n```\n\n\n\n## Model diagnostics \n\n\n```{r Diagnostics}\nget_regression_summaries(m1)\nget_regression_summaries(m2)\nget_regression_summaries(m3)\n```\n\n\n# Moving to an ML framework\n\n## Split sample using Caret\n\n\n```{r Caret}\nset.seed(505)\ntrain_index <- createDataPartition(wine$price, times = 1, p = 0.8, list = FALSE)\ntrain <- wine[train_index, ]\ntest <- wine[-train_index, ]\nhead(test)\n```\n\n\n## Compare RMSE across models\n- Retrain on models on the training set\n\n```{r Models} \nms <- list(\n  lm(price ~ points, data = train),\n  lm(price ~ points + bordeaux, data = train),\n  lm(price ~ points * bordeaux, data = train)\n)\n```\n\n- Test them all under the same conditions.\n\n\n```{r RMSE}\nmap(ms, function(m) {\n  get_regression_points(m, newdata = test) %>%\n    drop_na(residual) %>%\n    mutate(sq_residuals = residual^2) %>%\n    summarize(rmse = sqrt(mean(sq_residuals))) %>%\n    pluck(\"rmse\")\n}) %>% unlist()\n```\n\n<!--\n## Compare RMSE across models\n\n```r\nget_regression_points(lm(price~points, data = train), newdata = test) %>% \n  drop_na(residual) %>%   mutate(sq_residuals = residual^2) %>% \n  summarize(rmse = sqrt(mean(sq_residuals))) %>% pluck(\"rmse\")\n```\n```r\nget_regression_points(lm(price~points+bordeaux, data = train), newdata = test) %>% \n  drop_na(residual) %>%   mutate(sq_residuals = residual^2) %>% \n  summarize(rmse = sqrt(mean(sq_residuals))) %>% pluck(\"rmse\")\n```\n\n```r\nget_regression_points(lm(price~points*bordeaux, data = train), newdata = test) %>%   \n  drop_na(residual) %>%  mutate(sq_residuals = residual^2) %>% \n  summarize(rmse = sqrt(mean(sq_residuals))) %>% pluck(\"rmse\")\n```\n-->\n## Group Exercise (30m)\n\n1. Load the wine data set\n1. Visualize the relationship of points and price\n1. **Bonus:** Color the observations based on whether the wine is from Bordeaux \n1. **Bonus+:** Include regression lines\n1. **Bonus++:** Pick a non-Bordeaux category.\n\n<!--\n## Wine\n\n- Use the full `wine` dataset.\n```r\nwine <- wall %>% \n  mutate(bordeaux=(province==\"Bordeaux\")) %>%\n  drop_na(points,price,bordeaux)\n```\n-->\n\n## Plot\n- Points vs. price.\n\n```{r Naive Plot}\nwine %>%\n  ggplot(aes(x = points, y = price)) +\n  geom_smooth()\n```\n\n## Bonus\n- Color the Bordeaux region.\n\n```{r Bonus1}\nwine %>%\n  ggplot(aes(x = points, y = price, color = bordeaux)) +\n  geom_smooth()\n```\n\n## Bonus+\n- Include regression lines\n\n```{r Bonus2}\nwine %>%\n  mutate(m = predict(lm(price ~ points, data = wine))) %>%\n  ggplot() +\n  geom_smooth(aes(x = points, y = price, color = bordeaux)) +\n  geom_line(aes(x = points, y = m), colour = \"magenta\")\n```\n\n\n## Bonus++\n- Let's look at \"reserve\".\n\n```{r Bonus3}\nwine %>%\n  mutate(reserve = grepl(\"Reserve\", designation)) %>%\n  ggplot(aes(x = points, y = price, color = reserve)) +\n  geom_smooth()\n```\n\n## Bonus#\n- Anglophones to Francophiles.\n\n```{r Bonus4}\nwine %>%\n  mutate(reservæ = grepl(\"Reserve\", designation, ignore.case = TRUE) |\n    grepl(\"Reserva\", designation, ignore.case = TRUE)) %>%\n  ggplot(aes(x = points, y = price, color = reservæ)) +\n  geom_smooth()\n```\n\n\n## RჂservæ\n- Cross the Alps.\n\n```{r Bonus5}\nwine %>%\n  mutate(rჂservæ = grepl(\"Reserve|Reserva|Riserva\", designation, ignore.case = TRUE)) %>%\n  ggplot(aes(x = points, y = price, color = rჂservæ)) +\n  geom_smooth()\n```\n\n\n\n\n# Dinner break \n:::: {.columns}\n\n::: {.column width=\"50%\"}\n- On \"rჂservæ\"\n  - Ie or Iota (asomtavruli Ⴢ, nuskhuri ⴢ, mkhedruli ჲ, mtavruli Ჲ) is the 15th letter of the three [Georgian scripts](https://en.wikipedia.org/wiki/Georgian_scripts)\n:::\n\n::: {.column width=\"50%\"}\n![](images/nutcracker.png)\n:::\n\n::::\n\n\n# Classification and Ethics \n\n## The math of it...\n\n- Suppose I'm trying to predict sex based on height.\n  - Don't do this in real life (obviously).\n- We start by \n  - defining the outcome and predictors, and...\n  - creating training and test data.\n\n## Partition our Data\n\n\n```{r Partition}\ndata(heights) # from library(dslabs)\ny <- heights$sex\nx <- heights$height\nset.seed(505)\ntest_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)\ntest_set <- heights[test_index, ]\ntrain_set <- heights[-test_index, ]\nsummary(heights)\n```\n\n\nNote: this vignette is adapted from [this book](https://rafalab.github.io/dsbook/introduction-to-machine-learning.html)\n\n## Guessing\n\n- Let’s start by developing the simplest possible machine algorithm: guessing the outcome.\n\n```{r Guessing}\ny_hat <- sample(c(\"Male\", \"Female\"), length(test_index), replace = TRUE)\n```\n\nRecall:\n\n>[Y hat (written ŷ ) is the predicted value of y (the dependent variable) in a regression equation. It can also be considered to be the average value of the response variable.](https://www.statisticshowto.com/y-hat-definition/)\n\n## Accuracy\n- The overall accuracy is simply defined as the overall proportion that is predicted correctly:\n\n```{r Accuracy}\nmean(y_hat == test_set$sex)\n```\n\n- What would we have expected the accuracy to be?\n  - What much would we have expected accuracy to deviate from that expectionation?\n\n\n## Let's do better...\n\n\n```{r Better}\nsummary <- heights %>%\n  group_by(sex) %>%\n  summarize(mean(height), sd(height))\nsummary\n```\n\n\n## A simple predictive model\n\n- Idea: Predict `\"Male\"` if observation is within 2 standard deviations\n\n\n```{r Predict}\nmale_mean_less_2sd <- summary[2, ][\"mean(height)\"] - 2 * summary[2, ][\"sd(height)\"]\n\ny_hat <- ifelse(x > male_mean_less_2sd, \"Male\", \"Female\") %>%\n  factor(levels = levels(test_set$sex))\n\nc(male_mean_less_2sd, mean(y == y_hat))\n```\n\n\n- The accuracy goes up from ~0.50 to about ~0.80!!\n\n## Let's optimize\n\n\n```{r Optimize}\ncutoff <- seq(61, 70)\nget_accuracy <- function(x) {\n  y_hat <- ifelse(train_set$height > x, \"Male\", \"Female\")\n  mean(y_hat == train_set$sex)\n}\naccuracy <- map(cutoff, get_accuracy)\n\nunlist(accuracy)\n```\n\n\n- Most are much higher than 0.5!! \n\n## Let's take a gander\n- Easier for me to see it.\n\n```{r Gander}\nplot(cutoff, accuracy)\n```\n\n\n## Optimal Cutoff\n\n\n```{r Best cutoff}\nbest_cutoff <- cutoff[which.max(accuracy)]\nbest_cutoff\n```\n\n- Should we be cutting at an integer?\n\n## Apply & Evaluate\n\n\n```{r Cutoff test}\ny_hat <- ifelse(test_set$height > best_cutoff, \"Male\", \"Female\")\nmean(y_hat == test_set$sex)\n```\n\n\n\n## Confusion matrix\n\n```{r Matrix}\ntable(predicted = y_hat, actual = test_set$sex) %>%\n  as.data.frame() %>%\n  ggplot(aes(x = predicted, y = actual)) +\n  geom_tile(aes(fill = Freq), color = \"white\") +\n  scale_fill_gradient(low = \"white\", high = \"blue\") +\n  geom_text(aes(label = Freq), vjust = \"center\", color = \"black\", size = 24) +\n  labs(title = \"Confusion Matrix\", x = \"Predicted\", y = \"Actual\")\n```\n\n\n## Accuracy by sex\n\n\n```{r Accuracy by sex}\ntest_set %>%\n  mutate(y_hat = y_hat) %>%\n  group_by(sex) %>%\n  summarize(accuracy = mean(y_hat == sex))\n```\n\n&nbsp;\n\nIt's raining men.\n\n## Debrief\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n```{r Boxes}\nheights %>%\n  ggplot() +\n  geom_boxplot(aes(height, sex))\n```\n\n:::\n\n::: {.column width=\"50%\"}\n\n```{r Pie}\nslices <- heights %>%\n  group_by(sex) %>%\n  tally()\npie(slices$n, labels = slices$sex)\n```\n\n:::\n\n::::\n\n\n## Moral of the story\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/l5aZJBLAu1E?si=r1vMnz5WGl7tLjlq\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n\n## Other ethical issues\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n- Demographic data\n- Profit optimizing\n- Autonomous cars\n- Recommendation engines\n:::\n\n::: {.column width=\"50%\"}\n- Fair housing\n- Criminal sentencing\n- Choice of classification model\n- Drone warfare\n:::\n\n::::\n\n## Jameson on Ethics\n<blockquote>\nReasonable people will disagree over subtle matters of right and wrong... thus, the important part of data ethics is committing to *consider* the ethical consequences of your choices. \n\nThe difference between \"regular\" ethics and data ethics is that algorithms scale really easily. Thus, seemingly small decisions can have wide-ranging impact.\n</blockquote>\n\n## Calvin on Ethics\n\n> No ethical \\[computation\\] under capitalism\n\n- Usage of data `|` computing is ethicial `iff` it challenges rather than strengthens existing power relations.\n\n\n# Vocabulary\n\n## ML Terms\n\n**Definition of ML:** using data to find a function that minimizes prediction error.\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n- Features\n- Variables\n- Outcome variable\n- Regression\n:::\n\n::: {.column width=\"50%\"}\n- RMSE\n- Classification\n- Confusion matrix\n- Split Samples\n:::\n\n::::\n\n## **Features**  \n- **Definition:** Individual measurable properties or attributes of data.  \n- **Example:** Age, income, and education level in a dataset predicting loan approval.  \n\n## **Variables**  \n- **Definition:** Data points that can change and impact predictions.  \n- **Example:** Independent variables like weather, and dependent variables like crop yield.  \n\n## **Outcome Variable**  \n- **Definition:** The target or dependent variable the model predicts.  \n- **Example:** Predicting \"passed\" or \"failed\" for a student's exam result.  \n\n## Features vs. Variables  \n- **Features:** Inputs to the model, often selected or engineered from raw data.  \n  - Example: \"Average monthly income\" derived from raw transaction data.  \n- **Variables:** Broader term encompassing both inputs (independent) and outputs (dependent).  \n  - Example: \"House price\" (dependent variable) depends on features like size and location.\n\n## **Regression**  \n- **Definition:** Statistical method to model the relationship between variables.  \n- **Example:** Linear regression predicts house prices based on size and location.  \n\n## **RMSE (Root Mean Square Error)**  \n- **Definition:** A metric to measure prediction accuracy by averaging squared errors.  \n- **Example:** Lower RMSE in predicting drug response indicates a better model fit.  \n\n## **Classification**  \n- **Definition:** Task of predicting discrete categories or labels.  \n- **Example:** Classifying emails as \"spam\" or \"not spam.\"  \n\n## **Confusion Matrix**  \n- **Definition:** A table showing model performance in classification tasks.  \n- **Example:** Matrix rows show true values; columns show predicted outcomes.  \n\n```{r Matrix Deux}\n#| echo: false\ntable(predicted = y_hat, actual = test_set$sex) %>%\n  as.data.frame() %>%\n  ggplot(aes(x = predicted, y = actual)) +\n  geom_tile(aes(fill = Freq), color = \"white\") +\n  scale_fill_gradient(low = \"white\", high = \"blue\") +\n  geom_text(aes(label = Freq), vjust = \"center\", color = \"black\", size = 24) +\n  labs(title = \"Confusion Matrix\", x = \"Predicted\", y = \"Actual\")\n```\n\n\n## **Split Samples**  \n- **Definition:** Dividing data into training and testing subsets for validation.  \n- **Example:** 80% training, 20% testing ensures unbiased model evaluation.  \n```r\ntest_set <- heights[test_index, ]\ntrain_set <- heights[-test_index, ]\n```\n\n\n# Bonus Slides:<br> Precision-recall\n\n## Precision-recall tradeoff\n\n- Precision: TP / (TP + FP)\n- Recall: TP / (TP + FN)\n- Imagine I have a fraud-detection model that gives 1,000 credit card transactions each a risk score.\n\n\n\n## Precision-recall tradeoff\n\n- Imagine I have a fraud-detection model that gives 1,000 credit card transactions each a risk score.\n- The company chooses a risk score cutoff of 77 (for some reason). \n- There are 18 transactions with risk above 77. 12 are actually fraud. 20 fraudulent transactions have risk below 77.\n- What are precision, recall, and accuracy?\n\n\n## Precision-recall Exercise\n\n- Precision: TP / (TP + FP)\n- Recall: TP / (TP + FN)\n- 1,000 credit card transactions\n- The company chooses a risk score cutoff of 77\n- There are 18 transactions with risk above 77. \n  - 12 are actually fraud. \n  - 20 fraudulent transactions have risk below 77.\n- <span style=\"color:red;font-weight:bold\">TODO</span> Calculate precision, recall, and accuracy.\n\n## Solutions\n```\n- Definitions\n  - Precision: TP / (TP + FP)\n  - Recall:    TP / (TP + FN)\n- Computation\n  - Precision: 12 / (12 + 06)  ~= 67%\n  - Recall:    12 / (12 + 20)  ~= 38%\n  - Accuracy: (12 + 962)/1000  ~= 97%\n```\n\n\n## Precision-recall tradeoff\n\n- Precision: TP / (TP + FP)\n- Recall: TP / (TP + FN)\n- Image: Hands-on machine learning, A. Geron\n\n![](images/precision_recall.png)\n\n",
    "supporting": [
      "Python01_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}