{
  "hash": "795e7af445c9d2195e78cc50586448dd",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Machine Learning in Python\"\nsubtitle: \"Applied Machine Learning\"\nauthor: \"Calvin\"\n  \njupyter: python3\n\nexecute:\n    echo: true\n    freeze: true  # never re-render during project render\n---\n\n\n\n## Agenda\n\n1. Python Overview\n2. Review of Regression\n3. Classification\n4. Basic Feature Engineering\n\n## Quarto\n\n-   I switch from a R backend to a Python backend.\n-   I add the following below my title in my .qmd header:\n```\njupyter: python3\n```\n\n\n\n## Pip\n\n-   In Python, we can typically install packages via `pip`\n-   It is much typical to use `pip` at commandline.\n```bash\npython -m pip install sampleproject\n```\n-   Here is a \"clean\" way to do so from within the Python\n\n::: {#490575cd .cell execution_count=1}\n``` {.python .cell-code}\nimport subprocess  # A base package we need to install other packages\nimport sys         # A base package we need to install other packages\ninstall = lambda package : subprocess.check_call([sys.executable, \n                                                  \"-m\", \n                                                  \"pip\", \n                                                  \"install\", \n                                                  package])\n```\n:::\n\n\n## Packages\n\n- I'll build a list of packages at install them via a loop.\n\n::: {#9b97f765 .cell execution_count=2}\n``` {.python .cell-code}\nimport subprocess\nimport sys\n\ninstall = lambda package : subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n```\n:::\n\n\n<span style=\"color:red;font-weight:bold\">TODO</span>\n\n## Setup\n\n- We will work with a `wine` dataset that is enormous.\n  - Just to render a bit quickly, take a sample.\n  - You are welcome to work with the [full dataset](https://cd-public.github.io/courses/rmls25/dat/wine.rds)!\n\n\n```{r Setup}\nwine <- readRDS(gzcon(url(\"https://cd-public.github.io/courses/rmls25/dat/wine.rds\")))\nwine <- wine %>% drop_na(points, price)\n# performance concession\n# wall <- wine\n# wine = wall[sample(nrow(wall), 100), ]\nsummary(wine)\n```\n\n\n# Review of Regression\n\n## Single Variable\n\n- Pick the poshest province.\n\n```{r Single Variable}\nwine <- wine %>%\n  mutate(bordeaux = (province == \"Bordeaux\"))\nwine <- wine %>% drop_na(bordeaux)\ntop_n(wine, 10, bordeaux)\n```\n\n\n## Regress\n- Take a quick regression model over the wine.\n\n```{r Regress}\nm1 <- lm(price ~ points, data = wine)\nget_regression_table(m1)\n```\n\n## Let's draw it\n\n\n```{r Lets draw it 1}\n#| echo: false\nwine %>%\n  mutate(m1 = predict(m1)) %>%\n  ggplot() +\n  geom_smooth(aes(points, price)) +\n  geom_line(aes(points, m1), size = 2, color = \"orange\") +\n  labs(\n    title = \"Regression of Price on Points\",\n    x = \"Points\", y = \"Price\"\n  )\n```\n\n\n## Multiple regression\n\n\n```{r Multiple regression}\nm2 <- lm(price ~ points + bordeaux, data = wine)\nget_regression_table(m2)\n```\n\n\n## Let's draw it\n\n\n```{r Lets draw it 2}\n#| echo: false\nwine %>%\n  mutate(m2 = predict(m2)) %>%\n  ggplot() +\n  geom_smooth(aes(points, price)) +\n  geom_line(aes(x = points, y = m2, color = bordeaux), size = 2) +\n  labs(\n    title = \"Multiple Regression of Price on Points and Bordeaux\",\n    x = \"Points\", y = \"Price\"\n  )\n```\n\n\n## How about with an interaction?\n\n\n```{r Interaction}\nm3 <- lm(price ~ points * bordeaux, data = wine)\nget_regression_table(m3)\n```\n\n\n## Let's draw it\n\n```{r Lets draw it 3}\n#| echo: false\nwine %>%\n  mutate(m3 = predict(m3)) %>%\n  ggplot() +\n  geom_smooth(aes(points, price)) +\n  geom_line(aes(x = points, y = m3, color = bordeaux), size = 2) +\n  labs(\n    title = \"Interaction Model: Price on Points and Bordeaux\",\n    x = \"Points\", y = \"Price\"\n  )\n```\n\n\n\n## Model diagnostics \n\n\n```{r Diagnostics}\nget_regression_summaries(m1)\nget_regression_summaries(m2)\nget_regression_summaries(m3)\n```\n\n\n# Moving to an ML framework\n\n## Split sample using Caret\n\n\n```{r Caret}\nset.seed(505)\ntrain_index <- createDataPartition(wine$price, times = 1, p = 0.8, list = FALSE)\ntrain <- wine[train_index, ]\ntest <- wine[-train_index, ]\nhead(test)\n```\n\n\n## Compare RMSE across models\n- Retrain on models on the training set\n\n```{r Models} \nms <- list(\n  lm(price ~ points, data = train),\n  lm(price ~ points + bordeaux, data = train),\n  lm(price ~ points * bordeaux, data = train)\n)\n```\n\n- Test them all under the same conditions.\n\n\n```{r RMSE}\nmap(ms, function(m) {\n  get_regression_points(m, newdata = test) %>%\n    drop_na(residual) %>%\n    mutate(sq_residuals = residual^2) %>%\n    summarize(rmse = sqrt(mean(sq_residuals))) %>%\n    pluck(\"rmse\")\n}) %>% unlist()\n```\n\n<!--\n## Compare RMSE across models\n\n```r\nget_regression_points(lm(price~points, data = train), newdata = test) %>% \n  drop_na(residual) %>%   mutate(sq_residuals = residual^2) %>% \n  summarize(rmse = sqrt(mean(sq_residuals))) %>% pluck(\"rmse\")\n```\n```r\nget_regression_points(lm(price~points+bordeaux, data = train), newdata = test) %>% \n  drop_na(residual) %>%   mutate(sq_residuals = residual^2) %>% \n  summarize(rmse = sqrt(mean(sq_residuals))) %>% pluck(\"rmse\")\n```\n\n```r\nget_regression_points(lm(price~points*bordeaux, data = train), newdata = test) %>%   \n  drop_na(residual) %>%  mutate(sq_residuals = residual^2) %>% \n  summarize(rmse = sqrt(mean(sq_residuals))) %>% pluck(\"rmse\")\n```\n-->\n## Group Exercise (30m)\n\n1. Load the wine data set\n1. Visualize the relationship of points and price\n1. **Bonus:** Color the observations based on whether the wine is from Bordeaux \n1. **Bonus+:** Include regression lines\n1. **Bonus++:** Pick a non-Bordeaux category.\n\n<!--\n## Wine\n\n- Use the full `wine` dataset.\n```r\nwine <- wall %>% \n  mutate(bordeaux=(province==\"Bordeaux\")) %>%\n  drop_na(points,price,bordeaux)\n```\n-->\n\n## Plot\n- Points vs. price.\n\n```{r Naive Plot}\nwine %>%\n  ggplot(aes(x = points, y = price)) +\n  geom_smooth()\n```\n\n## Bonus\n- Color the Bordeaux region.\n\n```{r Bonus1}\nwine %>%\n  ggplot(aes(x = points, y = price, color = bordeaux)) +\n  geom_smooth()\n```\n\n## Bonus+\n- Include regression lines\n\n```{r Bonus2}\nwine %>%\n  mutate(m = predict(lm(price ~ points, data = wine))) %>%\n  ggplot() +\n  geom_smooth(aes(x = points, y = price, color = bordeaux)) +\n  geom_line(aes(x = points, y = m), colour = \"magenta\")\n```\n\n\n## Bonus++\n- Let's look at \"reserve\".\n\n```{r Bonus3}\nwine %>%\n  mutate(reserve = grepl(\"Reserve\", designation)) %>%\n  ggplot(aes(x = points, y = price, color = reserve)) +\n  geom_smooth()\n```\n\n## Bonus#\n- Anglophones to Francophiles.\n\n```{r Bonus4}\nwine %>%\n  mutate(reservæ = grepl(\"Reserve\", designation, ignore.case = TRUE) |\n    grepl(\"Reserva\", designation, ignore.case = TRUE)) %>%\n  ggplot(aes(x = points, y = price, color = reservæ)) +\n  geom_smooth()\n```\n\n\n## RჂservæ\n- Cross the Alps.\n\n```{r Bonus5}\nwine %>%\n  mutate(rჂservæ = grepl(\"Reserve|Reserva|Riserva\", designation, ignore.case = TRUE)) %>%\n  ggplot(aes(x = points, y = price, color = rჂservæ)) +\n  geom_smooth()\n```\n\n\n\n\n# Dinner break \n:::: {.columns}\n\n::: {.column width=\"50%\"}\n- On \"rჂservæ\"\n  - Ie or Iota (asomtavruli Ⴢ, nuskhuri ⴢ, mkhedruli ჲ, mtavruli Ჲ) is the 15th letter of the three [Georgian scripts](https://en.wikipedia.org/wiki/Georgian_scripts)\n:::\n\n::: {.column width=\"50%\"}\n![](images/nutcracker.png)\n:::\n\n::::\n\n\n# Classification and Ethics \n\n## The math of it...\n\n- Suppose I'm trying to predict sex based on height.\n  - Don't do this in real life (obviously).\n- We start by \n  - defining the outcome and predictors, and...\n  - creating training and test data.\n\n## Partition our Data\n\n\n```{r Partition}\ndata(heights) # from library(dslabs)\ny <- heights$sex\nx <- heights$height\nset.seed(505)\ntest_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)\ntest_set <- heights[test_index, ]\ntrain_set <- heights[-test_index, ]\nsummary(heights)\n```\n\n\nNote: this vignette is adapted from [this book](https://rafalab.github.io/dsbook/introduction-to-machine-learning.html)\n\n## Guessing\n\n- Let’s start by developing the simplest possible machine algorithm: guessing the outcome.\n\n```{r Guessing}\ny_hat <- sample(c(\"Male\", \"Female\"), length(test_index), replace = TRUE)\n```\n\nRecall:\n\n>[Y hat (written ŷ ) is the predicted value of y (the dependent variable) in a regression equation. It can also be considered to be the average value of the response variable.](https://www.statisticshowto.com/y-hat-definition/)\n\n## Accuracy\n- The overall accuracy is simply defined as the overall proportion that is predicted correctly:\n\n```{r Accuracy}\nmean(y_hat == test_set$sex)\n```\n\n- What would we have expected the accuracy to be?\n  - What much would we have expected accuracy to deviate from that expectionation?\n\n\n## Let's do better...\n\n\n```{r Better}\nsummary <- heights %>%\n  group_by(sex) %>%\n  summarize(mean(height), sd(height))\nsummary\n```\n\n\n## A simple predictive model\n\n- Idea: Predict `\"Male\"` if observation is within 2 standard deviations\n\n\n```{r Predict}\nmale_mean_less_2sd <- summary[2, ][\"mean(height)\"] - 2 * summary[2, ][\"sd(height)\"]\n\ny_hat <- ifelse(x > male_mean_less_2sd, \"Male\", \"Female\") %>%\n  factor(levels = levels(test_set$sex))\n\nc(male_mean_less_2sd, mean(y == y_hat))\n```\n\n\n- The accuracy goes up from ~0.50 to about ~0.80!!\n\n## Let's optimize\n\n\n```{r Optimize}\ncutoff <- seq(61, 70)\nget_accuracy <- function(x) {\n  y_hat <- ifelse(train_set$height > x, \"Male\", \"Female\")\n  mean(y_hat == train_set$sex)\n}\naccuracy <- map(cutoff, get_accuracy)\n\nunlist(accuracy)\n```\n\n\n- Most are much higher than 0.5!! \n\n## Let's take a gander\n- Easier for me to see it.\n\n```{r Gander}\nplot(cutoff, accuracy)\n```\n\n\n## Optimal Cutoff\n\n\n```{r Best cutoff}\nbest_cutoff <- cutoff[which.max(accuracy)]\nbest_cutoff\n```\n\n- Should we be cutting at an integer?\n\n## Apply & Evaluate\n\n\n```{r Cutoff test}\ny_hat <- ifelse(test_set$height > best_cutoff, \"Male\", \"Female\")\nmean(y_hat == test_set$sex)\n```\n\n\n\n## Confusion matrix\n\n```{r Matrix}\ntable(predicted = y_hat, actual = test_set$sex) %>%\n  as.data.frame() %>%\n  ggplot(aes(x = predicted, y = actual)) +\n  geom_tile(aes(fill = Freq), color = \"white\") +\n  scale_fill_gradient(low = \"white\", high = \"blue\") +\n  geom_text(aes(label = Freq), vjust = \"center\", color = \"black\", size = 24) +\n  labs(title = \"Confusion Matrix\", x = \"Predicted\", y = \"Actual\")\n```\n\n\n## Accuracy by sex\n\n\n```{r Accuracy by sex}\ntest_set %>%\n  mutate(y_hat = y_hat) %>%\n  group_by(sex) %>%\n  summarize(accuracy = mean(y_hat == sex))\n```\n\n&nbsp;\n\nIt's raining men.\n\n## Debrief\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n```{r Boxes}\nheights %>%\n  ggplot() +\n  geom_boxplot(aes(height, sex))\n```\n\n:::\n\n::: {.column width=\"50%\"}\n\n```{r Pie}\nslices <- heights %>%\n  group_by(sex) %>%\n  tally()\npie(slices$n, labels = slices$sex)\n```\n\n:::\n\n::::\n\n\n## Moral of the story\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/l5aZJBLAu1E?si=r1vMnz5WGl7tLjlq\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n\n## Other ethical issues\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n- Demographic data\n- Profit optimizing\n- Autonomous cars\n- Recommendation engines\n:::\n\n::: {.column width=\"50%\"}\n- Fair housing\n- Criminal sentencing\n- Choice of classification model\n- Drone warfare\n:::\n\n::::\n\n## Jameson on Ethics\n<blockquote>\nReasonable people will disagree over subtle matters of right and wrong... thus, the important part of data ethics is committing to *consider* the ethical consequences of your choices. \n\nThe difference between \"regular\" ethics and data ethics is that algorithms scale really easily. Thus, seemingly small decisions can have wide-ranging impact.\n</blockquote>\n\n## Calvin on Ethics\n\n> No ethical \\[computation\\] under capitalism\n\n- Usage of data `|` computing is ethicial `iff` it challenges rather than strengthens existing power relations.\n\n\n# Vocabulary\n\n## ML Terms\n\n**Definition of ML:** using data to find a function that minimizes prediction error.\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n- Features\n- Variables\n- Outcome variable\n- Regression\n:::\n\n::: {.column width=\"50%\"}\n- RMSE\n- Classification\n- Confusion matrix\n- Split Samples\n:::\n\n::::\n\n## **Features**  \n- **Definition:** Individual measurable properties or attributes of data.  \n- **Example:** Age, income, and education level in a dataset predicting loan approval.  \n\n## **Variables**  \n- **Definition:** Data points that can change and impact predictions.  \n- **Example:** Independent variables like weather, and dependent variables like crop yield.  \n\n## **Outcome Variable**  \n- **Definition:** The target or dependent variable the model predicts.  \n- **Example:** Predicting \"passed\" or \"failed\" for a student's exam result.  \n\n## Features vs. Variables  \n- **Features:** Inputs to the model, often selected or engineered from raw data.  \n  - Example: \"Average monthly income\" derived from raw transaction data.  \n- **Variables:** Broader term encompassing both inputs (independent) and outputs (dependent).  \n  - Example: \"House price\" (dependent variable) depends on features like size and location.\n\n## **Regression**  \n- **Definition:** Statistical method to model the relationship between variables.  \n- **Example:** Linear regression predicts house prices based on size and location.  \n\n## **RMSE (Root Mean Square Error)**  \n- **Definition:** A metric to measure prediction accuracy by averaging squared errors.  \n- **Example:** Lower RMSE in predicting drug response indicates a better model fit.  \n\n## **Classification**  \n- **Definition:** Task of predicting discrete categories or labels.  \n- **Example:** Classifying emails as \"spam\" or \"not spam.\"  \n\n## **Confusion Matrix**  \n- **Definition:** A table showing model performance in classification tasks.  \n- **Example:** Matrix rows show true values; columns show predicted outcomes.  \n\n```{r Matrix Deux}\n#| echo: false\ntable(predicted = y_hat, actual = test_set$sex) %>%\n  as.data.frame() %>%\n  ggplot(aes(x = predicted, y = actual)) +\n  geom_tile(aes(fill = Freq), color = \"white\") +\n  scale_fill_gradient(low = \"white\", high = \"blue\") +\n  geom_text(aes(label = Freq), vjust = \"center\", color = \"black\", size = 24) +\n  labs(title = \"Confusion Matrix\", x = \"Predicted\", y = \"Actual\")\n```\n\n\n## **Split Samples**  \n- **Definition:** Dividing data into training and testing subsets for validation.  \n- **Example:** 80% training, 20% testing ensures unbiased model evaluation.  \n```r\ntest_set <- heights[test_index, ]\ntrain_set <- heights[-test_index, ]\n```\n\n\n# Bonus Slides:<br> Precision-recall\n\n## Precision-recall tradeoff\n\n- Precision: TP / (TP + FP)\n- Recall: TP / (TP + FN)\n- Imagine I have a fraud-detection model that gives 1,000 credit card transactions each a risk score.\n\n\n\n## Precision-recall tradeoff\n\n- Imagine I have a fraud-detection model that gives 1,000 credit card transactions each a risk score.\n- The company chooses a risk score cutoff of 77 (for some reason). \n- There are 18 transactions with risk above 77. 12 are actually fraud. 20 fraudulent transactions have risk below 77.\n- What are precision, recall, and accuracy?\n\n\n## Precision-recall Exercise\n\n- Precision: TP / (TP + FP)\n- Recall: TP / (TP + FN)\n- 1,000 credit card transactions\n- The company chooses a risk score cutoff of 77\n- There are 18 transactions with risk above 77. \n  - 12 are actually fraud. \n  - 20 fraudulent transactions have risk below 77.\n- <span style=\"color:red;font-weight:bold\">TODO</span> Calculate precision, recall, and accuracy.\n\n## Solutions\n```\n- Definitions\n  - Precision: TP / (TP + FP)\n  - Recall:    TP / (TP + FN)\n- Computation\n  - Precision: 12 / (12 + 06)  ~= 67%\n  - Recall:    12 / (12 + 20)  ~= 38%\n  - Accuracy: (12 + 962)/1000  ~= 97%\n```\n\n\n## Precision-recall tradeoff\n\n- Precision: TP / (TP + FP)\n- Recall: TP / (TP + FN)\n- Image: Hands-on machine learning, A. Geron\n\n![](images/precision_recall.png)\n\n",
    "supporting": [
      "Python01_files"
    ],
    "filters": [],
    "includes": {}
  }
}