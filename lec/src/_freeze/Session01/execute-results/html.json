{
  "hash": "ea4260d576b812f814fec592c9d6fed1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Machine Learning Overview\"\nsubtitle: \"Applied Machine Learning\"\nauthor: \"Jameson > Hendrik > Calvin\"\n  \nexecute:\n    echo: true\n    freeze: true  # never re-render during project render\n---\n\n\n\n\n## Agenda\n\n1. Course Overview\n2. Review of Regression\n3. Dinner Break\n4. Classification and Ethics\n5. Basic Feature Engineering\n6. Vocabulary\n\n# Course Overview\n\n## Expectations and assignments\n\n1. Homework assignments\n2. Exams\n3. Modeling Project\n4. Course Policies\n5. My expectations for you\n\n## About me\n- BA Mathematics, BS Computer Science (UChicago)\n- MS, PhD Computer Science (UNC Chapel Hill)\n- Data mining, formal analysis, complex models\n- Joined Willamette 2021\n\n## About you?\n  - Background\n  - Goals for this program and/or course\n\n## Basic concepts in Machine Learning\n\n- What is a data scientist?\n- What is machine learning? \n- What is the role of judgment in machine learning?\n- What are the differences between machine learning, statistics and econometrics?\n- When is \"mere\" correlation enough? When is it not?\n\n## Packages\n\n- Today I use the following libraries:\n```r\nlocal({r <- getOption(\"repos\")\n       r[\"CRAN\"] <- \"https://cran.r-project.org\" \n       options(repos=r)\n})\n# New?\ninstall.packages(\"tidyverse\")\ninstall.packages(\"moderndive\")\ninstall.packages(\"caret\")\ninstall.packages(\"dslabs\")\n# Just for the slides\ninstall.packages(\"thematic\")\n```\n- You will have some but perhaps not others.\n\n## Libraries\n\n- I'll just include them upfront.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(moderndive)\nlibrary(caret)\nlibrary(dslabs)\n# Just for the slides\nlibrary(thematic)\ntheme_set(theme_dark())\nthematic_rmd(bg = \"#111\", fg = \"#eee\", accent = \"#eee\")\n```\n:::\n\n\n\n## Setup\n\n- We will work with a `wine` dataset that is enormous.\n  - Just to render a bit quickly, take a sample.\n  - You are welcome to work with the [full dataset](https://cd-public.github.io/courses/rmls25/dat/wine.rds)!\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwine <- readRDS(gzcon(url(\"https://cd-public.github.io/courses/rmls25/dat/wine.rds\")))\nwine <- wine %>% drop_na(points, price)\n# performance concession\n# wall <- wine\n# wine = wall[sample(nrow(wall), 100), ]\nsummary(wine)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       id           country          description        designation       \n Min.   :     1   Length:89556       Length:89556       Length:89556      \n 1st Qu.: 32742   Class :character   Class :character   Class :character  \n Median : 65613   Mode  :character   Mode  :character   Mode  :character  \n Mean   : 65192                                                           \n 3rd Qu.: 97738                                                           \n Max.   :129970                                                           \n     points           price           province           region_1        \n Min.   : 80.00   Min.   :   4.00   Length:89556       Length:89556      \n 1st Qu.: 87.00   1st Qu.:  17.00   Class :character   Class :character  \n Median : 89.00   Median :  25.00   Mode  :character   Mode  :character  \n Mean   : 88.65   Mean   :  35.56                                        \n 3rd Qu.: 91.00   3rd Qu.:  42.00                                        \n Max.   :100.00   Max.   :3300.00                                        \n   region_2         taster_name        taster_twitter_handle    title          \n Length:89556       Length:89556       Length:89556          Length:89556      \n Class :character   Class :character   Class :character      Class :character  \n Mode  :character   Mode  :character   Mode  :character      Mode  :character  \n                                                                               \n                                                                               \n                                                                               \n   variety             winery               year     \n Length:89556       Length:89556       Min.   :1995  \n Class :character   Class :character   1st Qu.:2010  \n Mode  :character   Mode  :character   Median :2012  \n                                       Mean   :2011  \n                                       3rd Qu.:2014  \n                                       Max.   :2015  \n```\n\n\n:::\n:::\n\n\n\n# Review of Regression\n\n## Single Variable\n\n- Pick the poshest province.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwine <- wine %>%\n  mutate(bordeaux = (province == \"Bordeaux\"))\nwine <- wine %>% drop_na(bordeaux)\ntop_n(wine, 10, bordeaux)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3,774 × 16\n      id country description designation points price province region_1 region_2\n   <dbl> <chr>   <chr>       <chr>        <dbl> <dbl> <chr>    <chr>    <chr>   \n 1    53 France  Fruity and… La Fleur d…     85    15 Bordeaux Bordeau… <NA>    \n 2   136 France  This wine'… <NA>            91    50 Bordeaux Saint-É… <NA>    \n 3   419 France  A smooth, … <NA>            89    20 Bordeaux Graves   <NA>    \n 4   477 France  An interes… <NA>            92    65 Bordeaux Pomerol  <NA>    \n 5   573 France  Fruity and… <NA>            89    14 Bordeaux Bordeau… <NA>    \n 6   575 France  This is a … <NA>            89    14 Bordeaux Bordeau… <NA>    \n 7   576 France  From a Gra… Les Terras…     89    37 Bordeaux Saint-É… <NA>    \n 8   578 France  A ripe per… Château Je…     89    15 Bordeaux Bordeaux <NA>    \n 9   792 France  The 45% Ca… La Sérénit…     90    30 Bordeaux Médoc    <NA>    \n10   795 France  This is th… Divin de C…     90    35 Bordeaux Saint-É… <NA>    \n# ℹ 3,764 more rows\n# ℹ 7 more variables: taster_name <chr>, taster_twitter_handle <chr>,\n#   title <chr>, variety <chr>, winery <chr>, year <dbl>, bordeaux <lgl>\n```\n\n\n:::\n:::\n\n\n\n## Regress\n- Take a quick regression model over the wine.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm1 <- lm(price ~ points, data = wine)\nget_regression_table(m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 7\n  term      estimate std_error statistic p_value lower_ci upper_ci\n  <chr>        <dbl>     <dbl>     <dbl>   <dbl>    <dbl>    <dbl>\n1 intercept  -489.       3.97      -123.       0  -497.    -482.  \n2 points        5.92     0.045      132.       0     5.83     6.01\n```\n\n\n:::\n:::\n\n\n## Let's draw it\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Session01_files/figure-revealjs/Lets draw it 1-1.png){width=960}\n:::\n:::\n\n\n\n## Multiple regression\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm2 <- lm(price ~ points + bordeaux, data = wine)\nget_regression_table(m2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 7\n  term         estimate std_error statistic p_value lower_ci upper_ci\n  <chr>           <dbl>     <dbl>     <dbl>   <dbl>    <dbl>    <dbl>\n1 intercept     -492.       3.97     -124.        0  -500.    -484.  \n2 points           5.95     0.045     133.        0     5.86     6.03\n3 bordeauxTRUE     8.70     0.661      13.2       0     7.41    10.0 \n```\n\n\n:::\n:::\n\n\n\n## Let's draw it\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Session01_files/figure-revealjs/Lets draw it 2-1.png){width=960}\n:::\n:::\n\n\n\n## How about with an interaction?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm3 <- lm(price ~ points * bordeaux, data = wine)\nget_regression_table(m3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 7\n  term                estimate std_error statistic p_value lower_ci upper_ci\n  <chr>                  <dbl>     <dbl>     <dbl>   <dbl>    <dbl>    <dbl>\n1 intercept            -461.       4.04     -114.        0  -469.    -453.  \n2 points                  5.60     0.045     123.        0     5.51     5.69\n3 bordeauxTRUE         -666.      18.8       -35.5       0  -703.    -629.  \n4 points:bordeauxTRUE     7.66     0.213      36.0       0     7.24     8.07\n```\n\n\n:::\n:::\n\n\n\n## Let's draw it\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Session01_files/figure-revealjs/Lets draw it 3-1.png){width=960}\n:::\n:::\n\n\n\n\n## Model diagnostics \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_regression_summaries(m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 9\n  r_squared adj_r_squared   mse  rmse sigma statistic p_value    df  nobs\n      <dbl>         <dbl> <dbl> <dbl> <dbl>     <dbl>   <dbl> <dbl> <dbl>\n1     0.164         0.164 1579.  39.7  39.7    17497.       0     1 89503\n```\n\n\n:::\n\n```{.r .cell-code}\nget_regression_summaries(m2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 9\n  r_squared adj_r_squared   mse  rmse sigma statistic p_value    df  nobs\n      <dbl>         <dbl> <dbl> <dbl> <dbl>     <dbl>   <dbl> <dbl> <dbl>\n1     0.165         0.165 1576.  39.7  39.7     8852.       0     2 89503\n```\n\n\n:::\n\n```{.r .cell-code}\nget_regression_summaries(m3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 9\n  r_squared adj_r_squared   mse  rmse sigma statistic p_value    df  nobs\n      <dbl>         <dbl> <dbl> <dbl> <dbl>     <dbl>   <dbl> <dbl> <dbl>\n1     0.177         0.177 1553.  39.4  39.4     6418.       0     3 89503\n```\n\n\n:::\n:::\n\n\n\n# Moving to an ML framework\n\n## Split sample using Caret\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(505)\ntrain_index <- createDataPartition(wine$price, times = 1, p = 0.8, list = FALSE)\ntrain <- wine[train_index, ]\ntest <- wine[-train_index, ]\nhead(test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 16\n     id country description  designation points price province region_1 region_2\n  <dbl> <chr>   <chr>        <chr>        <dbl> <dbl> <chr>    <chr>    <chr>   \n1     8 Germany Savory drie… Shine           87    12 Rheinhe… <NA>     <NA>    \n2    19 US      Red fruit a… <NA>            87    32 Virginia Virginia <NA>    \n3    20 US      Ripe aromas… Vin de Mai…     87    23 Virginia Virginia <NA>    \n4    28 Italy   Aromas sugg… Mascaria B…     87    17 Sicily … Cerasuo… <NA>    \n5    59 US      Aromas of c… <NA>            86    55 Washing… Columbi… Columbi…\n6    61 Italy   This densel… Prugneto        86    17 Central… Romagna  <NA>    \n# ℹ 7 more variables: taster_name <chr>, taster_twitter_handle <chr>,\n#   title <chr>, variety <chr>, winery <chr>, year <dbl>, bordeaux <lgl>\n```\n\n\n:::\n:::\n\n\n\n## Compare RMSE across models\n- Retrain on models on the training set\n\n\n::: {.cell}\n\n```{.r .cell-code}\nms <- list(\n  lm(price ~ points, data = train),\n  lm(price ~ points + bordeaux, data = train),\n  lm(price ~ points * bordeaux, data = train)\n)\n```\n:::\n\n\n- Test them all under the same conditions.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmap(ms, function(m) {\n  get_regression_points(m, newdata = test) %>%\n    drop_na(residual) %>%\n    mutate(sq_residuals = residual^2) %>%\n    summarize(rmse = sqrt(mean(sq_residuals))) %>%\n    pluck(\"rmse\")\n}) %>% unlist()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 36.00999 36.00970 35.85276\n```\n\n\n:::\n:::\n\n\n<!--\n## Compare RMSE across models\n\n```r\nget_regression_points(lm(price~points, data = train), newdata = test) %>% \n  drop_na(residual) %>%   mutate(sq_residuals = residual^2) %>% \n  summarize(rmse = sqrt(mean(sq_residuals))) %>% pluck(\"rmse\")\n```\n```r\nget_regression_points(lm(price~points+bordeaux, data = train), newdata = test) %>% \n  drop_na(residual) %>%   mutate(sq_residuals = residual^2) %>% \n  summarize(rmse = sqrt(mean(sq_residuals))) %>% pluck(\"rmse\")\n```\n\n```r\nget_regression_points(lm(price~points*bordeaux, data = train), newdata = test) %>%   \n  drop_na(residual) %>%  mutate(sq_residuals = residual^2) %>% \n  summarize(rmse = sqrt(mean(sq_residuals))) %>% pluck(\"rmse\")\n```\n-->\n## Group Exercise (30m)\n\n1. Load the wine data set\n1. Visualize the relationship of points and price\n1. **Bonus:** Color the observations based on whether the wine is from Bordeaux \n1. **Bonus+:** Include regression lines\n1. **Bonus++:** Pick a non-Bordeaux category.\n\n<!--\n## Wine\n\n- Use the full `wine` dataset.\n```r\nwine <- wall %>% \n  mutate(bordeaux=(province==\"Bordeaux\")) %>%\n  drop_na(points,price,bordeaux)\n```\n-->\n\n## Plot\n- Points vs. price.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwine %>%\n  ggplot(aes(x = points, y = price)) +\n  geom_smooth()\n```\n\n::: {.cell-output-display}\n![](Session01_files/figure-revealjs/Naive Plot-1.png){width=960}\n:::\n:::\n\n\n## Bonus\n- Color the Bordeaux region.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwine %>%\n  ggplot(aes(x = points, y = price, color = bordeaux)) +\n  geom_smooth()\n```\n\n::: {.cell-output-display}\n![](Session01_files/figure-revealjs/Bonus1-1.png){width=960}\n:::\n:::\n\n\n## Bonus+\n- Include regression lines\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwine %>%\n  mutate(m = predict(lm(price ~ points, data = wine))) %>%\n  ggplot() +\n  geom_smooth(aes(x = points, y = price, color = bordeaux)) +\n  geom_line(aes(x = points, y = m), colour = \"magenta\")\n```\n\n::: {.cell-output-display}\n![](Session01_files/figure-revealjs/Bonus2-1.png){width=960}\n:::\n:::\n\n\n\n## Bonus++\n- Let's look at \"reserve\".\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwine %>%\n  mutate(reserve = grepl(\"Reserve\", designation)) %>%\n  ggplot(aes(x = points, y = price, color = reserve)) +\n  geom_smooth()\n```\n\n::: {.cell-output-display}\n![](Session01_files/figure-revealjs/Bonus3-1.png){width=960}\n:::\n:::\n\n\n## Bonus#\n- Anglophones to Francophiles.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwine %>%\n  mutate(reservæ = grepl(\"Reserve\", designation, ignore.case = TRUE) |\n    grepl(\"Reserva\", designation, ignore.case = TRUE)) %>%\n  ggplot(aes(x = points, y = price, color = reservæ)) +\n  geom_smooth()\n```\n\n::: {.cell-output-display}\n![](Session01_files/figure-revealjs/Bonus4-1.png){width=960}\n:::\n:::\n\n\n\n## RჂservæ\n- Cross the Alps.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwine %>%\n  mutate(rჂservæ = grepl(\"Reserve|Reserva|Riserva\", designation, ignore.case = TRUE)) %>%\n  ggplot(aes(x = points, y = price, color = rჂservæ)) +\n  geom_smooth()\n```\n\n::: {.cell-output-display}\n![](Session01_files/figure-revealjs/Bonus5-1.png){width=960}\n:::\n:::\n\n\n\n\n\n# Dinner break \n:::: {.columns}\n\n::: {.column width=\"50%\"}\n- On \"rჂservæ\"\n  - Ie or Iota (asomtavruli Ⴢ, nuskhuri ⴢ, mkhedruli ჲ, mtavruli Ჲ) is the 15th letter of the three [Georgian scripts](https://en.wikipedia.org/wiki/Georgian_scripts)\n:::\n\n::: {.column width=\"50%\"}\n![](images/nutcracker.png)\n:::\n\n::::\n\n\n# Classification and Ethics \n\n## The math of it...\n\n- Suppose I'm trying to predict sex based on height.\n  - Don't do this in real life (obviously).\n- We start by \n  - defining the outcome and predictors, and...\n  - creating training and test data.\n\n## Partition our Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(heights) # from library(dslabs)\ny <- heights$sex\nx <- heights$height\nset.seed(505)\ntest_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)\ntest_set <- heights[test_index, ]\ntrain_set <- heights[-test_index, ]\nsummary(heights)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     sex          height     \n Female:238   Min.   :50.00  \n Male  :812   1st Qu.:66.00  \n              Median :68.50  \n              Mean   :68.32  \n              3rd Qu.:71.00  \n              Max.   :82.68  \n```\n\n\n:::\n:::\n\n\n\nNote: this vignette is adapted from [this book](https://rafalab.github.io/dsbook/introduction-to-machine-learning.html)\n\n## Guessing\n\n- Let’s start by developing the simplest possible machine algorithm: guessing the outcome.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny_hat <- sample(c(\"Male\", \"Female\"), length(test_index), replace = TRUE)\n```\n:::\n\n\nRecall:\n\n>[Y hat (written ŷ ) is the predicted value of y (the dependent variable) in a regression equation. It can also be considered to be the average value of the response variable.](https://www.statisticshowto.com/y-hat-definition/)\n\n## Accuracy\n- The overall accuracy is simply defined as the overall proportion that is predicted correctly:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(y_hat == test_set$sex)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5180952\n```\n\n\n:::\n:::\n\n\n- What would we have expected the accuracy to be?\n  - What much would we have expected accuracy to deviate from that expectionation?\n\n\n## Let's do better...\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary <- heights %>%\n  group_by(sex) %>%\n  summarize(mean(height), sd(height))\nsummary\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  sex    `mean(height)` `sd(height)`\n  <fct>           <dbl>        <dbl>\n1 Female           64.9         3.76\n2 Male             69.3         3.61\n```\n\n\n:::\n:::\n\n\n\n## A simple predictive model\n\n- Idea: Predict `\"Male\"` if observation is within 2 standard deviations\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmale_mean_less_2sd <- summary[2, ][\"mean(height)\"] - 2 * summary[2, ][\"sd(height)\"]\n\ny_hat <- ifelse(x > male_mean_less_2sd, \"Male\", \"Female\") %>%\n  factor(levels = levels(test_set$sex))\n\nc(male_mean_less_2sd, mean(y == y_hat))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$`mean(height)`\n[1] 62.09271\n\n[[2]]\n[1] 0.7733333\n```\n\n\n:::\n:::\n\n\n\n- The accuracy goes up from ~0.50 to about ~0.80!!\n\n## Let's optimize\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncutoff <- seq(61, 70)\nget_accuracy <- function(x) {\n  y_hat <- ifelse(train_set$height > x, \"Male\", \"Female\")\n  mean(y_hat == train_set$sex)\n}\naccuracy <- map(cutoff, get_accuracy)\n\nunlist(accuracy)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 0.7752381 0.7866667 0.8152381 0.8342857 0.8209524 0.7904762 0.7314286\n [8] 0.6819048 0.5961905 0.5104762\n```\n\n\n:::\n:::\n\n\n\n- Most are much higher than 0.5!! \n\n## Let's take a gander\n- Easier for me to see it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(cutoff, accuracy)\n```\n\n::: {.cell-output-display}\n![](Session01_files/figure-revealjs/Gander-1.png){width=960}\n:::\n:::\n\n\n\n## Optimal Cutoff\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_cutoff <- cutoff[which.max(accuracy)]\nbest_cutoff\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 64\n```\n\n\n:::\n:::\n\n\n- Should we be cutting at an integer?\n\n## Apply & Evaluate\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny_hat <- ifelse(test_set$height > best_cutoff, \"Male\", \"Female\")\nmean(y_hat == test_set$sex)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8190476\n```\n\n\n:::\n:::\n\n\n\n\n## Confusion matrix\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(predicted = y_hat, actual = test_set$sex) %>%\n  as.data.frame() %>%\n  ggplot(aes(x = predicted, y = actual)) +\n  geom_tile(aes(fill = Freq), color = \"white\") +\n  scale_fill_gradient(low = \"white\", high = \"blue\") +\n  geom_text(aes(label = Freq), vjust = \"center\", color = \"black\", size = 24) +\n  labs(title = \"Confusion Matrix\", x = \"Predicted\", y = \"Actual\")\n```\n\n::: {.cell-output-display}\n![](Session01_files/figure-revealjs/Matrix-1.png){width=960}\n:::\n:::\n\n\n\n## Accuracy by sex\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_set %>%\n  mutate(y_hat = y_hat) %>%\n  group_by(sex) %>%\n  summarize(accuracy = mean(y_hat == sex))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  sex    accuracy\n  <fct>     <dbl>\n1 Female    0.445\n2 Male      0.929\n```\n\n\n:::\n:::\n\n\n&nbsp;\n\nIt's raining men.\n\n## Debrief\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nheights %>%\n  ggplot() +\n  geom_boxplot(aes(height, sex))\n```\n\n::: {.cell-output-display}\n![](Session01_files/figure-revealjs/Boxes-1.png){width=960}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nslices <- heights %>%\n  group_by(sex) %>%\n  tally()\npie(slices$n, labels = slices$sex)\n```\n\n::: {.cell-output-display}\n![](Session01_files/figure-revealjs/Pie-1.png){width=960}\n:::\n:::\n\n\n:::\n\n::::\n\n\n## Moral of the story\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/l5aZJBLAu1E?si=r1vMnz5WGl7tLjlq\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n\n## Other ethical issues\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n- Demographic data\n- Profit optimizing\n- Autonomous cars\n- Recommendation engines\n:::\n\n::: {.column width=\"50%\"}\n- Fair housing\n- Criminal sentencing\n- Choice of classification model\n- Drone warfare\n:::\n\n::::\n\n## Jameson on Ethics\n<blockquote>\nReasonable people will disagree over subtle matters of right and wrong... thus, the important part of data ethics is committing to *consider* the ethical consequences of your choices. \n\nThe difference between \"regular\" ethics and data ethics is that algorithms scale really easily. Thus, seemingly small decisions can have wide-ranging impact.\n</blockquote>\n\n## Calvin on Ethics\n\n> No ethical \\[computation\\] under capitalism\n\n- Usage of data `|` computing is ethicial `iff` it challenges rather than strengthens existing power relations.\n\n\n# Vocabulary\n\n## ML Terms\n\n**Definition of ML:** using data to find a function that minimizes prediction error.\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n- Features\n- Variables\n- Outcome variable\n- Regression\n:::\n\n::: {.column width=\"50%\"}\n- RMSE\n- Classification\n- Confusion matrix\n- Split Samples\n:::\n\n::::\n\n## **Features**  \n- **Definition:** Individual measurable properties or attributes of data.  \n- **Example:** Age, income, and education level in a dataset predicting loan approval.  \n\n## **Variables**  \n- **Definition:** Data points that can change and impact predictions.  \n- **Example:** Independent variables like weather, and dependent variables like crop yield.  \n\n## **Outcome Variable**  \n- **Definition:** The target or dependent variable the model predicts.  \n- **Example:** Predicting \"passed\" or \"failed\" for a student's exam result.  \n\n## Features vs. Variables  \n- **Features:** Inputs to the model, often selected or engineered from raw data.  \n  - Example: \"Average monthly income\" derived from raw transaction data.  \n- **Variables:** Broader term encompassing both inputs (independent) and outputs (dependent).  \n  - Example: \"House price\" (dependent variable) depends on features like size and location.\n\n## **Regression**  \n- **Definition:** Statistical method to model the relationship between variables.  \n- **Example:** Linear regression predicts house prices based on size and location.  \n\n## **RMSE (Root Mean Square Error)**  \n- **Definition:** A metric to measure prediction accuracy by averaging squared errors.  \n- **Example:** Lower RMSE in predicting drug response indicates a better model fit.  \n\n## **Classification**  \n- **Definition:** Task of predicting discrete categories or labels.  \n- **Example:** Classifying emails as \"spam\" or \"not spam.\"  \n\n## **Confusion Matrix**  \n- **Definition:** A table showing model performance in classification tasks.  \n- **Example:** Matrix rows show true values; columns show predicted outcomes.  \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Session01_files/figure-revealjs/Matrix Deux-1.png){width=960}\n:::\n:::\n\n\n\n## **Split Samples**  \n- **Definition:** Dividing data into training and testing subsets for validation.  \n- **Example:** 80% training, 20% testing ensures unbiased model evaluation.  \n```r\ntest_set <- heights[test_index, ]\ntrain_set <- heights[-test_index, ]\n```\n\n\n# Bonus Slides:<br> Precision-recall\n\n## Precision-recall tradeoff\n\n- Precision: TP / (TP + FP)\n- Recall: TP / (TP + FN)\n- Imagine I have a fraud-detection model that gives 1,000 credit card transactions each a risk score.\n\n\n\n## Precision-recall tradeoff\n\n- Imagine I have a fraud-detection model that gives 1,000 credit card transactions each a risk score.\n- The company chooses a risk score cutoff of 77 (for some reason). \n- There are 18 transactions with risk above 77. 12 are actually fraud. 20 fraudulent transactions have risk below 77.\n- What are precision, recall, and accuracy?\n\n\n## Precision-recall Exercise\n\n- Precision: TP / (TP + FP)\n- Recall: TP / (TP + FN)\n- 1,000 credit card transactions\n- The company chooses a risk score cutoff of 77\n- There are 18 transactions with risk above 77. \n  - 12 are actually fraud. \n  - 20 fraudulent transactions have risk below 77.\n- <span style=\"color:red;font-weight:bold\">TODO</span> Calculate precision, recall, and accuracy.\n\n## Solutions\n```\n- Definitions\n  - Precision: TP / (TP + FP)\n  - Recall:    TP / (TP + FN)\n- Computation\n  - Precision: 12 / (12 + 06)  ~= 67%\n  - Recall:    12 / (12 + 20)  ~= 38%\n  - Accuracy: (12 + 962)/1000  ~= 97%\n```\n\n\n## Precision-recall tradeoff\n\n- Precision: TP / (TP + FP)\n- Recall: TP / (TP + FN)\n- Image: Hands-on machine learning, A. Geron\n\n![](images/precision_recall.png)\n\n\n\n",
    "supporting": [
      "Session01_files\\figure-revealjs"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}