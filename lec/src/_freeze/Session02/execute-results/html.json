{
  "hash": "b051bef2a9ce4946c3f7d9ffa3226772",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Feature Engineering & Variable Selection\"\nsubtitle: \"Applied Machine Learning\"\nauthor: \"Jameson > Hendrik > Calvin\"\n  \nexecute:\n    echo: true\n    cache: true\n    freeze: true  # never re-render during project render\n---\n\n\n\n## Agenda\n\n1.  Review of Homework 1\n2.  Feature Engineering I\n3.  Dinner Break\n4.  The Caret framework\n5.  Vocabulary\n\n## Packages\n\n-   Today I use the following libraries:\n\n``` r\nlocal({r <- getOption(\"repos\")\n       r[\"CRAN\"] <- \"https://cran.r-project.org\" \n       options(repos=r)\n})\n# Old\n# install.packages(\"tidyverse\")\n# install.packages(\"caret\")\n# New?\ninstall.packages(\"fastDummies\")\n# Just for the slides\n# install.packages(\"thematic\")\n```\n\n-   You will have some but perhaps not others.\n\n## Libraries\n\n-   I'll just include them upfront.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(fastDummies)\n# Just for the slides\nlibrary(thematic)\ntheme_set(theme_dark())\nthematic_rmd(bg = \"#111\", fg = \"#eee\", accent = \"#eee\")\n```\n:::\n\n\n\n## Setup\n\n-   We will work with a `wine` dataset that is enormous.\n    -   Just to render a bit quickly, take a sample.\n    -   You are welcome to work with the [full dataset](https://cd-public.github.io/courses/rmls25/dat/wine.rds)!\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwine <- readRDS(gzcon(url(\"https://cd-public.github.io/courses/rmls25/dat/wine.rds\")))\n# performance concession\n# wall <- wine\n# wine = wall[sample(nrow(wall), 100), ]\nsummary(wine)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       id           country          description        designation       \n Min.   :     1   Length:89556       Length:89556       Length:89556      \n 1st Qu.: 32742   Class :character   Class :character   Class :character  \n Median : 65613   Mode  :character   Mode  :character   Mode  :character  \n Mean   : 65192                                                           \n 3rd Qu.: 97738                                                           \n Max.   :129970                                                           \n     points           price           province           region_1        \n Min.   : 80.00   Min.   :   4.00   Length:89556       Length:89556      \n 1st Qu.: 87.00   1st Qu.:  17.00   Class :character   Class :character  \n Median : 89.00   Median :  25.00   Mode  :character   Mode  :character  \n Mean   : 88.65   Mean   :  35.56                                        \n 3rd Qu.: 91.00   3rd Qu.:  42.00                                        \n Max.   :100.00   Max.   :3300.00                                        \n   region_2         taster_name        taster_twitter_handle    title          \n Length:89556       Length:89556       Length:89556          Length:89556      \n Class :character   Class :character   Class :character      Class :character  \n Mode  :character   Mode  :character   Mode  :character      Mode  :character  \n                                                                               \n                                                                               \n                                                                               \n   variety             winery               year     \n Length:89556       Length:89556       Min.   :1995  \n Class :character   Class :character   1st Qu.:2010  \n Mode  :character   Mode  :character   Median :2012  \n                                       Mean   :2011  \n                                       3rd Qu.:2014  \n                                       Max.   :2015  \n```\n\n\n:::\n:::\n\n\n\n# Feature Engineering\n\n## Group Exercise (30m)\n\n1.  Identify 3 \"interesting\" features of the wine dataset\n2.  **Bonus** \n    -   &exist; Critic \"Roger Voss\"\n    -   &exist; a wine varietal(s) \"Voss\" seems to dislike\n    -   Find said varietal.\n\n## Categorical vs. Continuous\n\n-   What is a categorical variable?\n-   What is a continuous variable?\n-   Why visualize at the data before modeling it?\n\n## Categorical Example 1\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwine %>%\n  mutate(roger = taster_name == \"Roger Voss\") %>%\n  mutate(pinot_gris = variety == \"Pinot Gris\") %>%\n  drop_na(roger) %>%\n  group_by(roger, pinot_gris) %>%\n  summarize(points = mean(points)) %>%\n  ggplot() +\n  aes(pinot_gris, points, color = roger) +\n  geom_line(aes(group = roger), size = 2)\n```\n\n::: {.cell-output-display}\n![](Session02_files/figure-revealjs/Cat1-1.png){width=960}\n:::\n:::\n\n\n\n## Categorical Example 2\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwine %>%\n  filter(province == \"Oregon\") %>%\n  group_by(year) %>%\n  summarize(price = mean(price)) %>%\n  ggplot(aes(year, price)) +\n  geom_smooth() +\n  labs(title = \"Oregon wine over the years\")\n```\n\n::: {.cell-output-display}\n![](Session02_files/figure-revealjs/Cat2-1.png){width=960}\n:::\n:::\n\n\n\n## Exercise (15 min)\n\n1.  Group by winery and year, Find:\n    -   The average score, and \n    -   Number of reviews.\n2.  Find year-on-year change in score by winery.\n3.  How might you use this in prediction? \n    -   What kind of problem might it help with?\n\n## Year-on-Year Change Example\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwine %>%\n  group_by(winery, year) %>%\n  summarize(avg_score = mean(points), num_reviews = n_distinct(id)) %>%\n  select(year, winery, num_reviews, avg_score) %>%\n  arrange(winery, year) %>%\n  mutate(score_change = avg_score - lag(avg_score)) %>%\n  drop_na(score_change) %>%\n  summarize(mean(score_change))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 8,409 × 2\n   winery           `mean(score_change)`\n   <chr>                           <dbl>\n 1 100 Percent Wine               -1.5  \n 2 12 Linajes                     -0.333\n 3 12C Wines                       1.25 \n 4 14 Hands                       -0.111\n 5 2 Lads                         -2.25 \n 6 2 Up                            0    \n 7 21 Grams                        1    \n 8 29 & Oak Wines                 -2    \n 9 2Hawk                           0.75 \n10 2Plank                         -1.25 \n# ℹ 8,399 more rows\n```\n\n\n:::\n:::\n\n\n\n## Dummy Variables\n\n-   **What are Dummy Variables?**: \n    -   Represent categories as 0s and 1s for models.\\\n-   **Why Use Dummy Variables?**: \n    -   Handle categorical data in numerical algorithms.\\\n-   **Avoid Dummy Trap**: \n    -   Drop one column to prevent multicollinearity.\n\n## Many vs Few Dummies\n\n-   **Few Dummies**: \n    -   Simplifies models, risks losing fine-grained patterns.\n-   **Many Dummies**: \n    -   Captures detailed trends, increases model complexity.\n-   **Key Decision**: \n    -   Balance interpretability and predictive power.\n\n## \"fastDummies\" Package\n\n-   **Purpose**: \n    -   Quickly create dummy variables in R datasets.\n-   **Key Functions**: \n    -   `dummy_cols()` adds dummy columns efficiently.\n-   **Features**: \n    -   Handles multiple columns and missing data flexibly.\n\n## Few Dummies\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwine %>%\n  select(taster_name) %>%\n  dummy_cols() %>% # library(fastDummies)\n  select(1:4) %>%\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 4\n  taster_name        `taster_name_Alexander Peartree` taster_name_Anna Lee C. …¹\n  <chr>                                         <int>                      <int>\n1 Roger Voss                                        0                          0\n2 Paul Gregutt                                      0                          0\n3 Alexander Peartree                                1                          0\n4 Paul Gregutt                                      0                          0\n5 Michael Schachner                                 0                          0\n6 Kerin O’Keefe                                     0                          0\n# ℹ abbreviated name: ¹​`taster_name_Anna Lee C. Iijima`\n# ℹ 1 more variable: `taster_name_Anne Krebiehl MW` <int>\n```\n\n\n:::\n:::\n\n\n\n## Many Dummies\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwine %>%\n  select(variety) %>%\n  mutate(variety = fct_lump(variety, 4)) %>%\n  dummy_cols() %>%\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 6\n  variety    variety_Cabernet Sauvigno…¹ variety_Chardonnay `variety_Pinot Noir`\n  <fct>                            <int>              <int>                <int>\n1 Other                                0                  0                    0\n2 Other                                0                  0                    0\n3 Other                                0                  0                    0\n4 Pinot Noir                           0                  0                    1\n5 Other                                0                  0                    0\n6 Other                                0                  0                    0\n# ℹ abbreviated name: ¹​`variety_Cabernet Sauvignon`\n# ℹ 2 more variables: `variety_Red Blend` <int>, variety_Other <int>\n```\n\n\n:::\n:::\n\n\n\n## Other types of engineered categorical features...\n\n-   Words or phrases in text\n-   A given time period\n-   An arbitrary numerical cut-off\n-   Demographic variables\n\n## What about numerical features?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwine %>%\n  ggplot(aes(price)) +\n  geom_histogram()\n```\n\n::: {.cell-output-display}\n![](Session02_files/figure-revealjs/Price gg-1.png){width=960}\n:::\n:::\n\n\n\n## Take the natural log\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwine %>%\n  ggplot(aes(log(price))) +\n  geom_histogram()\n```\n\n::: {.cell-output-display}\n![](Session02_files/figure-revealjs/Log Price gg-1.png){width=960}\n:::\n:::\n\n\n\n## Standardizing\n\n-   Create a common scale across variables.\n    -   Mean-centering $x-\\bar{x}$\n    -   Scaling: $x/std(x)$\n\n-   Helps reduce bias when interactions are included. \n    -   (i.e. eliminates variance inflation).\n\n## Other transformations.\n\n-   I use logs \\> 95% of the time, standarizing \\~40%.\n-   There are [many other transformations](http://www.feat.engineering/numeric-one-to-many.html):\n    -   YoY, QoQ, etc. (absolute and percent)\n    -   log\n    -   polynomial transforms\n    -   lags!\n\n## Standardize\n\n-   `list(normalized = ~(scale(.) %>% as.vector))`: :\n    -   `scale(.)`: Standardizes the \"points\" column.\n    -   `%>% as.vector`: Converts back to a vector.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwine %>% mutate_at(\"points\", list(standardized = ~ (scale(.) %>% as.vector())))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 89,556 × 16\n      id country description designation points price province region_1 region_2\n   <dbl> <chr>   <chr>       <chr>        <dbl> <dbl> <chr>    <chr>    <chr>   \n 1     1 Portug… This is ri… Avidagos        87    15 Douro    <NA>     <NA>    \n 2     2 US      Tart and s… <NA>            87    14 Oregon   Willame… Willame…\n 3     3 US      Pineapple … Reserve La…     87    13 Michigan Lake Mi… <NA>    \n 4     4 US      Much like … Vintner's …     87    65 Oregon   Willame… Willame…\n 5     5 Spain   Blackberry… Ars In Vit…     87    15 Norther… Navarra  <NA>    \n 6     6 Italy   Here's a b… Belsito         87    16 Sicily … Vittoria <NA>    \n 7     7 France  This dry a… <NA>            87    24 Alsace   Alsace   <NA>    \n 8     8 Germany Savory dri… Shine           87    12 Rheinhe… <NA>     <NA>    \n 9     9 France  This has g… Les Natures     87    27 Alsace   Alsace   <NA>    \n10    10 US      Soft, supp… Mountain C…     87    19 Califor… Napa Va… Napa    \n# ℹ 89,546 more rows\n# ℹ 7 more variables: taster_name <chr>, taster_twitter_handle <chr>,\n#   title <chr>, variety <chr>, winery <chr>, year <dbl>, standardized <dbl>\n```\n\n\n:::\n:::\n\n\n\n## Interaction effects\n\n[This chapter](http://www.feat.engineering/detecting-interaction-effects.html) has a good overview of interactions.\n\n-   Start with domain knowledge.\n-   Use visualizations.\n-   3-way interactions exist, but are rare.\n    -   If you suspect a 3-way, also suspect your suspicions.\n    -   Complexity increases exponentially in \"ways\".\n    -   These are notoriously hard to explain.\n\n## Dinner (and virtual high fives)\n\n![](images/comic2.gif)\n\n# Le Bibliothèques _caret_\n\n## Philosophy\n\n\n\n```{dot}\n//| echo: false\ndigraph G {\n    \n    bgcolor=\"#101010\";\n\n    node [\n        fontcolor = \"#e0e0e0\",\n        color = \"#e0e0e0\",\n    ]\n\n    edge [\n        color = \"#e0e0e0\",\n        fontcolor = \"#e0e0e0\"\n    ]\n    node [shape=circle];\n    A [label=\"All Data\"];\n\n    node [shape=pentagon];\n    B [label=\"Training\"];\n    C [label=\"Testing\"];\n\n    node [shape=rectangle];\n    D [label=\"Resample 1\"];\n    E [label=\"Resample 2\"];\n    F [label=\"Resample B\"];\n\n    node [shape=ellipse];\n    G [label=\"Analysis\"];\n    H [label=\"Assessment\"];\n    I [label=\"Analysis\"];\n    J [label=\"Assessment\"];\n    K [label=\"Analysis\"];\n    L [label=\"Assessment\"];\n\n    A -> B;\n    A -> C;\n    B -> D;\n    B -> E;\n    B -> F;\n    D -> G;\n    D -> H;\n    E -> I;\n    E -> J;\n    F -> K;\n    F -> L;\n}\n```\n\n\n\n## Types of resampling\n\n-   [V-fold Cross-Validation](http://www.feat.engineering/resampling.html#cv)\n    -   Divides data into $k$ folds, trains on $k−1$ folds, validates on the remaining fold, for all folds.\n-   [Monte Carlo Cross-Validation](http://www.feat.engineering/resampling.html#monte-carlo-cross-validation)\n    -   Randomly splits data into training and validation sets multiple times, averaging results for evaluation.\n-   [The Bootstrap](http://www.feat.engineering/resampling.html#the-bootstrap)\n    -   Uses resampling with replacement to estimate model accuracy and variability.\n\n## Setup the Dataframe\n\n-   Follow [this link](https://topepo.github.io/caret) for the full documentation on caret.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwino <- wine %>% # 3 engineered features\n  mutate(fr = (country == \"France\")) %>%\n  mutate(cab = str_detect(variety, \"Cabernet\")) %>%\n  mutate(lprice = log(price)) %>%\n  drop_na(fr, cab) %>%\n  select(lprice, points, fr, cab)\n```\n:::\n\n\n\n-   Off hand, I would've standarized points as well, but\n-   We're following Jameson's code...\n    -   ...who *understands the data better*.\n\n## Split Samples\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwine_index <- createDataPartition(wino$lprice, p = 0.8, list = FALSE)\nwino_tr <- wino[wine_index, ]\nwino_te <- wino[-wine_index, ]\nsummary(wino_tr)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     lprice          points           fr             cab         \n Min.   :1.386   Min.   : 80.00   Mode :logical   Mode :logical  \n 1st Qu.:2.833   1st Qu.: 87.00   FALSE:59455     FALSE:65221    \n Median :3.219   Median : 89.00   TRUE :12148     TRUE :6382     \n Mean   :3.315   Mean   : 88.65                                  \n 3rd Qu.:3.738   3rd Qu.: 91.00                                  \n Max.   :8.102   Max.   :100.00                                  \n```\n\n\n:::\n:::\n\n\n\n## Train the model\n\n-   Configure `train` to cross validate\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm1 <- train(lprice ~ .,\n  data = wino_tr,\n  method = \"lm\",\n  trControl = trainControl(method = \"repeatedcv\", number = 5, repeats = 3)\n)\nm1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear Regression \n\n71603 samples\n    3 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold, repeated 3 times) \nSummary of sample sizes: 57282, 57282, 57282, 57283, 57283, 57282, ... \nResampling results:\n\n  RMSE       Rsquared  MAE      \n  0.5110322  0.395869  0.4025061\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n```\n\n\n:::\n:::\n\n\n\n## RMSE outputs\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(m1$resample)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        RMSE  Rsquared       MAE   Resample\n1  0.5050570 0.3974420 0.3983319 Fold1.Rep1\n2  0.5147500 0.3919909 0.4045014 Fold2.Rep1\n3  0.5116323 0.3870877 0.4032167 Fold3.Rep1\n4  0.5086963 0.4022081 0.4013298 Fold4.Rep1\n5  0.5068981 0.3988173 0.4010734 Fold5.Rep1\n6  0.5107712 0.3947978 0.4027170 Fold1.Rep2\n7  0.5092317 0.3919118 0.4021115 Fold2.Rep2\n8  0.5126777 0.3911989 0.4040957 Fold3.Rep2\n9  0.5029024 0.4062731 0.3961931 Fold4.Rep2\n10 0.5114262 0.3936230 0.4033014 Fold5.Rep2\n11 0.5098401 0.3947984 0.4024767 Fold1.Rep3\n12 0.5134853 0.3903323 0.4064414 Fold2.Rep3\n13 0.5059101 0.4035328 0.3996089 Fold3.Rep3\n14 0.5076320 0.3990091 0.3993792 Fold4.Rep3\n15 0.5101927 0.3902303 0.4005241 Fold5.Rep3\n```\n\n\n:::\n:::\n\n\n\n## Train vs. test\n\n::::: columns\n\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear Regression \n\n71603 samples\n    3 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold, repeated 3 times) \nSummary of sample sizes: 57281, 57283, 57282, 57284, 57282, 57282, ... \nResampling results:\n\n  RMSE       Rsquared   MAE      \n  0.5094069  0.3955502  0.4016868\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n```\n\n\n:::\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npostResample(pred = predict(m1, wino_te), obs = wino_te$lprice)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     RMSE  Rsquared       MAE \n0.5158986 0.3955532 0.4049303 \n```\n\n\n:::\n:::\n\n\n\n:::\n\n:::::\n\n## Group Exercise (30+ minutes)\n\n1.  Create 5-10 new features (in addition to points)\n2.  Create training and test data\n3.  For each, train a linear model for log(price)\n4.  Report RMSE on test set and cross-validated score.\n5.  (Re-)Engineer new(ish) features to lower the RMSE.\n\n# Feature selection\n\n## Stepwise selection\n\n-   **What is Stepwise Selection?**: Iterative method to add or remove predictors in a model based on statistical criteria.\\\n-   **Types**: Forward selection starts with no predictors; backward elimination starts with all predictors; stepwise combines both.\\\n-   **Goal**: Identify a model with strong predictive power and minimal overfitting.\n\n## Stepwise selection is bad\n\nHarrell (2015) provides a comprehensive indictment of the method that can be encapsulated by the statement:\n\n> **“… if this procedure had just been proposed as a statistical method, it would most likely be rejected because it violates every principle of statistical estimation and hypothesis testing.”**\n\n  Reference: [Harrell, F. 2015. Regression Modeling Strategies. Springer.](https://link.springer.com/book/10.1007/978-3-319-19425-7https://link.springer.com/book/10.1007/978-3-319-19425-7)\n\n## Engineer 9 features\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwino <- wine %>%\n  mutate(country = fct_lump(country, 4)) %>%    # 1:4,\n  mutate(variety = fct_lump(variety, 4)) %>%    # 5:8,\n  mutate(lprice = log(price)) %>%               #   9\n  select(lprice, points, country, variety) %>%\n  drop_na(.)\nhead(wino)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 4\n  lprice points country variety   \n   <dbl>  <dbl> <fct>   <fct>     \n1   2.71     87 Other   Other     \n2   2.64     87 US      Other     \n3   2.56     87 US      Other     \n4   4.17     87 US      Pinot Noir\n5   2.71     87 Spain   Other     \n6   2.77     87 Italy   Other     \n```\n\n\n:::\n:::\n\n\n\n## Add Dummy Columns\n- Careful - a destructive update to `wino`!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrenamer <- function(s) {\n  s %>% tolower() %>% str_replace(\"-| \", \"_\")\n}\n\nwino <- wino %>%\n  dummy_cols(remove_selected_columns = TRUE) %>%\n  rename_with(.fn = renamer) %>%\n  select(-ends_with(\"other\"))\nhead(wino)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 10\n  lprice points country_france country_italy country_spain country_us\n   <dbl>  <dbl>          <int>         <int>         <int>      <int>\n1   2.71     87              0             0             0          0\n2   2.64     87              0             0             0          1\n3   2.56     87              0             0             0          1\n4   4.17     87              0             0             0          1\n5   2.71     87              0             0             1          0\n6   2.77     87              0             1             0          0\n# ℹ 4 more variables: variety_cabernet_sauvignon <int>,\n#   variety_chardonnay <int>, variety_pinot_noir <int>, variety_red_blend <int>\n```\n\n\n:::\n:::\n\n\n\n## Basic Model\n-   Partition\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwine_index <- createDataPartition(wino$lprice, p = 0.8, list = FALSE)\nwino_tr <- wino[wine_index, ]\nwino_te <- wino[-wine_index, ]\n```\n:::\n\n\n- We would model the same way, so let's take aside.\n\n## Aside: Factoring\n- Same modelling command\n```\nmx <- train(lprice ~ .,\n  data = wino_tr,\n  method = \"lm\",\n  trControl = trainControl(method = \"repeatedcv\", number = 5, repeats = 3)\n)\n```\n-   I should factor this into a function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndo_training <- function(df, formula) {\n  train(formula,\n    data = df,\n    method = \"lm\",\n    trControl = trainControl(method = \"repeatedcv\", number = 5, repeats = 3)\n  )\n}\n```\n:::\n\n\n\n## Train vs. test\n\n::::: columns\n\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm2 <- do_training(\n  wino_tr, lprice ~ .\n)\nm2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear Regression \n\n71603 samples\n    9 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold, repeated 3 times) \nSummary of sample sizes: 57283, 57283, 57283, 57282, 57281, 57282, ... \nResampling results:\n\n  RMSE       Rsquared   MAE     \n  0.4893917  0.4463272  0.380097\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n```\n\n\n:::\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npostResample(\n  pred = predict(m2, wino_te),\n  obs = wino_te$lprice\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     RMSE  Rsquared       MAE \n0.4862957 0.4465611 0.3797848 \n```\n\n\n:::\n:::\n\n\n:::\n\n:::::\n\n\n## Variable Importance\n-   Importance depends on model used...\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(varImp(m2, scale = TRUE))\n```\n\n::: {.cell-output-display}\n![](Session02_files/figure-revealjs/Importance-1.png){width=960}\n:::\n:::\n\n\n\n## Variable Importance\n\n-   Each (linear model) coefficient has a standard error, \n    -   Measures certainty of coefficient given data.\n-   For the t-statistic, \n    -   Confidence that the coefficient is different from 0\n    -   We divide the coefficient by the standard error.\n-   If \"small\" error relative to coefficient\n    -   Then \"big\" t-statistic & high feature importance!\n-   What about coefficient as variable importance?\n\n## [Recursive Feature Elimination](https://topepo.github.io/caret/recursive-feature-elimination.html) {.smaller}\n\n1. Tune/train the model on the training set using all predictors.\n2. Calculate model performance.\n3. Calculate variable importance or rankings.\n4. **for** each subset size $S_i$, i = 1...S **do**\n    1. Keep the $S_i$ most important variables.\n    2. [Optional] Pre-process the data.\n    3. Tune/train the model on the training set using $S_i$ predictors.\n    4. Calculate model performance.\n    5. [Optional] Recalculate the rankings for each predictor.\n5. **end**\n6. Calculate the performance profile over the $S_i$.\n7. Determine the appropriate number of predictors.\n8. Use the model corresponding to the optimal $S_i$.\n\n## Size Drop\n-   It did not seem like 2024 `r` could handle 90k wine samples.\n-   We drop to the 1k data set for this demonstration.\n-   There are ways to address performance.\n    -   I say: Out-of-scope.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwino <- wino[sample(nrow(wino), 1000), ]\n```\n:::\n\n\n\n## Partition Again\n-   Partition\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwine_index <- createDataPartition(wino$lprice, p = 0.8, list = FALSE)\nwino_tr <- wino[wine_index, ]\nwino_te <- wino[-wine_index, ]\n```\n:::\n\n\n\n## Caret RFE\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncontrol <- rfeControl(functions = rfFuncs, method = \"cv\", number = 2)\n# run the RFE algorithm\nresults <- rfe(select(wino_tr, -lprice), wino_tr$lprice, sizes = c(1:3), rfeControl = control)\n# summarize the results\nprint(results)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRecursive feature selection\n\nOuter resampling method: Cross-Validated (2 fold) \n\nResampling performance over subset size:\n\n Variables   RMSE Rsquared    MAE  RMSESD RsquaredSD    MAESD Selected\n         1 0.5112   0.3909 0.4019 0.01567    0.03006 0.003842         \n         2 0.5244   0.3795 0.4158 0.01992    0.04985 0.011290         \n         3 0.5317   0.3974 0.4230 0.01826    0.06426 0.008711         \n         9 0.4853   0.4557 0.3789 0.01672    0.02408 0.009817        *\n\nThe top 5 variables (out of 9):\n   points, country_italy, country_us, variety_pinot_noir, variety_chardonnay\n```\n\n\n:::\n:::\n\n\n\n## See Results\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredictors(results)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"points\"                     \"country_italy\"             \n[3] \"country_us\"                 \"variety_pinot_noir\"        \n[5] \"variety_chardonnay\"         \"country_france\"            \n[7] \"variety_cabernet_sauvignon\" \"variety_red_blend\"         \n[9] \"country_spain\"             \n```\n\n\n:::\n:::\n\n\n\n## Plot Results\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(results)\n```\n\n::: {.cell-output-display}\n![](Session02_files/figure-revealjs/Plot Results-1.png){width=960}\n:::\n:::\n\n\n\n## Practical Workflow\n\n\n\n```{dot}\n//| echo: false\ndigraph feature_engineering_pipeline {\n    \n    bgcolor=\"#101010\";\n\n    node [\n        fontcolor = \"#e0e0e0\",\n        color = \"#e0e0e0\",\n    ]\n\n    edge [\n        color = \"#e0e0e0\",\n        fontcolor = \"#e0e0e0\"\n    ]\n    node [shape=box];\n    \"Raw Data\" -> \"Lots of Features\" [label=\"Feature Engineering\"];\n    \"Lots of Features\" -> \"Candidate Features\" [label=\"Feature Selection\"];\n    \"Candidate Features\" -> \"Shortlist Features\" [label=\"Expert Input\"];\n    \"Shortlist Features\" -> \"Finalist Models\" [label=\"DS Judgement\"];\n    \"Finalist Models\" -> \"Production\" [label=\"Business Unit\"];\n}\n```\n\n\n\n## Key Terms\n\n::::: columns\n\n::: {.column width=\"50%\"}\n-   Feature Engineering\n-   Categorical Feature\n-   Continuous Feature\n-   Dummy\n-   Interaction\n:::\n\n::: {.column width=\"50%\"}\n-   Caret\n-   Model\n-   Resampling\n-   Train vs. Test Data\n-   Variable Importance\n:::\n\n:::::\n\n\n# Bonus Slides:<br> Linear Regression\n\n## 5 Assumptions of Linear Regression\n\n-   Linear regressions have a well-developed statistical theory.\n\n-   This brings perks like confidence intervals on predictions.\n\n-   It also has \"costs\" in that assumptions need to be satisfied.\n\n## The Five\n\n1. Linearity\n2. Constant variance\n3. Normality\n4. Imperfect multicollinearity\n5. Exogeneity\n\n## 1.  Linearity \n\n-   **The dependent variable is a linear combination of the features.**\n\n-   This is less of big deal than it might seem! \n\n-   If y is actually quadratic in x, then y is linear in x\\^2! \n    -   That's feature engineering.\n\n## 2.  Constant variance\n\n-   Or *homoscedasticity*\n\n-   **The variance of the errors do not depend on the values of the features.**\n\n-   Don't make bigger prediction errors for some values of x than for others.\n\n## 3.  Normality \n\n-   **The errors should be independent and normally distributed.**\n\n-   A scatter plot of target variable value and residual (model error) should look like white noise.\n\n## 4.  Lack of perfect multicollinearity \n\n-   **None predictors should be a perfect linear combination of others.**\n\n-   This can happen if you over-engineer features\n    -   This is uncommon. \n    -   You'll see an error that your coefficient matrix is singular or something.\n\n## 5.  Exogeneity \n\n-   **Model errors should be independent of the values of the features.**\n\n-   In particular, errors should have mean zero. \n\n-   It's always good to look at a histogram of your residuals (see also normality).\n\n## First Test\n\n-   Determine whether the errors are normally distributed, like Shapiro-Wilk (also, plot them).\n\n![](images/residuals.png)\n\n## 5 Assumptions of Linear Regression: testing\n\n-   Second I would always look at fitted value vs. residual to check homoscedasticity.\n\n![](images/heterosceda.png)\n\n![](images/homosceda.png)\n\n-   For more, see for example https://people.duke.edu/\\~rnau/testing.htm",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}