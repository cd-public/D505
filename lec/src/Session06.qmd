---
title: "Decision Trees"
subtitle: "Applied Machine Learning"
author: "Jameson > Hendrik > Calvin"
  
execute:
    echo: true
    cache: true
    freeze: true  # never re-render during project render
---

# Midterm 3/3

## Brief recap of course so far {.smaller}

- Linear regression (e.g. `lprice ~ .`), assumptions of model, interpretation.
- $K$-NN (e.g., `province ~ .`), multi-class supervised classification. Hyperparameter $k$.
- Naive Bayes (e.g., `province ~ .`), multi-class supervised classification.
- Logistic regression (e.g., `province=="Oregon" ~ .`), binary supervised classification. Elastic net. 
- Feature engineering (logarithms, center/scaling, Box Cox, tidytext, etc.).
- Feature selection (correlation, linear / logistic coefficients, frequent words, frequent words by class, etc.).

## Modality Discussion

- I would release an assignment electronically at 6 PM
- We can do in person or otherwise.
- It will be "cheat proof"
  - I will ask you nothing for which it matters how you determine the answer.
  - If e.g. ChatGPT can be mind controlled into doing high quality feature engineering, you get points for mind controlling ChatGPT.

# First Model Due 3/10

##  Publish

- Each group should create:

1. An annotated `.*md` file, and 
2. The .rds/.pickle/.parquet file that it generates, that
3. Contains *only* the features you want in the model.

- Under version control, on GitHub.

##  Constraints

- I will run:

1. The specified $K$NN or Naive Bayes model,
2. With: `province ~ .` (or the whole data frame in `scikit`)
3. With repeated 5-fold cross validation
4. With the same index for partitioning training and test sets for every group.
5. On whatever is turned in before class.
6. Bragging rights for highest Kappa

## Context

- The "final exam" is that during the last class you will present your model results as though you are speaking to the
managers of a large winery.
- It should be presented from a Quarto presentation on GitHub or perhaps e.g. RPubs.
- You must present via the in-room "teaching machine" computer, not your own physical device, to ensure that you are comfortable distributing your findings.

## Group Meetings

- You should have a group assignment
- Meet in your groups!
- Talk about your homework *with* your group.


# Agenda

1. Decision-trees
2. Weighted models (revisited)
3. Random forests
4. Bagged models
5. Group work

# Decision trees


## Libraries Setup
```{r libs}
sh <- suppressPackageStartupMessages
sh(library(tidyverse))
sh(library(caret))
sh(library(tidytext))
sh(library(SnowballC))
sh(library(rpart)) # New?
sh(library(randomForest)) # New?
sh(library(doParallel)) # New?
data(stop_words)
sh(library(thematic))
theme_set(theme_dark())
thematic_rmd(bg = "#111", fg = "#eee", accent = "#eee")
```

## Dataframe

```{r rds}
wine <- readRDS(gzcon(url("https://cd-public.github.io/D505/dat/pinot.rds")))
names(wine)[names(wine) == 'id'] = 'id'
```

## Wine Words

```{r setup}
wine_words <- function(df, j, stem){ 
  words <- df %>%
    unnest_tokens(word, description) %>%
    anti_join(stop_words) %>% 
    filter(!(word %in% c("wine","pinot","vineyard")))
  
  if(stem){
    words <- words %>% mutate(word = wordStem(word))
  }
  
  words %>% count(id, word) %>%  group_by(id) %>%  mutate(exists = (n>0)) %>% 
    ungroup %>% group_by(word) %>%  mutate(total = sum(n)) %>% filter(total > j) %>% 
    pivot_wider(id_cols = id, names_from = word, values_from = exists, values_fill = list(exists=0)) %>% 
    right_join(select(df,id,province)) %>% select(-id) %>% mutate(across(-province, ~replace_na(.x, F)))
}
```

## Make Wino

```{r wino}
wino <- wine_words(wine, 2000, F) %>% 
  filter(province %in% c("Oregon","California")) %>% 
  arrange(province)

wino
```

## Algorithm

1. Select the best attribute -> $A$ 
2. Assign $A$ as the decision attribute (test case) for the `NODE`. 
3. For each value of $a \in A$, create a new descendant of the `NODE`. 
4. Sort the training examples to the appropriate descendant node leaf.
5. If examples are perfectly classified, then `STOP` else iterate over the new leaf nodes.

## Visualize

![](https://upload.wikimedia.org/wikipedia/commons/8/87/Beachdecisiontree.png)

## In practice

- Where are going on vacation?
  - If top 25 city in US, say city.
    - Chicago
  - If US but not top 25 city, say state.
    - Utah
  - If not US, say nation-state.
    - Colombia

## Information Gain

- The \emph{information content} of a piece of information is how "surprising" it is.
- In sports, perhaps, wins above replacement.
- In grades, perhaps, standard deviations above the mean
- In weather, perhaps, date of a rainstorm in desert vs in rainforest.

## Example

- I tell you 123456 is *not* going to win the lottery
  - Very little information, and very unsurprising. 
- If I tell you 123456 *will* win the lottery
  - Very high information, very surprising. 


## Formula 

$$ 
I(p) = \log \left(\frac{1}{p}\right)
$$

- If $p = 1$, information is $0$
- As $p$ becomes small, $I(p)$ grows. 

## Datasets

- The **entropy** of a dataset is its "average information content":

$$
{\rm Entropy}=\sum_{i=1}(p_i)\log\left(\frac{1}{p_i}\right) = \sum - p_i\log(p_i)
$$

- $p$ is the proportion of the class under consideration. 
- If we have only one category, then $p_i = 1$ and entropy is 0 (no "disorder"). 

## Samples

- It rains 36 days per year in Phoenix.
- There are exactly 360 days per year [src](https://en.wikipedia.org/wiki/360-day_calendar)
- The "parent" node decides if a day rains, or not, and sends to other decision makers.
    - $p_0 = .9$: No rain.
    - $p_1 = .1$: Yes rain.

## Entropy Calculation

$$
\begin{align}
{\rm Entropy} &= \sum - p_i\log(p_i) \\
&= -.9 * \log(.9) -.1 * \log(.1) 
\end{align}
$$

```{r entphx}
entropy <- -.9 * log(.9) -.1 * log(.1)
entropy
```


## Portland

- In Portland it rains 153-164 days a year.
    - That is exactly half of 365
    - (Don't check)

## Entropy Calculation

$$
\begin{align}
{\rm Entropy} &= \sum - p_i\log(p_i) \\
&= -.5 * \log(.5) -.5 * \log(.5)  \\
&= -\log(.5) 
\end{align}
$$

```{r entpdx}
entropy <- -log(.5)
entropy
```

## Surpised?

- It is more suprising to correctly guess half of days than correctly guess one tenth of days.


## Optimize

- A decision tree looks to determine the optimal binary split
- Said split maximizes *information gain*:
    - The entropy of the parent, less
    - The entropy of the child nodes
        - Averaged
- Say probability of snow given precipitation

## Exercise

- Say we wish to classify wines by province.
    - We could first see if they are fruity
        - "fruit" $\in$ `desc`
    - We could first see if they are tannic.
        - "tanni" $\in$ `desc`
- Which is better?


## Split the data 

```{r split}
wine_index <- createDataPartition(wino$province, p = 0.8, list = FALSE)
train <- wino[ wine_index, ]
test <- wino[-wine_index, ]

table(train$province)
```

## Fit a basic model

```{r rpart}
ctrl <- trainControl(method = "cv")

fit <- train(province ~ .,
             data = train, 
             method = "rpart",
             trControl = ctrl,
             metric = "Kappa")
fit$finalModel
```
## Confusion Matrix

```{r}
pred <- predict(fit, newdata=test)
confusionMatrix(factor(pred), factor(test$province))
```

## Let's tune

- By setting a tune control we can try more trees.

```{r}
fit <- train(province ~ .,
             data = train, 
             method = "rpart",
             trControl = ctrl,
             tuneLength = 15,
             metric = "Kappa")
fit$finalModel
```

## Confusion Matrix

```{r}
pred <- predict(fit, newdata=test)
confusionMatrix(factor(pred), factor(test$province))
```

## Results

- Kappa 0.3538 -> 0.4668
- Mostly by finding more Oregon wines.

## Variable Importance

- Permutation importance
- Average split quality

```{r}
fit$finalModel$variable.importance
```

## Potential Overfitting

- Should we prune on...

  - Depth?
  - Class size?
  - Complexity?
  - Minimum Information Gain?


## Weighted Models

- Remember weights?
  - California is TOO BIG
  - Repeal Prop 13
- Where do these weights come from?

```{r}
weight_train <- train %>% 
  mutate(weights=case_when(
    province=="Burgundy" ~ 3,
    province=="California" ~ 1,
    province=="Casablanca_Valley" ~ 37,
    province=="Marlborough" ~ 18.5,
    province=="New_York" ~ 37,
    province=="Oregon" ~ 1.4))
```

## Progressive overload

```{r}
fit <- train(province ~ .,
             data = train, 
             method = "rpart",
             tuneLength = 10,
             weights = weight_train$weights,
             trControl = ctrl)
fit$finalModel
```

## Confusion Matrix

```{r}
pred <- predict(fit, newdata=test)
confusionMatrix(factor(pred), factor(test$province))
```

## Tune grids

```{r}
hyperparam_grid = expand.grid(cp = c(0, 0.01, 0.05, 0.1))

fit <- train(province ~ .,
             data = train, 
             method = "rpart",
             trControl = ctrl,
             tuneGrid = hyperparam_grid,
             metric = "Kappa")
fit$finalModel
```
## Confusion Matrix

```{r}
pred <- predict(fit, newdata=test)
confusionMatrix(factor(pred), factor(test$province))
```

## Exercise?

- Can you try to *overfit* as much as possible? 
  - Set `cp = 0`, 
  - Generate tons of features, 
  - See how out of sample performance is?
- `cp` = complexity parameter.
- Solution on next slide, more or less.

## Solution

```{r}
fit <- train(province ~ .,
             data = train, 
             method = "rpart",
             trControl = trainControl(method="none"),
             tuneGrid = data.frame(cp = 0))
fit
```


## Confusion Matrix

```{r}
pred <- predict(fit, newdata=test)
confusionMatrix(factor(pred), factor(test$province))
```

# Variance 


[Understanding the Bias-Variance Tradeoff by Scott Fortman-Roe](Understanding the Bias-Variance Tradeoff)

```{=html}
<table>
    <tbody><tr>
        <th></th>
        <td>
            Low Variance
        </td>
        <td>
            High Variance
        </td>
    </tr>
    <tr>
        <td class="r90">
            Low Bias
        </td>
        <td>
            <div id="bullsEyeLBLV"><svg width="200" height="200" style="border: medium;"><circle class="ring" cx="100" cy="100" r="90.9090909090909" style="fill: white;"></circle><circle class="ring" cx="100" cy="100" r="62.5" style="fill: rgb(118, 179, 216);"></circle><circle class="ring" cx="100" cy="100" r="36.36363636363637" style="fill: white;"></circle><circle class="ring" cx="100" cy="100" r="13.333333333333334" style="fill: rgb(193, 47, 62);"></circle><circle class="arrow" cx="95.2417299219265" cy="98.78231506348611" r="2.5"></circle><circle class="arrow" cx="94.98877477792978" cy="98.35369950854978" r="2.5"></circle><circle class="arrow" cx="98.64499086445454" cy="90.1611985381718" r="2.5"></circle><circle class="arrow" cx="98.29842135491629" cy="103.46102989370513" r="2.5"></circle><circle class="arrow" cx="95.79393665077546" cy="100.68784532768193" r="2.5"></circle><circle class="arrow" cx="99.98795100308158" cy="101.81130596018545" r="2.5"></circle><circle class="arrow" cx="89.34113757239132" cy="96.3757227502656" r="2.5"></circle><circle class="arrow" cx="92.93663382940163" cy="93.85728904385391" r="2.5"></circle><circle class="arrow" cx="91.27400877365645" cy="92.74432495644449" r="2.5"></circle><circle class="arrow" cx="98.35171143651966" cy="90.25639751501993" r="2.5"></circle><circle class="arrow" cx="89.84479267773501" cy="89.80271518867866" r="2.5"></circle><circle class="arrow" cx="102.92811906669978" cy="102.31429122988574" r="2.5"></circle><circle class="arrow" cx="98.43292822053172" cy="94.33502755999424" r="2.5"></circle><circle class="arrow" cx="100.03977392090455" cy="88.32650985799843" r="2.5"></circle><circle class="arrow" cx="90.45638062240717" cy="91.38063914598084" r="2.5"></circle><circle class="arrow" cx="91.814500886036" cy="105.83526025811605" r="2.5"></circle><circle class="arrow" cx="93.48689316677351" cy="94.63423346645308" r="2.5"></circle><circle class="arrow" cx="93.2603587976381" cy="98.98875335330786" r="2.5"></circle><circle class="arrow" cx="96.46568926721694" cy="92.17361145704697" r="2.5"></circle><circle class="arrow" cx="91.90571922505733" cy="88.55473443715668" r="2.5"></circle><circle class="arrow" cx="103.20972223139" cy="96.86760959426755" r="2.5"></circle><circle class="arrow" cx="90.45135687581899" cy="102.46241789283684" r="2.5"></circle><circle class="arrow" cx="96.83775784414765" cy="109.14141342737429" r="2.5"></circle><circle class="arrow" cx="88.47009267490499" cy="102.33783592020343" r="2.5"></circle><circle class="arrow" cx="103.08969765418682" cy="99.25599334201341" r="2.5"></circle></svg></div>
        </td>
        <td>
            <div id="bullsEyeLBHV"><svg width="200" height="200" style="border: medium;"><circle class="ring" cx="100" cy="100" r="90.9090909090909" style="fill: white;"></circle><circle class="ring" cx="100" cy="100" r="62.5" style="fill: rgb(118, 179, 216);"></circle><circle class="ring" cx="100" cy="100" r="36.36363636363637" style="fill: white;"></circle><circle class="ring" cx="100" cy="100" r="13.333333333333334" style="fill: rgb(193, 47, 62);"></circle><circle class="arrow" cx="135.30208525468566" cy="109.53667975411805" r="2.5"></circle><circle class="arrow" cx="128.43144410223016" cy="73.55338417474053" r="2.5"></circle><circle class="arrow" cx="62.46329544014869" cy="115.09033226489368" r="2.5"></circle><circle class="arrow" cx="126.27343247249648" cy="66.14548779266283" r="2.5"></circle><circle class="arrow" cx="87.5311440807998" cy="96.79804569434428" r="2.5"></circle><circle class="arrow" cx="89.86553587619488" cy="73.31114567408036" r="2.5"></circle><circle class="arrow" cx="72.14224642068129" cy="71.56278538776868" r="2.5"></circle><circle class="arrow" cx="133.67943947732815" cy="99.4197165637036" r="2.5"></circle><circle class="arrow" cx="95.39079143636448" cy="118.7690125167322" r="2.5"></circle><circle class="arrow" cx="94.97114920044513" cy="89.90155532250407" r="2.5"></circle><circle class="arrow" cx="97.76397547830885" cy="104.44717591002323" r="2.5"></circle><circle class="arrow" cx="89.30844911697717" cy="85.24513430882038" r="2.5"></circle><circle class="arrow" cx="84.9656556483016" cy="110.46435847056802" r="2.5"></circle><circle class="arrow" cx="94.47025121828862" cy="111.22511714783076" r="2.5"></circle><circle class="arrow" cx="81.19000260405745" cy="87.45496868419164" r="2.5"></circle><circle class="arrow" cx="52.31412324140461" cy="109.70597207613788" r="2.5"></circle><circle class="arrow" cx="84.07493934147824" cy="121.38043888867571" r="2.5"></circle><circle class="arrow" cx="79.96730411760538" cy="96.32433545651551" r="2.5"></circle><circle class="arrow" cx="97.5698900562818" cy="104.32403135668825" r="2.5"></circle><circle class="arrow" cx="64.83727832847151" cy="102.24003307409187" r="2.5"></circle><circle class="arrow" cx="96.43564661227427" cy="111.54952202719106" r="2.5"></circle><circle class="arrow" cx="113.09578479104827" cy="118.67825786463287" r="2.5"></circle><circle class="arrow" cx="103.04118268265579" cy="107.66971927290987" r="2.5"></circle><circle class="arrow" cx="76.27875910415312" cy="128.31617509558063" r="2.5"></circle><circle class="arrow" cx="129.12754678814048" cy="95.5293181876095" r="2.5"></circle></svg></div>
        </td>
    </tr>
    <tr>
        <td class="r90">
            High Bias
        </td>
        <td>
            <div id="bullsEyeHBLV"><svg width="200" height="200" style="border: medium;"><circle class="ring" cx="100" cy="100" r="90.9090909090909" style="fill: white;"></circle><circle class="ring" cx="100" cy="100" r="62.5" style="fill: rgb(118, 179, 216);"></circle><circle class="ring" cx="100" cy="100" r="36.36363636363637" style="fill: white;"></circle><circle class="ring" cx="100" cy="100" r="13.333333333333334" style="fill: rgb(193, 47, 62);"></circle><circle class="arrow" cx="93.27374996049726" cy="42.328637258595556" r="2.5"></circle><circle class="arrow" cx="105.52581398889991" cy="44.93556686811315" r="2.5"></circle><circle class="arrow" cx="95.26101246647794" cy="46.7755302240622" r="2.5"></circle><circle class="arrow" cx="112.05060873387474" cy="52.86933183374843" r="2.5"></circle><circle class="arrow" cx="110.17742629325286" cy="57.67330743618805" r="2.5"></circle><circle class="arrow" cx="102.62633835599787" cy="47.63329341720682" r="2.5"></circle><circle class="arrow" cx="104.97396150323235" cy="53.86880949845978" r="2.5"></circle><circle class="arrow" cx="103.76256282114713" cy="48.3116835441338" r="2.5"></circle><circle class="arrow" cx="97.84406081280112" cy="48.05373982750756" r="2.5"></circle><circle class="arrow" cx="105.47413674536088" cy="60.8819662990806" r="2.5"></circle><circle class="arrow" cx="101.32433258243718" cy="50.42653499700785" r="2.5"></circle><circle class="arrow" cx="107.75895816267607" cy="53.5123837318865" r="2.5"></circle><circle class="arrow" cx="102.34151233023265" cy="43.22134477028904" r="2.5"></circle><circle class="arrow" cx="102.74836927714325" cy="48.99085752653546" r="2.5"></circle><circle class="arrow" cx="103.5514955929248" cy="50.700765047971295" r="2.5"></circle><circle class="arrow" cx="99.66489163788832" cy="52.960882058099266" r="2.5"></circle><circle class="arrow" cx="104.03334679960989" cy="50.56365067938392" r="2.5"></circle><circle class="arrow" cx="112.40294162147084" cy="51.05603578517508" r="2.5"></circle><circle class="arrow" cx="103.77185736249525" cy="48.69041724555987" r="2.5"></circle><circle class="arrow" cx="105.75923213844915" cy="52.16305294199713" r="2.5"></circle><circle class="arrow" cx="100.02620538056256" cy="45.76240834088125" r="2.5"></circle><circle class="arrow" cx="92.6900668670068" cy="49.1548990106344" r="2.5"></circle><circle class="arrow" cx="102.52276709795272" cy="54.66889873214721" r="2.5"></circle><circle class="arrow" cx="92.63240239912123" cy="54.816287121818064" r="2.5"></circle><circle class="arrow" cx="93.67433387840616" cy="44.1308081872826" r="2.5"></circle></svg></div>
        </td>
        <td>
            <div id="bullsEyeHBHV"><svg width="200" height="200" style="border: medium;"><circle class="ring" cx="100" cy="100" r="90.9090909090909" style="fill: white;"></circle><circle class="ring" cx="100" cy="100" r="62.5" style="fill: rgb(118, 179, 216);"></circle><circle class="ring" cx="100" cy="100" r="36.36363636363637" style="fill: white;"></circle><circle class="ring" cx="100" cy="100" r="13.333333333333334" style="fill: rgb(193, 47, 62);"></circle><circle class="arrow" cx="103.71226190486529" cy="35.245951954634805" r="2.5"></circle><circle class="arrow" cx="85.66758527040301" cy="67.2447331281883" r="2.5"></circle><circle class="arrow" cx="62.28568550397415" cy="32.140931024292215" r="2.5"></circle><circle class="arrow" cx="44.453050499204494" cy="27.594136705300244" r="2.5"></circle><circle class="arrow" cx="65.5267486415562" cy="34.97572973546029" r="2.5"></circle><circle class="arrow" cx="52.31696098385054" cy="63.901284157820186" r="2.5"></circle><circle class="arrow" cx="48.16744830325244" cy="18.83190221128146" r="2.5"></circle><circle class="arrow" cx="74.76574746979051" cy="45.429332524802895" r="2.5"></circle><circle class="arrow" cx="93.43125456438462" cy="10.626913149263643" r="2.5"></circle><circle class="arrow" cx="68.5906869780192" cy="46.07543950204163" r="2.5"></circle><circle class="arrow" cx="76.78805778650607" cy="37.09650785588681" r="2.5"></circle><circle class="arrow" cx="72.76655176927713" cy="50.971128326292146" r="2.5"></circle><circle class="arrow" cx="42.724353763919886" cy="61.927273912222816" r="2.5"></circle><circle class="arrow" cx="112.20625419743854" cy="39.15213297199204" r="2.5"></circle><circle class="arrow" cx="88.36900320964696" cy="38.854779509806455" r="2.5"></circle><circle class="arrow" cx="63.66053914630819" cy="56.46641986123598" r="2.5"></circle><circle class="arrow" cx="68.96191500872605" cy="43.16404842444755" r="2.5"></circle><circle class="arrow" cx="35.635519046608465" cy="28.74648178852344" r="2.5"></circle><circle class="arrow" cx="68.60182810836173" cy="59.740609767146466" r="2.5"></circle><circle class="arrow" cx="95.5153403974868" cy="25.547080430710736" r="2.5"></circle><circle class="arrow" cx="46.37075852226722" cy="19.62606830684345" r="2.5"></circle><circle class="arrow" cx="89.9950666768221" cy="50.71556277512999" r="2.5"></circle><circle class="arrow" cx="64.45958584171213" cy="42.52949470919935" r="2.5"></circle><circle class="arrow" cx="95.57718864645554" cy="4.197361489016814" r="2.5"></circle><circle class="arrow" cx="70.63354751617322" cy="32.127872854044625" r="2.5"></circle></svg></div>
        </td>
    </tr>
</tbody></table>
```


## Random Forest (subsets of features)

![](images/randomForest.jpg)

```{r}
fit <- train(province ~ .,
             data = train, 
             method = "rf",
             trControl = ctrl)

fit
```

## Confusion Matrix

```{r}
pred <- predict(fit, newdata=test)
confusionMatrix(factor(pred),factor(test$province))
```

## Bagging (subsets of data)

- "Bootstrap aggregating"
- Builds multiple models with bootstrap samples (combinations with repetitions) using a single algorithm. 
- The models’ predictions are combined with voting (for classification) or averaging (for numeric prediction). 
- Voting means the bagging model’s prediction is based on the majority of learners’ prediction for a class. 

## Treebag

```{r}

fit <- train(province ~ .,
             data = train, 
             trControl = ctrl,
             method = "treebag")

fit
```

## Confusion Matrix

```{r}
pred <- predict(fit, newdata=test)
confusionMatrix(factor(pred),factor(test$province))
```

## Random forest with class weights

```{r}

fit <- train(province ~ .,
             data = train, 
             method = "rf",
             weights = weight_train$weights,
             trControl = ctrl)

fit
```

## Confusion Matrix

```{r}
pred <- predict(fit, newdata=test)
confusionMatrix(factor(pred),factor(test$province))
```

             

## Model Comparison

```{r}
cl <- makePSOCKcluster(3)
registerDoParallel(cl)
system.time({
  rpart.fit <- train(province~., data = train, method="rpart",trControl=ctrl);
  treebag.fit <- train(province~., data = train, method="treebag", trControl=ctrl);
  rf.fit <- train(province~., data = train, method="rf", trControl=ctrl);
})
```

## Results
```{r}
stopCluster(cl) # close multi-core cluster
rm(cl)

results <- resamples(list(DecisionTree=rpart.fit, BaggedTree=treebag.fit, RandomForest=rf.fit))
summary(results)
```


## Pros

- Easy to use and understand. 
- Can handle both categorical and numerical data. 
- Resistant to outliers, hence require little data preprocessing. 
- New features can be easily added. 
- Can be used to build larger classifiers by using ensemble methods.

## Cons

- Prone to overfitting. 
- Require some kind of measurement as to how well they are doing. 
- Need to be careful with parameter tuning. 
- Can create biased learned trees if some classes dominate.


