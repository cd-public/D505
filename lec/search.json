[
  {
    "objectID": "Session01.html#agenda",
    "href": "Session01.html#agenda",
    "title": "Machine Learning Overview",
    "section": "Agenda",
    "text": "Agenda\n\nCourse Overview\nReview of Regression\nDinner Break\nClassification and Ethics\nBasic Feature Engineering\nVocabulary"
  },
  {
    "objectID": "Session01.html#expectations-and-assignments",
    "href": "Session01.html#expectations-and-assignments",
    "title": "Machine Learning Overview",
    "section": "Expectations and assignments",
    "text": "Expectations and assignments\n\nHomework assignments\nExams\nModeling Project\nCourse Policies\nMy expectations for you"
  },
  {
    "objectID": "Session01.html#about-me",
    "href": "Session01.html#about-me",
    "title": "Machine Learning Overview",
    "section": "About me",
    "text": "About me\n\nBA Mathematics, BS Computer Science (UChicago)\nMS, PhD Computer Science (UNC Chapel Hill)\nData mining, formal analysis, complex models\nJoined Willamette 2021"
  },
  {
    "objectID": "Session01.html#about-you",
    "href": "Session01.html#about-you",
    "title": "Machine Learning Overview",
    "section": "About you?",
    "text": "About you?\n\nBackground\nGoals for this program and/or course"
  },
  {
    "objectID": "Session01.html#basic-concepts-in-machine-learning",
    "href": "Session01.html#basic-concepts-in-machine-learning",
    "title": "Machine Learning Overview",
    "section": "Basic concepts in Machine Learning",
    "text": "Basic concepts in Machine Learning\n\nWhat is a data scientist?\nWhat is machine learning?\nWhat is the role of judgment in machine learning?\nWhat are the differences between machine learning, statistics and econometrics?\nWhen is “mere” correlation enough? When is it not?"
  },
  {
    "objectID": "Session01.html#setup",
    "href": "Session01.html#setup",
    "title": "Machine Learning Overview",
    "section": "Setup",
    "text": "Setup\n\nWe will work with a wine dataset that is enormous.\n\nJust to render a bit quickly, take a sample.\nYou are welcome to work with the full dataset!\n\n\n\nwine &lt;- readRDS(gzcon(url(\"https://cd-public.github.io/courses/rmls25/dat/wine.rds\")))\nwine &lt;- wine %&gt;% drop_na(points, price)\n# performance concession\n# wall &lt;- wine\n# wine = wall[sample(nrow(wall), 100), ]\nsummary(wine)\n\n       id           country          description        designation       \n Min.   :     1   Length:89556       Length:89556       Length:89556      \n 1st Qu.: 32742   Class :character   Class :character   Class :character  \n Median : 65613   Mode  :character   Mode  :character   Mode  :character  \n Mean   : 65192                                                           \n 3rd Qu.: 97738                                                           \n Max.   :129970                                                           \n     points           price           province           region_1        \n Min.   : 80.00   Min.   :   4.00   Length:89556       Length:89556      \n 1st Qu.: 87.00   1st Qu.:  17.00   Class :character   Class :character  \n Median : 89.00   Median :  25.00   Mode  :character   Mode  :character  \n Mean   : 88.65   Mean   :  35.56                                        \n 3rd Qu.: 91.00   3rd Qu.:  42.00                                        \n Max.   :100.00   Max.   :3300.00                                        \n   region_2         taster_name        taster_twitter_handle    title          \n Length:89556       Length:89556       Length:89556          Length:89556      \n Class :character   Class :character   Class :character      Class :character  \n Mode  :character   Mode  :character   Mode  :character      Mode  :character  \n                                                                               \n                                                                               \n                                                                               \n   variety             winery               year     \n Length:89556       Length:89556       Min.   :1995  \n Class :character   Class :character   1st Qu.:2010  \n Mode  :character   Mode  :character   Median :2012  \n                                       Mean   :2011  \n                                       3rd Qu.:2014  \n                                       Max.   :2015"
  },
  {
    "objectID": "Session01.html#single-variable",
    "href": "Session01.html#single-variable",
    "title": "Machine Learning Overview",
    "section": "Single Variable",
    "text": "Single Variable\n\nPick the poshest province.\n\n\nwine &lt;- wine %&gt;%\n  mutate(bordeaux = (province == \"Bordeaux\"))\nwine &lt;- wine %&gt;% drop_na(bordeaux)\ntop_n(wine, 10, bordeaux)\n\n# A tibble: 3,774 × 16\n      id country description designation points price province region_1 region_2\n   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   \n 1    53 France  Fruity and… La Fleur d…     85    15 Bordeaux Bordeau… &lt;NA&gt;    \n 2   136 France  This wine'… &lt;NA&gt;            91    50 Bordeaux Saint-É… &lt;NA&gt;    \n 3   419 France  A smooth, … &lt;NA&gt;            89    20 Bordeaux Graves   &lt;NA&gt;    \n 4   477 France  An interes… &lt;NA&gt;            92    65 Bordeaux Pomerol  &lt;NA&gt;    \n 5   573 France  Fruity and… &lt;NA&gt;            89    14 Bordeaux Bordeau… &lt;NA&gt;    \n 6   575 France  This is a … &lt;NA&gt;            89    14 Bordeaux Bordeau… &lt;NA&gt;    \n 7   576 France  From a Gra… Les Terras…     89    37 Bordeaux Saint-É… &lt;NA&gt;    \n 8   578 France  A ripe per… Château Je…     89    15 Bordeaux Bordeaux &lt;NA&gt;    \n 9   792 France  The 45% Ca… La Sérénit…     90    30 Bordeaux Médoc    &lt;NA&gt;    \n10   795 France  This is th… Divin de C…     90    35 Bordeaux Saint-É… &lt;NA&gt;    \n# ℹ 3,764 more rows\n# ℹ 7 more variables: taster_name &lt;chr&gt;, taster_twitter_handle &lt;chr&gt;,\n#   title &lt;chr&gt;, variety &lt;chr&gt;, winery &lt;chr&gt;, year &lt;dbl&gt;, bordeaux &lt;lgl&gt;"
  },
  {
    "objectID": "Session01.html#lets-draw-it",
    "href": "Session01.html#lets-draw-it",
    "title": "Machine Learning Overview",
    "section": "Let’s draw it",
    "text": "Let’s draw it"
  },
  {
    "objectID": "Session01.html#multiple-regression",
    "href": "Session01.html#multiple-regression",
    "title": "Machine Learning Overview",
    "section": "Multiple regression",
    "text": "Multiple regression\n\nm2 &lt;- lm(price ~ points + bordeaux, data = wine)\nget_regression_table(m2)\n\n# A tibble: 3 × 7\n  term         estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept     -492.       3.97     -124.        0  -500.    -484.  \n2 points           5.95     0.045     133.        0     5.86     6.03\n3 bordeauxTRUE     8.70     0.661      13.2       0     7.41    10.0"
  },
  {
    "objectID": "Session01.html#lets-draw-it-1",
    "href": "Session01.html#lets-draw-it-1",
    "title": "Machine Learning Overview",
    "section": "Let’s draw it",
    "text": "Let’s draw it"
  },
  {
    "objectID": "Session01.html#how-about-with-an-interaction",
    "href": "Session01.html#how-about-with-an-interaction",
    "title": "Machine Learning Overview",
    "section": "How about with an interaction?",
    "text": "How about with an interaction?\n\nm3 &lt;- lm(price ~ points * bordeaux, data = wine)\nget_regression_table(m3)\n\n# A tibble: 4 × 7\n  term                estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept            -461.       4.04     -114.        0  -469.    -453.  \n2 points                  5.60     0.045     123.        0     5.51     5.69\n3 bordeauxTRUE         -666.      18.8       -35.5       0  -703.    -629.  \n4 points:bordeauxTRUE     7.66     0.213      36.0       0     7.24     8.07"
  },
  {
    "objectID": "Session01.html#lets-draw-it-2",
    "href": "Session01.html#lets-draw-it-2",
    "title": "Machine Learning Overview",
    "section": "Let’s draw it",
    "text": "Let’s draw it"
  },
  {
    "objectID": "Session01.html#model-diagnostics",
    "href": "Session01.html#model-diagnostics",
    "title": "Machine Learning Overview",
    "section": "Model diagnostics",
    "text": "Model diagnostics\n\nget_regression_summaries(m1)\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared   mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.164         0.164 1579.  39.7  39.7    17497.       0     1 89503\n\nget_regression_summaries(m2)\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared   mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.165         0.165 1576.  39.7  39.7     8852.       0     2 89503\n\nget_regression_summaries(m3)\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared   mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.177         0.177 1553.  39.4  39.4     6418.       0     3 89503"
  },
  {
    "objectID": "Session01.html#split-sample-using-caret",
    "href": "Session01.html#split-sample-using-caret",
    "title": "Machine Learning Overview",
    "section": "Split sample using Caret",
    "text": "Split sample using Caret\n\nset.seed(505)\ntrain_index &lt;- createDataPartition(wine$price, times = 1, p = 0.8, list = FALSE)\ntrain &lt;- wine[train_index, ]\ntest &lt;- wine[-train_index, ]\nhead(test)\n\n# A tibble: 6 × 16\n     id country description  designation points price province region_1 region_2\n  &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;        &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   \n1     8 Germany Savory drie… Shine           87    12 Rheinhe… &lt;NA&gt;     &lt;NA&gt;    \n2    19 US      Red fruit a… &lt;NA&gt;            87    32 Virginia Virginia &lt;NA&gt;    \n3    20 US      Ripe aromas… Vin de Mai…     87    23 Virginia Virginia &lt;NA&gt;    \n4    28 Italy   Aromas sugg… Mascaria B…     87    17 Sicily … Cerasuo… &lt;NA&gt;    \n5    59 US      Aromas of c… &lt;NA&gt;            86    55 Washing… Columbi… Columbi…\n6    61 Italy   This densel… Prugneto        86    17 Central… Romagna  &lt;NA&gt;    \n# ℹ 7 more variables: taster_name &lt;chr&gt;, taster_twitter_handle &lt;chr&gt;,\n#   title &lt;chr&gt;, variety &lt;chr&gt;, winery &lt;chr&gt;, year &lt;dbl&gt;, bordeaux &lt;lgl&gt;"
  },
  {
    "objectID": "Session01.html#compare-rmse-across-models",
    "href": "Session01.html#compare-rmse-across-models",
    "title": "Machine Learning Overview",
    "section": "Compare RMSE across models",
    "text": "Compare RMSE across models\n\nRetrain on models on the training set\n\n\nms &lt;- list(\n  lm(price ~ points, data = train),\n  lm(price ~ points + bordeaux, data = train),\n  lm(price ~ points * bordeaux, data = train)\n)\n\n\nTest them all under the same conditions.\n\n\nmap(ms, function(m) {\n  get_regression_points(m, newdata = test) %&gt;%\n    drop_na(residual) %&gt;%\n    mutate(sq_residuals = residual^2) %&gt;%\n    summarize(rmse = sqrt(mean(sq_residuals))) %&gt;%\n    pluck(\"rmse\")\n}) %&gt;% unlist()\n\n[1] 36.00999 36.00970 35.85276"
  },
  {
    "objectID": "Session01.html#exercise-30m",
    "href": "Session01.html#exercise-30m",
    "title": "Machine Learning Overview",
    "section": "Exercise (30m)",
    "text": "Exercise (30m)\n\nBreak into groups\nLoad the wine data set\nCreate a visualization of the relationship between points and price\nBonus: Color the observations based on whether the wine is from Bordeaux\nBonus+: Include regression lines\nBonus++: Pick novel binary variable from “designation” column…"
  },
  {
    "objectID": "Session01.html#plot-1",
    "href": "Session01.html#plot-1",
    "title": "Machine Learning Overview",
    "section": "Plot 1",
    "text": "Plot 1\n\nCreate a visualization of the relationship between points and price.\n\n\nwine %&gt;%\n  drop_na(points, price) %&gt;%\n  ggplot(aes(x=points, y=price)) + geom_point()"
  },
  {
    "objectID": "Session01.html#plot-2",
    "href": "Session01.html#plot-2",
    "title": "Machine Learning Overview",
    "section": "Plot 2",
    "text": "Plot 2\n\nColor the observations based on whether the wine is from Bordeaux\n\n\nwine %&gt;%\n  drop_na(points, price) %&gt;%\n  ggplot(aes(x=points, y=price, color=bordeaux)) + geom_point()"
  },
  {
    "objectID": "Session01.html#plot-3",
    "href": "Session01.html#plot-3",
    "title": "Machine Learning Overview",
    "section": "Plot 3",
    "text": "Plot 3\n\nwine %&gt;%\n  drop_na(points, price) %&gt;%\n  ggplot(aes(x=points, y=price)) + geom_point()"
  },
  {
    "objectID": "Session01.html#plot-4",
    "href": "Session01.html#plot-4",
    "title": "Machine Learning Overview",
    "section": "Plot 4",
    "text": "Plot 4\n\nInclude regression lines for Bordeaux or Neaux\n\n\nwine %&gt;%\n  drop_na(points, price, bordeaux) %&gt;%\n  ggplot(aes(x=points, y=price, color=bordeaux)) + geom_point() +\n  geom_smooth(method=lm)"
  },
  {
    "objectID": "Session01.html#extra-1",
    "href": "Session01.html#extra-1",
    "title": "Machine Learning Overview",
    "section": "Extra 1",
    "text": "Extra 1\n\nLet’s look at “reserve”.\n\n\nwine %&gt;%\n  mutate(reserve=grepl(\"Reserve\", designation)) %&gt;%\n  ggplot(aes(x=points, y=price, color=reserve)) + geom_point() +\n  geom_smooth(method=lm)"
  },
  {
    "objectID": "Session01.html#extra-2",
    "href": "Session01.html#extra-2",
    "title": "Machine Learning Overview",
    "section": "Extra 2",
    "text": "Extra 2\n\nAnglophones to Francophiles.\n\n\nwine %&gt;%\n  mutate(reservæ=grepl(\"Reserve\", designation, ignore.case = TRUE) | \n           grepl(\"Reserva\", designation, ignore.case = TRUE)) %&gt;%\n  ggplot(aes(x=points, y=price, color=reservæ)) + geom_point() +\n  geom_smooth(method=lm)"
  },
  {
    "objectID": "Session01.html#extra-3",
    "href": "Session01.html#extra-3",
    "title": "Machine Learning Overview",
    "section": "Extra 3",
    "text": "Extra 3\n\n\n\nCross the Alps.\n\nIe or Iota (asomtavruli Ⴢ, nuskhuri ⴢ, mkhedruli ჲ, mtavruli Ჲ) is the 15th letter of the three Georgian scripts\n\n\n\n\nwine %&gt;%\n  mutate(rჂservæ=grepl(\"Reserve|Reserva|Riserva\", designation, ignore.case = TRUE)) %&gt;%\n  ggplot(aes(x=points, y=price, color=rჂservæ)) + geom_point() +\n  geom_smooth(method=lm)"
  },
  {
    "objectID": "Session01.html#the-math-of-it",
    "href": "Session01.html#the-math-of-it",
    "title": "Machine Learning Overview",
    "section": "The math of it…",
    "text": "The math of it…\n\nSuppose I’m trying to predict sex based on height.\n\nDon’t do this in real life (obviously).\n\nWe start by\n\ndefining the outcome and predictors, and…\ncreating training and test data."
  },
  {
    "objectID": "Session01.html#guessing.",
    "href": "Session01.html#guessing.",
    "title": "Machine Learning Overview",
    "section": "Guessing.",
    "text": "Guessing.\nLet’s start by developing the simplest possible machine algorithm: guessing the outcome.\n\ny_hat &lt;- sample(c(\"Male\", \"Female\"), length(test_index), replace = TRUE) %&gt;%\n  factor(levels = levels(test_set$sex))\n\nThe overall accuracy is simply defined as the overall proportion that is predicted correctly:\n\nmean(y_hat == test_set$sex)\n\n[1] 0.4990476"
  },
  {
    "objectID": "Session01.html#lets-do-better",
    "href": "Session01.html#lets-do-better",
    "title": "Machine Learning Overview",
    "section": "Let’s do better…",
    "text": "Let’s do better…\n\nsummary &lt;- heights %&gt;%\n  group_by(sex) %&gt;%\n  summarize(mean(height), sd(height))\nsummary\n\n# A tibble: 2 × 3\n  sex    `mean(height)` `sd(height)`\n  &lt;fct&gt;           &lt;dbl&gt;        &lt;dbl&gt;\n1 Female           64.9         3.76\n2 Male             69.3         3.61"
  },
  {
    "objectID": "Session01.html#a-simple-predictive-model",
    "href": "Session01.html#a-simple-predictive-model",
    "title": "Machine Learning Overview",
    "section": "A simple predictive model",
    "text": "A simple predictive model\n\nIdea: Predict \"Male\" if observation is within 2 standard deviations\n\n\nmale_mean_less_2sd &lt;- summary[2, ][\"mean(height)\"] - 2 * summary[2, ][\"sd(height)\"]\n\ny_hat &lt;- ifelse(x &gt; male_mean_less_2sd, \"Male\", \"Female\") %&gt;%\n  factor(levels = levels(test_set$sex))\n\nc(male_mean_less_2sd, mean(y == y_hat))\n\n$`mean(height)`\n[1] 62.09271\n\n[[2]]\n[1] 0.7733333\n\n\n\nThe accuracy goes up from ~0.50 to about ~0.80!!"
  },
  {
    "objectID": "Session01.html#lets-optimize",
    "href": "Session01.html#lets-optimize",
    "title": "Machine Learning Overview",
    "section": "Let’s optimize",
    "text": "Let’s optimize\n\ncutoff &lt;- seq(61, 70)\nget_accuracy &lt;- function(x) {\n  y_hat &lt;- ifelse(train_set$height &gt; x, \"Male\", \"Female\")\n  mean(y_hat == train_set$sex)\n}\naccuracy &lt;- map(cutoff, get_accuracy)\n\nunlist(accuracy)\n\n [1] 0.7752381 0.7866667 0.8152381 0.8342857 0.8209524 0.7904762 0.7314286\n [8] 0.6819048 0.5961905 0.5104762\n\n\n\nMost are much higher than 0.5!!"
  },
  {
    "objectID": "Session01.html#the-cutoff-resulting-in-this-accuracy-is",
    "href": "Session01.html#the-cutoff-resulting-in-this-accuracy-is",
    "title": "Machine Learning Overview",
    "section": "The cutoff resulting in this accuracy is:",
    "text": "The cutoff resulting in this accuracy is:\n\nbest_cutoff &lt;- cutoff[which.max(accuracy)]\nbest_cutoff\n\n[1] 64"
  },
  {
    "objectID": "Session01.html#how-does-it-do-on-the-test-data",
    "href": "Session01.html#how-does-it-do-on-the-test-data",
    "title": "Machine Learning Overview",
    "section": "How does it do on the test data?",
    "text": "How does it do on the test data?\n\ny_hat &lt;- ifelse(test_set$height &gt; best_cutoff, \"Male\", \"Female\")\nmean(y_hat == test_set$sex)\n\n[1] 0.8190476"
  },
  {
    "objectID": "Session01.html#confusion-matrix",
    "href": "Session01.html#confusion-matrix",
    "title": "Machine Learning Overview",
    "section": "Confusion matrix",
    "text": "Confusion matrix\n\ntable(predicted = y_hat, actual = test_set$sex) %&gt;%\n  as.data.frame() %&gt;%\n  ggplot(aes(x = predicted, y = actual)) +\n  geom_tile(aes(fill = Freq), color = \"white\") +\n  scale_fill_gradient(low = \"white\", high = \"blue\") +\n  geom_text(aes(label = Freq), vjust = \"center\", color = \"black\", size = 24) +\n  labs(title = \"Confusion Matrix\", x = \"Predicted\", y = \"Actual\")"
  },
  {
    "objectID": "Session01.html#accuracy-by-sex",
    "href": "Session01.html#accuracy-by-sex",
    "title": "Machine Learning Overview",
    "section": "Accuracy by sex",
    "text": "Accuracy by sex\n\ntest_set %&gt;%\n  mutate(y_hat = y_hat) %&gt;%\n  group_by(sex) %&gt;%\n  summarize(accuracy = mean(y_hat == sex))\n\n# A tibble: 2 × 2\n  sex    accuracy\n  &lt;fct&gt;     &lt;dbl&gt;\n1 Female    0.445\n2 Male      0.929\n\n\n \nIt’s raining men."
  },
  {
    "objectID": "Session01.html#moral-of-the-story",
    "href": "Session01.html#moral-of-the-story",
    "title": "Machine Learning Overview",
    "section": "Moral of the story",
    "text": "Moral of the story"
  },
  {
    "objectID": "Session01.html#other-ethical-issues",
    "href": "Session01.html#other-ethical-issues",
    "title": "Machine Learning Overview",
    "section": "Other ethical issues",
    "text": "Other ethical issues\n\n\n\nDemographic data\nProfit optimizing\nAutonomous cars\nRecommendation engines\n\n\n\nFair housing\nCriminal sentencing\nChoice of classification model\nDrone warfare"
  },
  {
    "objectID": "Session01.html#ml-terms",
    "href": "Session01.html#ml-terms",
    "title": "Machine Learning Overview",
    "section": "ML Terms",
    "text": "ML Terms\nDefinition of ML: using data to find a function that minimizes prediction error.\n\n\n\nFeatures\nVariables\nOutcome variable\nRegression\n\n\n\nRMSE\nClassification\nConfusion matrix\nSplit Samples"
  },
  {
    "objectID": "Session01.html#precision-recall-tradeoff",
    "href": "Session01.html#precision-recall-tradeoff",
    "title": "Machine Learning Overview",
    "section": "Precision-recall tradeoff",
    "text": "Precision-recall tradeoff\n\nPrecision: TP / (TP + FP)\nRecall: TP / (TP + FN)\nImagine I have a fraud-detection model that gives 1,000 credit card transactions each a risk score."
  },
  {
    "objectID": "Session01.html#precision-recall-tradeoff-1",
    "href": "Session01.html#precision-recall-tradeoff-1",
    "title": "Machine Learning Overview",
    "section": "Precision-recall tradeoff",
    "text": "Precision-recall tradeoff\n\nImagine I have a fraud-detection model that gives 1,000 credit card transactions each a risk score.\nThe company chooses a risk score cutoff of 77 (for some reason).\nThere are 18 transactions with risk above 77. 12 are actually fraud. 20 fraudulent transactions have risk below 77.\nWhat are precision, recall, and accuracy?"
  },
  {
    "objectID": "Session01.html#precision-recall-tradeoff-2",
    "href": "Session01.html#precision-recall-tradeoff-2",
    "title": "Machine Learning Overview",
    "section": "Precision-recall tradeoff",
    "text": "Precision-recall tradeoff\n\nPrecision: TP / (TP + FP)\nRecall: TP / (TP + FN)\nImage: Hands-on machine learning, A. Geron"
  },
  {
    "objectID": "Session01.html#precision-recall-tradeoff-3",
    "href": "Session01.html#precision-recall-tradeoff-3",
    "title": "Machine Learning Overview",
    "section": "Precision-recall tradeoff",
    "text": "Precision-recall tradeoff\n\nPrecision: TP / (TP + FP)\nRecall: TP / (TP + FN)\nImage: Hands-on machine learning, A. Geron"
  },
  {
    "objectID": "Session01.html#packages",
    "href": "Session01.html#packages",
    "title": "Machine Learning Overview",
    "section": "Packages",
    "text": "Packages\n\nToday I use the following libraries:\n\nlocal({r &lt;- getOption(\"repos\")\n       r[\"CRAN\"] &lt;- \"https://cran.r-project.org\" \n       options(repos=r)\n})\n# New?\ninstall.packages(\"tidyverse\")\ninstall.packages(\"moderndive\")\ninstall.packages(\"caret\")\ninstall.packages(\"dslabs\")\n# Just for the slides\ninstall.packages(\"thematic\")\n\nYou will have some but perhaps not others."
  },
  {
    "objectID": "Session01.html#libraries",
    "href": "Session01.html#libraries",
    "title": "Machine Learning Overview",
    "section": "Libraries",
    "text": "Libraries\n\nI’ll just include them upfront.\n\n\nlibrary(tidyverse)\nlibrary(moderndive)\nlibrary(caret)\nlibrary(dslabs)\n# Just for the slides\nlibrary(thematic)\ntheme_set(theme_dark())\nthematic_rmd(bg = \"#111\", fg = \"#eee\", accent = \"#eee\")"
  },
  {
    "objectID": "Session01.html#compare-rmse-across-models-1",
    "href": "Session01.html#compare-rmse-across-models-1",
    "title": "Machine Learning Overview",
    "section": "Compare RMSE across models",
    "text": "Compare RMSE across models\n\nget_regression_points(lm(price~points, data = train), newdata = test) %&gt;% \n  drop_na(residual) %&gt;%   mutate(sq_residuals = residual^2) %&gt;% \n  summarize(rmse = sqrt(mean(sq_residuals))) %&gt;% pluck(\"rmse\")\n\n[1] 25.84228\n\n\n\nget_regression_points(lm(price~points+bordeaux, data = train), newdata = test) %&gt;% \n  drop_na(residual) %&gt;%   mutate(sq_residuals = residual^2) %&gt;% \n  summarize(rmse = sqrt(mean(sq_residuals))) %&gt;% pluck(\"rmse\")\n\n[1] 25.83773\n\n\n\nget_regression_points(lm(price~points*bordeaux, data = train), newdata = test) %&gt;%   \n  drop_na(residual) %&gt;%  mutate(sq_residuals = residual^2) %&gt;% \n  summarize(rmse = sqrt(mean(sq_residuals))) %&gt;% pluck(\"rmse\")\n\n[1] 25.69329"
  },
  {
    "objectID": "Session01.html#factor",
    "href": "Session01.html#factor",
    "title": "Machine Learning Overview",
    "section": "Factor",
    "text": "Factor"
  },
  {
    "objectID": "Session01.html#plot",
    "href": "Session01.html#plot",
    "title": "Machine Learning Overview",
    "section": "Plot",
    "text": "Plot\n\nPoints vs. price.\n\n\nwine %&gt;%\n  ggplot(aes(x = points, y = price)) +\n  geom_smooth()"
  },
  {
    "objectID": "Session01.html#bonus",
    "href": "Session01.html#bonus",
    "title": "Machine Learning Overview",
    "section": "Bonus",
    "text": "Bonus\n\nColor the Bordeaux region.\n\n\nwine %&gt;%\n  ggplot(aes(x = points, y = price, color = bordeaux)) +\n  geom_smooth()"
  },
  {
    "objectID": "Session01.html#bonus-1",
    "href": "Session01.html#bonus-1",
    "title": "Machine Learning Overview",
    "section": "Bonus+",
    "text": "Bonus+\n\nInclude regression lines\n\n\nwine %&gt;%\n  mutate(m = predict(lm(price ~ points, data = wine))) %&gt;%\n  ggplot() +\n  geom_smooth(aes(x = points, y = price, color = bordeaux)) +\n  geom_line(aes(x = points, y = m), colour = \"magenta\")"
  },
  {
    "objectID": "Session01.html#bonus-2",
    "href": "Session01.html#bonus-2",
    "title": "Machine Learning Overview",
    "section": "Bonus++",
    "text": "Bonus++\n\nLet’s look at “reserve”.\n\n\nwine %&gt;%\n  mutate(reserve = grepl(\"Reserve\", designation)) %&gt;%\n  ggplot(aes(x = points, y = price, color = reserve)) +\n  geom_smooth()"
  },
  {
    "objectID": "Session01.html#bonus-3",
    "href": "Session01.html#bonus-3",
    "title": "Machine Learning Overview",
    "section": "Bonus",
    "text": "Bonus\n\nAnglophones to Francophiles.\n\n\nwine %&gt;%\n  mutate(reservæ = grepl(\"Reserve\", designation, ignore.case = TRUE) |\n    grepl(\"Reserva\", designation, ignore.case = TRUE)) %&gt;%\n  ggplot(aes(x = points, y = price, color = reservæ)) +\n  geom_smooth()"
  },
  {
    "objectID": "Session01.html#fin",
    "href": "Session01.html#fin",
    "title": "Machine Learning Overview",
    "section": "FIN",
    "text": "FIN\n\n\n\nCross the Alps.\n\nIe or Iota (asomtavruli Ⴢ, nuskhuri ⴢ, mkhedruli ჲ, mtavruli Ჲ) is the 15th letter of the three Georgian scripts\n\n\n\n\nwine %&gt;%\n  mutate(rჂservæ=grepl(\"Reserve|Reserva|Riserva\", designation, ignore.case = TRUE)) %&gt;%\n  ggplot(aes(x=points, y=price, color=rჂservæ)) + geom_point() +\n  geom_smooth(method=lm)"
  },
  {
    "objectID": "Session01.html#rⴢservæ",
    "href": "Session01.html#rⴢservæ",
    "title": "Machine Learning Overview",
    "section": "RჂservæ",
    "text": "RჂservæ\n\nCross the Alps.\n\n\nwine %&gt;%\n  mutate(rჂservæ = grepl(\"Reserve|Reserva|Riserva\", designation, ignore.case = TRUE)) %&gt;%\n  ggplot(aes(x = points, y = price, color = rჂservæ)) +\n  geom_smooth()"
  },
  {
    "objectID": "Session01.html#guessing",
    "href": "Session01.html#guessing",
    "title": "Machine Learning Overview",
    "section": "Guessing",
    "text": "Guessing\n\nLet’s start by developing the simplest possible machine algorithm: guessing the outcome.\n\n\ny_hat &lt;- sample(c(\"Male\", \"Female\"), length(test_index), replace = TRUE)\n\nRecall:\n\nY hat (written ŷ ) is the predicted value of y (the dependent variable) in a regression equation. It can also be considered to be the average value of the response variable."
  },
  {
    "objectID": "Session01.html#regress",
    "href": "Session01.html#regress",
    "title": "Machine Learning Overview",
    "section": "Regress",
    "text": "Regress\n\nTake a quick regression model over the wine.\n\n\nm1 &lt;- lm(price ~ points, data = wine)\nget_regression_table(m1)\n\n# A tibble: 2 × 7\n  term      estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept  -489.       3.97      -123.       0  -497.    -482.  \n2 points        5.92     0.045      132.       0     5.83     6.01"
  },
  {
    "objectID": "Session01.html#group-exercise-30m",
    "href": "Session01.html#group-exercise-30m",
    "title": "Machine Learning Overview",
    "section": "Group Exercise (30m)",
    "text": "Group Exercise (30m)\n\nLoad the wine data set\nVisualize the relationship of points and price\nBonus: Color the observations based on whether the wine is from Bordeaux\nBonus+: Include regression lines\nBonus++: Pick a non-Bordeaux category."
  },
  {
    "objectID": "Session01.html#wine",
    "href": "Session01.html#wine",
    "title": "Machine Learning Overview",
    "section": "Wine",
    "text": "Wine\n\nUse the full wine dataset.\n\n\nwine &lt;- wall %&gt;% \n  mutate(bordeaux=(province==\"Bordeaux\")) %&gt;%\n  drop_na(points,price,bordeaux)"
  },
  {
    "objectID": "Session01.html#accuracy",
    "href": "Session01.html#accuracy",
    "title": "Machine Learning Overview",
    "section": "Accuracy",
    "text": "Accuracy\n\nThe overall accuracy is simply defined as the overall proportion that is predicted correctly:\n\n\nmean(y_hat == test_set$sex)\n\n[1] 0.5180952\n\n\n\nWhat would we have expected the accuracy to be?\n\nWhat much would we have expected accuracy to deviate from that expectionation?"
  },
  {
    "objectID": "Session01.html#partition-our-data",
    "href": "Session01.html#partition-our-data",
    "title": "Machine Learning Overview",
    "section": "Partition our Data",
    "text": "Partition our Data\n\ndata(heights) # from library(dslabs)\ny &lt;- heights$sex\nx &lt;- heights$height\nset.seed(505)\ntest_index &lt;- createDataPartition(y, times = 1, p = 0.5, list = FALSE)\ntest_set &lt;- heights[test_index, ]\ntrain_set &lt;- heights[-test_index, ]\nsummary(heights)\n\n     sex          height     \n Female:238   Min.   :50.00  \n Male  :812   1st Qu.:66.00  \n              Median :68.50  \n              Mean   :68.32  \n              3rd Qu.:71.00  \n              Max.   :82.68  \n\n\nNote: this vignette is adapted from this book"
  },
  {
    "objectID": "Session01.html#lets-take-a-gander",
    "href": "Session01.html#lets-take-a-gander",
    "title": "Machine Learning Overview",
    "section": "Let’s take a gander",
    "text": "Let’s take a gander\n\nEasier for me to see it.\n\n\nplot(cutoff, accuracy)"
  },
  {
    "objectID": "Session01.html#debrief",
    "href": "Session01.html#debrief",
    "title": "Machine Learning Overview",
    "section": "Debrief",
    "text": "Debrief\n\n\n\nheights %&gt;%\n  ggplot() +\n  geom_boxplot(aes(height, sex))\n\n\n\n\n\n\n\n\n\n\nslices &lt;- heights %&gt;%\n  group_by(sex) %&gt;%\n  tally()\npie(slices$n, labels = slices$sex)"
  },
  {
    "objectID": "Session01.html#jameson-on-ethics",
    "href": "Session01.html#jameson-on-ethics",
    "title": "Machine Learning Overview",
    "section": "Jameson on Ethics",
    "text": "Jameson on Ethics\n\nReasonable people will disagree over subtle matters of right and wrong… thus, the important part of data ethics is committing to consider the ethical consequences of your choices.\nThe difference between “regular” ethics and data ethics is that algorithms scale really easily. Thus, seemingly small decisions can have wide-ranging impact."
  },
  {
    "objectID": "Session01.html#calvin-on-ethics",
    "href": "Session01.html#calvin-on-ethics",
    "title": "Machine Learning Overview",
    "section": "Calvin on Ethics",
    "text": "Calvin on Ethics\n\nNo ethical [computation] under capitalism\n\n\nUsage of data | computing is ethicial iff it challenges rather than strengthens existing power relations."
  },
  {
    "objectID": "Session01.html#solutions",
    "href": "Session01.html#solutions",
    "title": "Machine Learning Overview",
    "section": "Solutions",
    "text": "Solutions\n- Definitions\n  - Precision: TP / (TP + FP)\n  - Recall:    TP / (TP + FN)\n- Computation\n  - Precision: 12 / (12 + 06)  ~= 67%\n  - Recall:    12 / (12 + 20)  ~= 38%\n  - Accuracy: (12 + 962)/1000  ~= 97%"
  },
  {
    "objectID": "Session01.html#optimal-cutoff",
    "href": "Session01.html#optimal-cutoff",
    "title": "Machine Learning Overview",
    "section": "Optimal Cutoff",
    "text": "Optimal Cutoff\n\nbest_cutoff &lt;- cutoff[which.max(accuracy)]\nbest_cutoff\n\n[1] 64\n\n\n\nShould we be cutting at an integer?"
  },
  {
    "objectID": "Session01.html#apply-evaluate",
    "href": "Session01.html#apply-evaluate",
    "title": "Machine Learning Overview",
    "section": "Apply & Evaluate",
    "text": "Apply & Evaluate\n\ny_hat &lt;- ifelse(test_set$height &gt; best_cutoff, \"Male\", \"Female\")\nmean(y_hat == test_set$sex)\n\n[1] 0.8190476"
  },
  {
    "objectID": "Session01.html#features",
    "href": "Session01.html#features",
    "title": "Machine Learning Overview",
    "section": "Features",
    "text": "Features\n\nDefinition: Individual measurable properties or attributes of data.\n\nExample: Age, income, and education level in a dataset predicting loan approval."
  },
  {
    "objectID": "Session01.html#variables",
    "href": "Session01.html#variables",
    "title": "Machine Learning Overview",
    "section": "Variables",
    "text": "Variables\n\nDefinition: Data points that can change and impact predictions.\n\nExample: Independent variables like weather, and dependent variables like crop yield."
  },
  {
    "objectID": "Session01.html#outcome-variable",
    "href": "Session01.html#outcome-variable",
    "title": "Machine Learning Overview",
    "section": "Outcome Variable",
    "text": "Outcome Variable\n\nDefinition: The target or dependent variable the model predicts.\n\nExample: Predicting “passed” or “failed” for a student’s exam result."
  },
  {
    "objectID": "Session01.html#regression",
    "href": "Session01.html#regression",
    "title": "Machine Learning Overview",
    "section": "Regression",
    "text": "Regression\n\nDefinition: Statistical method to model the relationship between variables.\n\nExample: Linear regression predicts house prices based on size and location."
  },
  {
    "objectID": "Session01.html#rmse-root-mean-square-error",
    "href": "Session01.html#rmse-root-mean-square-error",
    "title": "Machine Learning Overview",
    "section": "RMSE (Root Mean Square Error)",
    "text": "RMSE (Root Mean Square Error)\n\nDefinition: A metric to measure prediction accuracy by averaging squared errors.\n\nExample: Lower RMSE in predicting drug response indicates a better model fit."
  },
  {
    "objectID": "Session01.html#classification",
    "href": "Session01.html#classification",
    "title": "Machine Learning Overview",
    "section": "Classification",
    "text": "Classification\n\nDefinition: Task of predicting discrete categories or labels.\n\nExample: Classifying emails as “spam” or “not spam.”"
  },
  {
    "objectID": "Session01.html#confusion-matrix-1",
    "href": "Session01.html#confusion-matrix-1",
    "title": "Machine Learning Overview",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\nDefinition: A table showing model performance in classification tasks.\n\nExample: Matrix rows show true values; columns show predicted outcomes."
  },
  {
    "objectID": "Session01.html#split-samples",
    "href": "Session01.html#split-samples",
    "title": "Machine Learning Overview",
    "section": "Split Samples",
    "text": "Split Samples\n\nDefinition: Dividing data into training and testing subsets for validation.\n\nExample: 80% training, 20% testing ensures unbiased model evaluation.\n\n\ntest_set &lt;- heights[test_index, ]\ntrain_set &lt;- heights[-test_index, ]"
  },
  {
    "objectID": "Session01.html#difference-between-features-and-variables",
    "href": "Session01.html#difference-between-features-and-variables",
    "title": "Machine Learning Overview",
    "section": "Difference Between Features and Variables",
    "text": "Difference Between Features and Variables\n\nFeatures: Inputs to the model, often selected or engineered from raw data.\n\nExample: “Average monthly income” derived from raw transaction data.\n\n\nVariables: Broader term encompassing both inputs (independent) and outputs (dependent).\n\nExample: “House price” (dependent variable) depends on features like size and location."
  },
  {
    "objectID": "Session01.html#features-vs.-variables",
    "href": "Session01.html#features-vs.-variables",
    "title": "Machine Learning Overview",
    "section": "Features vs. Variables",
    "text": "Features vs. Variables\n\nFeatures: Inputs to the model, often selected or engineered from raw data.\n\nExample: “Average monthly income” derived from raw transaction data.\n\n\nVariables: Broader term encompassing both inputs (independent) and outputs (dependent).\n\nExample: “House price” (dependent variable) depends on features like size and location."
  },
  {
    "objectID": "Session02.html#agenda",
    "href": "Session02.html#agenda",
    "title": "Feature Engineering & Variable Selection",
    "section": "Agenda",
    "text": "Agenda\n\nReview of Homework 1\nFeature Engineering I\nDinner Break\nThe Caret framework\nVocabulary"
  },
  {
    "objectID": "Session02.html#packages",
    "href": "Session02.html#packages",
    "title": "Feature Engineering & Variable Selection",
    "section": "Packages",
    "text": "Packages\n\nToday I use the following libraries:\n\nlocal({r &lt;- getOption(\"repos\")\n       r[\"CRAN\"] &lt;- \"https://cran.r-project.org\" \n       options(repos=r)\n})\n# Old\ninstall.packages(\"tidyverse\")\ninstall.packages(\"caret\")\n# New?\ninstall.packages(\"fastDummies\")\n# Just for the slides\ninstall.packages(\"thematic\")\n\nYou will have some but perhaps not others."
  },
  {
    "objectID": "Session02.html#libraries",
    "href": "Session02.html#libraries",
    "title": "Feature Engineering & Variable Selection",
    "section": "Libraries",
    "text": "Libraries\n\nI’ll just include them upfront.\n\n\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(fastDummies)\n# Just for the slides\nlibrary(thematic)\ntheme_set(theme_dark())\nthematic_rmd(bg = \"#111\", fg = \"#eee\", accent = \"#eee\")"
  },
  {
    "objectID": "Session02.html#setup",
    "href": "Session02.html#setup",
    "title": "Feature Engineering & Variable Selection",
    "section": "Setup",
    "text": "Setup\n\nWe will work with a wine dataset that is enormous.\n\nJust to render a bit quickly, take a sample.\nYou are welcome to work with the full dataset!\n\n\n\nwine &lt;- readRDS(gzcon(url(\"https://cd-public.github.io/courses/rmls25/dat/wine.rds\")))\n# performance concession\n# wall &lt;- wine\n# wine = wall[sample(nrow(wall), 100), ]\nsummary(wine)\n\n       id           country          description        designation       \n Min.   :     1   Length:89556       Length:89556       Length:89556      \n 1st Qu.: 32742   Class :character   Class :character   Class :character  \n Median : 65613   Mode  :character   Mode  :character   Mode  :character  \n Mean   : 65192                                                           \n 3rd Qu.: 97738                                                           \n Max.   :129970                                                           \n     points           price           province           region_1        \n Min.   : 80.00   Min.   :   4.00   Length:89556       Length:89556      \n 1st Qu.: 87.00   1st Qu.:  17.00   Class :character   Class :character  \n Median : 89.00   Median :  25.00   Mode  :character   Mode  :character  \n Mean   : 88.65   Mean   :  35.56                                        \n 3rd Qu.: 91.00   3rd Qu.:  42.00                                        \n Max.   :100.00   Max.   :3300.00                                        \n   region_2         taster_name        taster_twitter_handle    title          \n Length:89556       Length:89556       Length:89556          Length:89556      \n Class :character   Class :character   Class :character      Class :character  \n Mode  :character   Mode  :character   Mode  :character      Mode  :character  \n                                                                               \n                                                                               \n                                                                               \n   variety             winery               year     \n Length:89556       Length:89556       Min.   :1995  \n Class :character   Class :character   1st Qu.:2010  \n Mode  :character   Mode  :character   Median :2012  \n                                       Mean   :2011  \n                                       3rd Qu.:2014  \n                                       Max.   :2015"
  },
  {
    "objectID": "Session02.html#exercise-30m",
    "href": "Session02.html#exercise-30m",
    "title": "Feature Engineering I & Variable Selection",
    "section": "Exercise (30m)",
    "text": "Exercise (30m)\n\nGather in groups\nIdentify 3 “interesting” features of the wine dataset\nBonus Identify the wine variety (or varieties) that Roger Voss seems to dislike compared to the other critics"
  },
  {
    "objectID": "Session02.html#categorical-vs.-continuous-variables",
    "href": "Session02.html#categorical-vs.-continuous-variables",
    "title": "Feature Engineering I & Variable Selection",
    "section": "Categorical vs. Continuous Variables",
    "text": "Categorical vs. Continuous Variables\n\nWhat is a categorical variable?\nWhat is a continuous variable?\nWhy do we need to “look” at the data before modeling it?"
  },
  {
    "objectID": "Session02.html#categorical-example-1",
    "href": "Session02.html#categorical-example-1",
    "title": "Feature Engineering & Variable Selection",
    "section": "Categorical Example 1",
    "text": "Categorical Example 1\n\nwine %&gt;%\n  mutate(roger = taster_name == \"Roger Voss\") %&gt;%\n  mutate(pinot_gris = variety == \"Pinot Gris\") %&gt;%\n  drop_na(roger) %&gt;%\n  group_by(roger, pinot_gris) %&gt;%\n  summarize(points = mean(points)) %&gt;%\n  ggplot() +\n  aes(pinot_gris, points, color = roger) +\n  geom_line(aes(group = roger), size = 2)"
  },
  {
    "objectID": "Session02.html#categorical-example-2",
    "href": "Session02.html#categorical-example-2",
    "title": "Feature Engineering & Variable Selection",
    "section": "Categorical Example 2",
    "text": "Categorical Example 2\n\nwine %&gt;%\n  filter(province == \"Oregon\") %&gt;%\n  group_by(year) %&gt;%\n  summarize(price = mean(price)) %&gt;%\n  ggplot(aes(year, price)) +\n  geom_smooth() +\n  labs(title = \"Oregon wine over the years\")"
  },
  {
    "objectID": "Session02.html#categorical-example-2-1",
    "href": "Session02.html#categorical-example-2-1",
    "title": "Feature Engineering I & Variable Selection",
    "section": "Categorical Example 2",
    "text": "Categorical Example 2\n\nwine %&gt;% \n  filter(province==\"Oregon\") %&gt;% \n  group_by(year) %&gt;% \n  summarise(points=mean(points)) %&gt;% \n  ggplot(aes(year,points))+\n  geom_line()+\n  labs(title = \"Oregon wine over the years\")"
  },
  {
    "objectID": "Session02.html#exercise-15-min",
    "href": "Session02.html#exercise-15-min",
    "title": "Feature Engineering & Variable Selection",
    "section": "Exercise (15 min)",
    "text": "Exercise (15 min)\n\nGroup by winery and year, Find:\n\nThe average score, and\nNumber of reviews.\n\nFind year-on-year change in score by winery.\nHow might you use this in prediction?\n\nWhat kind of problem might it help with?"
  },
  {
    "objectID": "Session02.html#year-on-year-change-example",
    "href": "Session02.html#year-on-year-change-example",
    "title": "Feature Engineering & Variable Selection",
    "section": "Year-on-Year Change Example",
    "text": "Year-on-Year Change Example\n\nwine %&gt;%\n  group_by(winery, year) %&gt;%\n  summarize(avg_score = mean(points), num_reviews = n_distinct(id)) %&gt;%\n  select(year, winery, num_reviews, avg_score) %&gt;%\n  arrange(winery, year) %&gt;%\n  mutate(score_change = avg_score - lag(avg_score)) %&gt;%\n  drop_na(score_change) %&gt;%\n  summarize(mean(score_change))\n\n# A tibble: 8,409 × 2\n   winery           `mean(score_change)`\n   &lt;chr&gt;                           &lt;dbl&gt;\n 1 100 Percent Wine               -1.5  \n 2 12 Linajes                     -0.333\n 3 12C Wines                       1.25 \n 4 14 Hands                       -0.111\n 5 2 Lads                         -2.25 \n 6 2 Up                            0    \n 7 21 Grams                        1    \n 8 29 & Oak Wines                 -2    \n 9 2Hawk                           0.75 \n10 2Plank                         -1.25 \n# ℹ 8,399 more rows"
  },
  {
    "objectID": "Session02.html#encoding-categorical-features-few-dummies",
    "href": "Session02.html#encoding-categorical-features-few-dummies",
    "title": "Feature Engineering & Variable Selection",
    "section": "Encoding categorical features: few dummies",
    "text": "Encoding categorical features: few dummies\n\nwine %&gt;%\n  select(taster_name) %&gt;%\n  dummy_cols() %&gt;% # library(fastDummies)\n  select(1:4) %&gt;%\n  head()\n\n# A tibble: 6 × 4\n  taster_name        `taster_name_Alexander Peartree` taster_name_Anna Lee C. …¹\n  &lt;chr&gt;                                         &lt;int&gt;                      &lt;int&gt;\n1 Roger Voss                                        0                          0\n2 Paul Gregutt                                      0                          0\n3 Alexander Peartree                                1                          0\n4 Paul Gregutt                                      0                          0\n5 Michael Schachner                                 0                          0\n6 Kerin O’Keefe                                     0                          0\n# ℹ abbreviated name: ¹​`taster_name_Anna Lee C. Iijima`\n# ℹ 1 more variable: `taster_name_Anne Krebiehl MW` &lt;int&gt;"
  },
  {
    "objectID": "Session02.html#encoding-categorical-features-many-dummies",
    "href": "Session02.html#encoding-categorical-features-many-dummies",
    "title": "Feature Engineering & Variable Selection",
    "section": "Encoding categorical features: many dummies",
    "text": "Encoding categorical features: many dummies\n\nwine %&gt;%\n  select(variety) %&gt;%\n  mutate(variety = fct_lump(variety, 4)) %&gt;%\n  dummy_cols() %&gt;%\n  head()\n\n# A tibble: 6 × 6\n  variety    variety_Cabernet Sauvigno…¹ variety_Chardonnay `variety_Pinot Noir`\n  &lt;fct&gt;                            &lt;int&gt;              &lt;int&gt;                &lt;int&gt;\n1 Other                                0                  0                    0\n2 Other                                0                  0                    0\n3 Other                                0                  0                    0\n4 Pinot Noir                           0                  0                    1\n5 Other                                0                  0                    0\n6 Other                                0                  0                    0\n# ℹ abbreviated name: ¹​`variety_Cabernet Sauvignon`\n# ℹ 2 more variables: `variety_Red Blend` &lt;int&gt;, variety_Other &lt;int&gt;"
  },
  {
    "objectID": "Session02.html#other-types-of-engineered-categorical-features",
    "href": "Session02.html#other-types-of-engineered-categorical-features",
    "title": "Feature Engineering & Variable Selection",
    "section": "Other types of engineered categorical features…",
    "text": "Other types of engineered categorical features…\n\nWords or phrases in text\nA given time period\nAn arbitrary numerical cut-off\nDemographic variables"
  },
  {
    "objectID": "Session02.html#what-about-numerical-features",
    "href": "Session02.html#what-about-numerical-features",
    "title": "Feature Engineering & Variable Selection",
    "section": "What about numerical features?",
    "text": "What about numerical features?\n\nwine %&gt;%\n  ggplot(aes(price)) +\n  geom_histogram()"
  },
  {
    "objectID": "Session02.html#take-the-natural-log",
    "href": "Session02.html#take-the-natural-log",
    "title": "Feature Engineering & Variable Selection",
    "section": "Take the natural log",
    "text": "Take the natural log\n\nwine %&gt;%\n  ggplot(aes(log(price))) +\n  geom_histogram()"
  },
  {
    "objectID": "Session02.html#standardizing",
    "href": "Session02.html#standardizing",
    "title": "Feature Engineering & Variable Selection",
    "section": "Standardizing",
    "text": "Standardizing\n\nCreate a common scale across variables.\n\nMean-centering \\(x-\\bar{x}\\)\nScaling: \\(x/std(x)\\)\n\nHelps reduce bias when interactions are included.\n\n(i.e. eliminates variance inflation)."
  },
  {
    "objectID": "Session02.html#other-transformations.",
    "href": "Session02.html#other-transformations.",
    "title": "Feature Engineering & Variable Selection",
    "section": "Other transformations.",
    "text": "Other transformations.\n\nI use logs &gt; 95% of the time, standarizing ~40%.\nThere are many other transformations:\n\nYoY, QoQ, etc. (absolute and percent)\nlog\npolynomial transforms\nlags!"
  },
  {
    "objectID": "Session02.html#interaction-effects",
    "href": "Session02.html#interaction-effects",
    "title": "Feature Engineering & Variable Selection",
    "section": "Interaction effects",
    "text": "Interaction effects\nThis chapter has a good overview of interactions.\n\nStart with domain knowledge.\nUse visualizations.\n3-way interactions exist, but are rare.\n\nIf you suspect a 3-way, also suspect your suspicions.\nComplexity increases exponentially in “ways”.\nThese are notoriously hard to explain."
  },
  {
    "objectID": "Session02.html#dinner-and-virtual-high-fives",
    "href": "Session02.html#dinner-and-virtual-high-fives",
    "title": "Feature Engineering & Variable Selection",
    "section": "Dinner (and virtual high fives)",
    "text": "Dinner (and virtual high fives)"
  },
  {
    "objectID": "Session02.html#philosophy",
    "href": "Session02.html#philosophy",
    "title": "Feature Engineering & Variable Selection",
    "section": "Philosophy",
    "text": "Philosophy\n\n\n\n\n\n\n\nG\n\n\n\nA\n\nAll Data\n\n\n\nB\n\nTraining\n\n\n\nA-&gt;B\n\n\n\n\n\nC\n\nTesting\n\n\n\nA-&gt;C\n\n\n\n\n\nD\n\nResample 1\n\n\n\nB-&gt;D\n\n\n\n\n\nE\n\nResample 2\n\n\n\nB-&gt;E\n\n\n\n\n\nF\n\nResample B\n\n\n\nB-&gt;F\n\n\n\n\n\nG\n\nAnalysis\n\n\n\nD-&gt;G\n\n\n\n\n\nH\n\nAssessment\n\n\n\nD-&gt;H\n\n\n\n\n\nI\n\nAnalysis\n\n\n\nE-&gt;I\n\n\n\n\n\nJ\n\nAssessment\n\n\n\nE-&gt;J\n\n\n\n\n\nK\n\nAnalysis\n\n\n\nF-&gt;K\n\n\n\n\n\nL\n\nAssessment\n\n\n\nF-&gt;L"
  },
  {
    "objectID": "Session02.html#types-of-resampling",
    "href": "Session02.html#types-of-resampling",
    "title": "Feature Engineering & Variable Selection",
    "section": "Types of resampling",
    "text": "Types of resampling\n\nV-fold Cross-Validation\n\nDivides data into \\(k\\) folds, trains on \\(k−1\\) folds, validates on the remaining fold, for all folds.\n\nMonte Carlo Cross-Validation\n\nRandomly splits data into training and validation sets multiple times, averaging results for evaluation.\n\nThe Bootstrap\n\nUses resampling with replacement to estimate model accuracy and variability."
  },
  {
    "objectID": "Session02.html#setup-the-dataframe",
    "href": "Session02.html#setup-the-dataframe",
    "title": "Feature Engineering & Variable Selection",
    "section": "Setup the Dataframe",
    "text": "Setup the Dataframe\n\nFollow this link for the full documentation on caret.\n\n\nwino &lt;- wine %&gt;% # 3 engineered features\n  mutate(fr = (country == \"France\")) %&gt;%\n  mutate(cab = str_detect(variety, \"Cabernet\")) %&gt;%\n  mutate(lprice = log(price)) %&gt;%\n  drop_na(fr, cab) %&gt;%\n  select(lprice, points, fr, cab)\n\n\nOff hand, I would’ve standarized points as well, but\nWe’re following Jameson’s code…\n\n…who understands the data better."
  },
  {
    "objectID": "Session02.html#split-samples",
    "href": "Session02.html#split-samples",
    "title": "Feature Engineering & Variable Selection",
    "section": "Split Samples",
    "text": "Split Samples\n\nwine_index &lt;- createDataPartition(wino$lprice, p = 0.8, list = FALSE)\nwino_tr &lt;- wino[wine_index, ]\nwino_te &lt;- wino[-wine_index, ]\nsummary(wino_tr)\n\n     lprice          points           fr             cab         \n Min.   :1.386   Min.   : 80.00   Mode :logical   Mode :logical  \n 1st Qu.:2.833   1st Qu.: 87.00   FALSE:59513     FALSE:65205    \n Median :3.219   Median : 89.00   TRUE :12090     TRUE :6398     \n Mean   :3.314   Mean   : 88.64                                  \n 3rd Qu.:3.738   3rd Qu.: 91.00                                  \n Max.   :8.102   Max.   :100.00"
  },
  {
    "objectID": "Session02.html#train-the-model",
    "href": "Session02.html#train-the-model",
    "title": "Feature Engineering & Variable Selection",
    "section": "Train the model",
    "text": "Train the model\n\nConfigure train to cross validate\n\n\nm1 &lt;- train(lprice ~ .,\n  data = wino_tr,\n  method = \"lm\",\n  trControl = trainControl(method = \"repeatedcv\", number = 5, repeats = 3)\n)\nm1\n\nLinear Regression \n\n71603 samples\n    3 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold, repeated 3 times) \nSummary of sample sizes: 57282, 57282, 57282, 57283, 57283, 57283, ... \nResampling results:\n\n  RMSE       Rsquared   MAE      \n  0.5114639  0.3935319  0.4030508\n\nTuning parameter 'intercept' was held constant at a value of TRUE"
  },
  {
    "objectID": "Session02.html#rmse-outputs",
    "href": "Session02.html#rmse-outputs",
    "title": "Feature Engineering & Variable Selection",
    "section": "RMSE outputs",
    "text": "RMSE outputs\n\nprint(m1$resample)\n\n        RMSE  Rsquared       MAE   Resample\n1  0.5146601 0.3853599 0.4059389 Fold1.Rep1\n2  0.5094933 0.3935665 0.4014247 Fold2.Rep1\n3  0.5077189 0.3996866 0.4008294 Fold3.Rep1\n4  0.5061893 0.3958357 0.3993910 Fold4.Rep1\n5  0.5192489 0.3933709 0.4077061 Fold5.Rep1\n6  0.5078214 0.4044637 0.4009670 Fold1.Rep2\n7  0.5109179 0.3910638 0.4034304 Fold2.Rep2\n8  0.5135655 0.3964864 0.4056476 Fold3.Rep2\n9  0.5149233 0.3882239 0.4035995 Fold4.Rep2\n10 0.5100689 0.3873541 0.4015906 Fold5.Rep2\n11 0.5115516 0.3908095 0.4042859 Fold1.Rep3\n12 0.5078380 0.3900568 0.4009942 Fold2.Rep3\n13 0.5142245 0.4009447 0.4037128 Fold3.Rep3\n14 0.5126319 0.3920606 0.4021288 Fold4.Rep3\n15 0.5111053 0.3936950 0.4041146 Fold5.Rep3"
  },
  {
    "objectID": "Session02.html#train-vs.-test",
    "href": "Session02.html#train-vs.-test",
    "title": "Feature Engineering & Variable Selection",
    "section": "Train vs. test",
    "text": "Train vs. test\n\n\n\nm1\n\nLinear Regression \n\n71603 samples\n    3 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold, repeated 3 times) \nSummary of sample sizes: 57282, 57282, 57282, 57283, 57283, 57283, ... \nResampling results:\n\n  RMSE       Rsquared   MAE      \n  0.5114639  0.3935319  0.4030508\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\n\npostResample(pred = predict(m1, wino_te), obs = wino_te$lprice)\n\n     RMSE  Rsquared       MAE \n0.5076998 0.4035000 0.3997605"
  },
  {
    "objectID": "Session02.html#train-vs.-test-1",
    "href": "Session02.html#train-vs.-test-1",
    "title": "Feature Engineering & Variable Selection",
    "section": "Train vs. test",
    "text": "Train vs. test\n\n\n\nm2 &lt;- do_training(\n  wino_tr, lprice ~ .\n)\nm2\n\nLinear Regression \n\n71603 samples\n    9 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold, repeated 3 times) \nSummary of sample sizes: 57282, 57282, 57284, 57283, 57281, 57281, ... \nResampling results:\n\n  RMSE       Rsquared  MAE      \n  0.4889463  0.445219  0.3801405\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\n\npostResample(\n  pred = predict(m2, wino_te),\n  obs = wino_te$lprice\n)\n\n     RMSE  Rsquared       MAE \n0.4880452 0.4509507 0.3788557"
  },
  {
    "objectID": "Session02.html#exercise-30-40-minutes",
    "href": "Session02.html#exercise-30-40-minutes",
    "title": "Feature Engineering I & Variable Selection",
    "section": "Exercise (30-40 minutes)",
    "text": "Exercise (30-40 minutes)\n\nGather in groups\nCreate 5-10 new features (in addition to points)\nCreate training and test data\nUse your new predictors to train a linear regression model for log(price)\nReport RMSE on test set and the cross-validated score.\nKeep tweaking/engineering new features to lower the RMSE.\nShould you focus on the CV score or the test score when tweaking to optimize score?\nDoes it make a difference if you use standardized points instead of just points?"
  },
  {
    "objectID": "Session02.html#stepwise-selection-is-bad",
    "href": "Session02.html#stepwise-selection-is-bad",
    "title": "Feature Engineering & Variable Selection",
    "section": "Stepwise selection is bad",
    "text": "Stepwise selection is bad\nHarrell (2015) provides a comprehensive indictment of the method that can be encapsulated by the statement:\n\n“… if this procedure had just been proposed as a statistical method, it would most likely be rejected because it violates every principle of statistical estimation and hypothesis testing.”\n\n  Reference: Harrell, F. 2015. Regression Modeling Strategies. Springer."
  },
  {
    "objectID": "Session02.html#engineer-9-features",
    "href": "Session02.html#engineer-9-features",
    "title": "Feature Engineering & Variable Selection",
    "section": "Engineer 9 features",
    "text": "Engineer 9 features\n\nwino &lt;- wine %&gt;%\n  mutate(country = fct_lump(country, 4)) %&gt;%    # 1:4,\n  mutate(variety = fct_lump(variety, 4)) %&gt;%    # 5:8,\n  mutate(lprice = log(price)) %&gt;%               #   9\n  select(lprice, points, country, variety) %&gt;%\n  drop_na(.)\nhead(wino)\n\n# A tibble: 6 × 4\n  lprice points country variety   \n   &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;   &lt;fct&gt;     \n1   2.71     87 Other   Other     \n2   2.64     87 US      Other     \n3   2.56     87 US      Other     \n4   4.17     87 US      Pinot Noir\n5   2.71     87 Spain   Other     \n6   2.77     87 Italy   Other"
  },
  {
    "objectID": "Session02.html#basic-model",
    "href": "Session02.html#basic-model",
    "title": "Feature Engineering & Variable Selection",
    "section": "Basic Model",
    "text": "Basic Model\n\nPartition\n\n\nwine_index &lt;- createDataPartition(wino$lprice, p = 0.8, list = FALSE)\nwino_tr &lt;- wino[wine_index, ]\nwino_te &lt;- wino[-wine_index, ]\n\n\nWe would model the same way, so let’s take aside."
  },
  {
    "objectID": "Session02.html#results-train",
    "href": "Session02.html#results-train",
    "title": "Feature Engineering I & Variable Selection",
    "section": "Results (train)",
    "text": "Results (train)\n\nm2\n\nLinear Regression \n\n71603 samples\n    9 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold, repeated 3 times) \nSummary of sample sizes: 57283, 57281, 57283, 57281, 57284, 57281, ... \nResampling results:\n\n  RMSE       Rsquared  MAE      \n  0.4884919  0.444705  0.3800067\n\nTuning parameter 'intercept' was held constant at a value of TRUE"
  },
  {
    "objectID": "Session02.html#results-test",
    "href": "Session02.html#results-test",
    "title": "Feature Engineering I & Variable Selection",
    "section": "Results (test)",
    "text": "Results (test)\n\nwine_pred &lt;- predict(m2, wino_te)\npostResample(pred = wine_pred, obs = wino_te$lprice)\n\n     RMSE  Rsquared       MAE \n0.4898876 0.4529010 0.3796093"
  },
  {
    "objectID": "Session02.html#variable-importance-depends-on-model-used",
    "href": "Session02.html#variable-importance-depends-on-model-used",
    "title": "Feature Engineering I & Variable Selection",
    "section": "Variable Importance (depends on model used)",
    "text": "Variable Importance (depends on model used)\n\n# estimate variable importance\nimportance &lt;- varImp(m2, scale = TRUE)\n# plot importance\nplot(importance)"
  },
  {
    "objectID": "Session02.html#variable-importance-linear-regression",
    "href": "Session02.html#variable-importance-linear-regression",
    "title": "Feature Engineering I & Variable Selection",
    "section": "Variable Importance (linear regression)",
    "text": "Variable Importance (linear regression)\n\nEach coefficient in a linear model has a standard error,\n\nMeasures certainty of coefficient given data.\n\nFor the t-statistic,\n\nConfidence that the coefficient is different from 0\nWe divide the coefficient by the standard error.\n\nIf the standard error is “small” relative to the coefficient\n\nThen “big” t-statistic,\nThen high feature importance!\n\nWhat about coefficient as variable importance?"
  },
  {
    "objectID": "Session02.html#recursive-feature-elimination",
    "href": "Session02.html#recursive-feature-elimination",
    "title": "Feature Engineering & Variable Selection",
    "section": "Recursive Feature Elimination",
    "text": "Recursive Feature Elimination\n\nTune/train the model on the training set using all predictors.\nCalculate model performance.\nCalculate variable importance or rankings.\nfor each subset size \\(S_i\\), i = 1…S do\n\nKeep the \\(S_i\\) most important variables.\n[Optional] Pre-process the data.\nTune/train the model on the training set using \\(S_i\\) predictors.\nCalculate model performance.\n[Optional] Recalculate the rankings for each predictor.\n\nend\nCalculate the performance profile over the \\(S_i\\).\nDetermine the appropriate number of predictors.\nUse the model corresponding to the optimal \\(S_i\\)."
  },
  {
    "objectID": "Session02.html#using-recursive-feature-elimination-in-caret",
    "href": "Session02.html#using-recursive-feature-elimination-in-caret",
    "title": "Feature Engineering I & Variable Selection",
    "section": "Using recursive feature elimination in caret",
    "text": "Using recursive feature elimination in caret\n\nx &lt;- select(wino_tr, -lprice)\ny &lt;- wino_tr$lprice\n\ncontrol &lt;- rfeControl(functions = rfFuncs, method = \"cv\", number = 2)\n# run the RFE algorithm\nresults &lt;- rfe(x, y, sizes = c(1:3), rfeControl = control)\n# summarize the results\nprint(results)\n# list the chosen features\npredictors(results)\n# plot the results\nplot(results, type = c(\"g\", \"o\"))"
  },
  {
    "objectID": "Session02.html#feature-selection-in-practice",
    "href": "Session02.html#feature-selection-in-practice",
    "title": "Feature Engineering I & Variable Selection",
    "section": "Feature selection in practice",
    "text": "Feature selection in practice\n\nRaw data —feature engineering—&gt;\nLots and lots of features! —feature selection—&gt;\nShortlist of features (based on metric) —expert input—&gt;\nShortlist of features II —DS judgement—&gt;\nFinalist models —stakeholders—&gt;\nProduction model"
  },
  {
    "objectID": "Session02.html#key-terms",
    "href": "Session02.html#key-terms",
    "title": "Feature Engineering & Variable Selection",
    "section": "Key Terms",
    "text": "Key Terms\n\n\n\nFeature Engineering\nCategorical Feature\nContinuous Feature\nDummy\nInteraction\n\n\n\nCaret\nModel\nResampling\nTrain vs. Test Data\nVariable Importance"
  },
  {
    "objectID": "Session02.html#assumptions-of-linear-regression",
    "href": "Session02.html#assumptions-of-linear-regression",
    "title": "Feature Engineering & Variable Selection",
    "section": "5 Assumptions of Linear Regression",
    "text": "5 Assumptions of Linear Regression\n\nLinear regressions have a well-developed statistical theory.\nThis brings perks like confidence intervals on predictions.\nIt also has “costs” in that assumptions need to be satisfied."
  },
  {
    "objectID": "Session02.html#assumptions-of-linear-regression-1",
    "href": "Session02.html#assumptions-of-linear-regression-1",
    "title": "Feature Engineering I & Variable Selection",
    "section": "5 Assumptions of Linear Regression",
    "text": "5 Assumptions of Linear Regression\n\nLinearity - the dependent variable is a linear combination of the features.\n\n\nThis is less of big deal than it might seem! If y is actually quadratic in x, then y is linear in x^2! That’s feature engineering."
  },
  {
    "objectID": "Session02.html#assumptions-of-linear-regression-2",
    "href": "Session02.html#assumptions-of-linear-regression-2",
    "title": "Feature Engineering I & Variable Selection",
    "section": "5 Assumptions of Linear Regression",
    "text": "5 Assumptions of Linear Regression\n\nConstant variance / homoscedasticity - the variance of the errors do not depend on the values of the features.\n\n\nIt’s important (for linear regressions) that you don’t make bigger prediction errors for some values of x than for others."
  },
  {
    "objectID": "Session02.html#assumptions-of-linear-regression-3",
    "href": "Session02.html#assumptions-of-linear-regression-3",
    "title": "Feature Engineering I & Variable Selection",
    "section": "5 Assumptions of Linear Regression",
    "text": "5 Assumptions of Linear Regression\n\nNormality - the errors should be independent and normally distributed.\n\n\nIf you imagine a scatter plot of target variable value and residual (model error), it should look like white noise."
  },
  {
    "objectID": "Session02.html#assumptions-of-linear-regression-4",
    "href": "Session02.html#assumptions-of-linear-regression-4",
    "title": "Feature Engineering I & Variable Selection",
    "section": "5 Assumptions of Linear Regression",
    "text": "5 Assumptions of Linear Regression\n\nLack of perfect multicollinearity - none of your predictors should be a perfect linear combination of the others.\n\n\nThis can happen if you over-engineer features but in my experience this is less of a concern. You’ll see an error that your coefficient matrix is singular or something."
  },
  {
    "objectID": "Session02.html#assumptions-of-linear-regression-5",
    "href": "Session02.html#assumptions-of-linear-regression-5",
    "title": "Feature Engineering I & Variable Selection",
    "section": "5 Assumptions of Linear Regression",
    "text": "5 Assumptions of Linear Regression\n\nExogeneity - model errors should be independent of the values of the features.\n\n\nIn particular, errors should have mean zero. It’s always good to look at a histogram of your residuals (see also normality)."
  },
  {
    "objectID": "Session02.html#assumptions-of-linear-regression-testing",
    "href": "Session02.html#assumptions-of-linear-regression-testing",
    "title": "Feature Engineering & Variable Selection",
    "section": "5 Assumptions of Linear Regression: testing",
    "text": "5 Assumptions of Linear Regression: testing\n\nSecond I would always look at fitted value vs. residual to check homoscedasticity.\n\n\n\n\nFor more, see for example https://people.duke.edu/~rnau/testing.htm"
  },
  {
    "objectID": "Session02.html#assumptions-of-linear-regression-testing-1",
    "href": "Session02.html#assumptions-of-linear-regression-testing-1",
    "title": "Feature Engineering I & Variable Selection",
    "section": "5 Assumptions of Linear Regression: testing",
    "text": "5 Assumptions of Linear Regression: testing\n\nSecond I would always look at fitted value vs. residual to check homoscedasticity.\n\n\n\n\nFor more, see for example https://people.duke.edu/~rnau/testing.htm"
  },
  {
    "objectID": "samp.html#agenda",
    "href": "samp.html#agenda",
    "title": "Samp",
    "section": "Agenda",
    "text": "Agenda\n\nCourse Overview\nReview of Regression\nDinner Break\nClassification and Ethics\nBasic Feature Engineering\nVocabulary"
  },
  {
    "objectID": "samp.html#expectations-and-assignments",
    "href": "samp.html#expectations-and-assignments",
    "title": "Samp",
    "section": "Expectations and assignments",
    "text": "Expectations and assignments\n\nHomework assignments\nExams\nModeling Project\nCourse Policies\nMy expectations for you"
  },
  {
    "objectID": "samp.html#about-me",
    "href": "samp.html#about-me",
    "title": "Samp",
    "section": "About me",
    "text": "About me\n\nBA Mathematics, BS Computer Science (UChicago)\nMS, PhD Computer Science (UNC Chapel Hill)\nData mining, formal analysis, complex models\nJoined Willamette 2021"
  },
  {
    "objectID": "samp.html#about-you",
    "href": "samp.html#about-you",
    "title": "Samp",
    "section": "About you?",
    "text": "About you?\n\nBackground\nGoals for this program and/or course"
  },
  {
    "objectID": "samp.html#basic-concepts-in-machine-learning",
    "href": "samp.html#basic-concepts-in-machine-learning",
    "title": "Samp",
    "section": "Basic concepts in Machine Learning",
    "text": "Basic concepts in Machine Learning\n\nWhat is a data scientist?\nWhat is machine learning?\nWhat is the role of judgment in machine learning?\nWhat are the differences between machine learning, statistics and econometrics?\nWhen is “mere” correlation enough? When is it not?"
  },
  {
    "objectID": "samp.html#packages",
    "href": "samp.html#packages",
    "title": "Samp",
    "section": "Packages",
    "text": "Packages\n\nToday I use the following libraries:\n\nlocal({r &lt;- getOption(\"repos\")\n       r[\"CRAN\"] &lt;- \"https://cran.r-project.org\" \n       options(repos=r)\n})\n# New?\ninstall.packages(\"tidyverse\")\ninstall.packages(\"moderndive\")\ninstall.packages(\"caret\")\ninstall.packages(\"dslabs\")\n# Just for the slides\ninstall.packages(\"thematic\")\n\nYou will have some but perhaps not others."
  },
  {
    "objectID": "samp.html#libraries",
    "href": "samp.html#libraries",
    "title": "Samp",
    "section": "Libraries",
    "text": "Libraries\n\nI’ll just include them upfront."
  },
  {
    "objectID": "samp.html#partition-our-data",
    "href": "samp.html#partition-our-data",
    "title": "Samp",
    "section": "Partition our Data",
    "text": "Partition our Data\n\n\n     sex          height     \n Female:238   Min.   :50.00  \n Male  :812   1st Qu.:66.00  \n              Median :68.50  \n              Mean   :68.32  \n              3rd Qu.:71.00  \n              Max.   :82.68"
  },
  {
    "objectID": "Session02.html#categorical-vs.-continuous",
    "href": "Session02.html#categorical-vs.-continuous",
    "title": "Feature Engineering & Variable Selection",
    "section": "Categorical vs. Continuous",
    "text": "Categorical vs. Continuous\n\nWhat is a categorical variable?\nWhat is a continuous variable?\nWhy visualize at the data before modeling it?"
  },
  {
    "objectID": "Session02.html#dummy-variables",
    "href": "Session02.html#dummy-variables",
    "title": "Feature Engineering & Variable Selection",
    "section": "Dummy Variables",
    "text": "Dummy Variables\n\nWhat are Dummy Variables?:\n\nRepresent categories as 0s and 1s for models.\n\n\nWhy Use Dummy Variables?:\n\nHandle categorical data in numerical algorithms.\n\n\nAvoid Dummy Trap:\n\nDrop one column to prevent multicollinearity."
  },
  {
    "objectID": "Session02.html#many-vs-few-dummies",
    "href": "Session02.html#many-vs-few-dummies",
    "title": "Feature Engineering & Variable Selection",
    "section": "Many vs Few Dummies",
    "text": "Many vs Few Dummies\n\nFew Dummies:\n\nSimplifies models, risks losing fine-grained patterns.\n\nMany Dummies:\n\nCaptures detailed trends, increases model complexity.\n\nKey Decision:\n\nBalance interpretability and predictive power."
  },
  {
    "objectID": "Session02.html#fastdummies-package",
    "href": "Session02.html#fastdummies-package",
    "title": "Feature Engineering & Variable Selection",
    "section": "“fastDummies” Package",
    "text": "“fastDummies” Package\n\nPurpose:\n\nQuickly create dummy variables in R datasets.\n\nKey Functions:\n\ndummy_cols() adds dummy columns efficiently.\n\nFeatures:\n\nHandles multiple columns and missing data flexibly."
  },
  {
    "objectID": "Session02.html#normalize",
    "href": "Session02.html#normalize",
    "title": "Feature Engineering I & Variable Selection",
    "section": "Normalize",
    "text": "Normalize\n\nlist(normalized = ~(scale(.) %&gt;% as.vector)): Creates a new column named “standardized” by applying the following transformation:\n\nscale(.): Standardizes the “points” column (i.e., subtracts the mean and divides by the standard deviation).\n%&gt;% as.vector: Converts the scaled values back to a vector.\n\n\n\nwine %&gt;% mutate_at(\"points\", list(standardized = ~ (scale(.) %&gt;% as.vector())))\n\n# A tibble: 89,556 × 16\n      id country description designation points price province region_1 region_2\n   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   \n 1     1 Portug… This is ri… Avidagos        87    15 Douro    &lt;NA&gt;     &lt;NA&gt;    \n 2     2 US      Tart and s… &lt;NA&gt;            87    14 Oregon   Willame… Willame…\n 3     3 US      Pineapple … Reserve La…     87    13 Michigan Lake Mi… &lt;NA&gt;    \n 4     4 US      Much like … Vintner's …     87    65 Oregon   Willame… Willame…\n 5     5 Spain   Blackberry… Ars In Vit…     87    15 Norther… Navarra  &lt;NA&gt;    \n 6     6 Italy   Here's a b… Belsito         87    16 Sicily … Vittoria &lt;NA&gt;    \n 7     7 France  This dry a… &lt;NA&gt;            87    24 Alsace   Alsace   &lt;NA&gt;    \n 8     8 Germany Savory dri… Shine           87    12 Rheinhe… &lt;NA&gt;     &lt;NA&gt;    \n 9     9 France  This has g… Les Natures     87    27 Alsace   Alsace   &lt;NA&gt;    \n10    10 US      Soft, supp… Mountain C…     87    19 Califor… Napa Va… Napa    \n# ℹ 89,546 more rows\n# ℹ 7 more variables: taster_name &lt;chr&gt;, taster_twitter_handle &lt;chr&gt;,\n#   title &lt;chr&gt;, variety &lt;chr&gt;, winery &lt;chr&gt;, year &lt;dbl&gt;, standardized &lt;dbl&gt;"
  },
  {
    "objectID": "Session02.html#group-exercise-30m",
    "href": "Session02.html#group-exercise-30m",
    "title": "Feature Engineering & Variable Selection",
    "section": "Group Exercise (30m)",
    "text": "Group Exercise (30m)\n\nIdentify 3 “interesting” features of the wine dataset\nBonus\n\n∃ Critic “Roger Voss”\n∃ a wine varietal(s) “Voss” seems to dislike\nFind said varietal."
  },
  {
    "objectID": "Session02.html#group-exercise-30-minutes",
    "href": "Session02.html#group-exercise-30-minutes",
    "title": "Feature Engineering & Variable Selection",
    "section": "Group Exercise (30+ minutes)",
    "text": "Group Exercise (30+ minutes)\n\nCreate 5-10 new features (in addition to points)\nCreate training and test data\nFor each, train a linear model for log(price)\nReport RMSE on test set and cross-validated score.\n(Re-)Engineer new(ish) features to lower the RMSE."
  },
  {
    "objectID": "Session02.html#stepwise-selection",
    "href": "Session02.html#stepwise-selection",
    "title": "Feature Engineering & Variable Selection",
    "section": "Stepwise selection",
    "text": "Stepwise selection\n\nWhat is Stepwise Selection?: Iterative method to add or remove predictors in a model based on statistical criteria.\n\nTypes: Forward selection starts with no predictors; backward elimination starts with all predictors; stepwise combines both.\n\nGoal: Identify a model with strong predictive power and minimal overfitting."
  },
  {
    "objectID": "Session02.html#add-dummy-columns",
    "href": "Session02.html#add-dummy-columns",
    "title": "Feature Engineering & Variable Selection",
    "section": "Add Dummy Columns",
    "text": "Add Dummy Columns\n\nCareful - a destructive update to wino!\n\n\nwino &lt;- wino %&gt;%\n  dummy_cols(remove_selected_columns = TRUE) %&gt;%\n  rename_with(.fn = function(s) {\n    s %&gt;%\n      tolower() %&gt;%\n      str_replace(\"-| \", \"_\")\n  }) %&gt;%\n  select(-ends_with(\"other\"))\nhead(wino)\n\n# A tibble: 6 × 10\n  lprice points country_france country_italy country_spain country_us\n   &lt;dbl&gt;  &lt;dbl&gt;          &lt;int&gt;         &lt;int&gt;         &lt;int&gt;      &lt;int&gt;\n1   2.71     87              0             0             0          0\n2   2.64     87              0             0             0          1\n3   2.56     87              0             0             0          1\n4   4.17     87              0             0             0          1\n5   2.71     87              0             0             1          0\n6   2.77     87              0             1             0          0\n# ℹ 4 more variables: variety_cabernet_sauvignon &lt;int&gt;,\n#   variety_chardonnay &lt;int&gt;, variety_pinot_noir &lt;int&gt;, variety_red_blend &lt;int&gt;"
  },
  {
    "objectID": "Session02.html#standardize",
    "href": "Session02.html#standardize",
    "title": "Feature Engineering & Variable Selection",
    "section": "Standardize",
    "text": "Standardize\n\nlist(normalized = ~(scale(.) %&gt;% as.vector)): :\n\nscale(.): Standardizes the “points” column.\n%&gt;% as.vector: Converts back to a vector.\n\n\n\nwine %&gt;% mutate_at(\"points\", list(standardized = ~ (scale(.) %&gt;% as.vector())))\n\n# A tibble: 89,556 × 16\n      id country description designation points price province region_1 region_2\n   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   \n 1     1 Portug… This is ri… Avidagos        87    15 Douro    &lt;NA&gt;     &lt;NA&gt;    \n 2     2 US      Tart and s… &lt;NA&gt;            87    14 Oregon   Willame… Willame…\n 3     3 US      Pineapple … Reserve La…     87    13 Michigan Lake Mi… &lt;NA&gt;    \n 4     4 US      Much like … Vintner's …     87    65 Oregon   Willame… Willame…\n 5     5 Spain   Blackberry… Ars In Vit…     87    15 Norther… Navarra  &lt;NA&gt;    \n 6     6 Italy   Here's a b… Belsito         87    16 Sicily … Vittoria &lt;NA&gt;    \n 7     7 France  This dry a… &lt;NA&gt;            87    24 Alsace   Alsace   &lt;NA&gt;    \n 8     8 Germany Savory dri… Shine           87    12 Rheinhe… &lt;NA&gt;     &lt;NA&gt;    \n 9     9 France  This has g… Les Natures     87    27 Alsace   Alsace   &lt;NA&gt;    \n10    10 US      Soft, supp… Mountain C…     87    19 Califor… Napa Va… Napa    \n# ℹ 89,546 more rows\n# ℹ 7 more variables: taster_name &lt;chr&gt;, taster_twitter_handle &lt;chr&gt;,\n#   title &lt;chr&gt;, variety &lt;chr&gt;, winery &lt;chr&gt;, year &lt;dbl&gt;, standardized &lt;dbl&gt;"
  },
  {
    "objectID": "Session02.html#variable-importance",
    "href": "Session02.html#variable-importance",
    "title": "Feature Engineering & Variable Selection",
    "section": "Variable Importance",
    "text": "Variable Importance\n\nImportance depends on model used…\n\n\nplot(varImp(m2, scale = TRUE))"
  },
  {
    "objectID": "Session02.html#factoring",
    "href": "Session02.html#factoring",
    "title": "Feature Engineering & Variable Selection",
    "section": "Factoring",
    "text": "Factoring\n\nSame modelling command\n\nmx &lt;- train(lprice ~ .,\n  data = wino_tr,\n  method = \"lm\",\n  trControl = trainControl(method = \"repeatedcv\", number = 5, repeats = 3)\n)\n\nI should factor this into a function.\n\n\ndo_training &lt;- function(df, formula) {\n  train(formula,\n    data = df,\n    method = \"lm\",\n    trControl = trainControl(method = \"repeatedcv\", number = 5, repeats = 3)\n  )\n}"
  },
  {
    "objectID": "Session02.html#variable-importance-1",
    "href": "Session02.html#variable-importance-1",
    "title": "Feature Engineering & Variable Selection",
    "section": "Variable Importance",
    "text": "Variable Importance\n\nEach (linear model) coefficient has a standard error,\n\nMeasures certainty of coefficient given data.\n\nFor the t-statistic,\n\nConfidence that the coefficient is different from 0\nWe divide the coefficient by the standard error.\n\nIf “small” error relative to coefficient\n\nThen “big” t-statistic & high feature importance!\n\nWhat about coefficient as variable importance?"
  },
  {
    "objectID": "Session02.html#caret-rfe",
    "href": "Session02.html#caret-rfe",
    "title": "Feature Engineering & Variable Selection",
    "section": "Caret RFE",
    "text": "Caret RFE\n\nx &lt;- select(wino_tr, -lprice)\ny &lt;- wino_tr$lprice\n\ncontrol &lt;- rfeControl(functions = rfFuncs, method = \"cv\", number = 2)\n# run the RFE algorithm\nresults &lt;- rfe(x, y, sizes = c(1:3), rfeControl = control)\n# summarize the results\nprint(results)\n# list the chosen features\npredictors(results)\n# plot the results\nplot(results, type = c(\"g\", \"o\"))"
  },
  {
    "objectID": "Session02.html#practical-workflow",
    "href": "Session02.html#practical-workflow",
    "title": "Feature Engineering & Variable Selection",
    "section": "Practical Workflow",
    "text": "Practical Workflow\n\n\n\n\n\n\n\nfeature_engineering_pipeline\n\n\n\nRaw Data\n\nRaw Data\n\n\n\nLots of Features\n\nLots of Features\n\n\n\nRaw Data-&gt;Lots of Features\n\n\nFeature Engineering\n\n\n\nCandidate Features\n\nCandidate Features\n\n\n\nLots of Features-&gt;Candidate Features\n\n\nFeature Selection\n\n\n\nShortlist Features\n\nShortlist Features\n\n\n\nCandidate Features-&gt;Shortlist Features\n\n\nExpert Input\n\n\n\nFinalist Models\n\nFinalist Models\n\n\n\nShortlist Features-&gt;Finalist Models\n\n\nDS Judgement\n\n\n\nProduction\n\nProduction\n\n\n\nFinalist Models-&gt;Production\n\n\nBusiness Unit"
  },
  {
    "objectID": "Session02.html#the-five",
    "href": "Session02.html#the-five",
    "title": "Feature Engineering & Variable Selection",
    "section": "The Five",
    "text": "The Five\n\nLinearity\nConstant variance\nNormality\nImperfect multicollinearity\nExogeneity"
  },
  {
    "objectID": "Session02.html#linearity",
    "href": "Session02.html#linearity",
    "title": "Feature Engineering & Variable Selection",
    "section": "1. Linearity",
    "text": "1. Linearity\n\nThe dependent variable is a linear combination of the features.\nThis is less of big deal than it might seem!\nIf y is actually quadratic in x, then y is linear in x^2!\n\nThat’s feature engineering."
  },
  {
    "objectID": "Session02.html#constant-variance",
    "href": "Session02.html#constant-variance",
    "title": "Feature Engineering & Variable Selection",
    "section": "2. Constant variance /",
    "text": "2. Constant variance /\n\nOr homoscedasticity\nThe variance of the errors do not depend on the values of the features.\nDon’t make bigger prediction errors for some values of x than for others."
  },
  {
    "objectID": "Session02.html#normality",
    "href": "Session02.html#normality",
    "title": "Feature Engineering & Variable Selection",
    "section": "3. Normality",
    "text": "3. Normality\n\nThe errors should be independent and normally distributed.\nA scatter plot of target variable value and residual (model error) should look like white noise."
  },
  {
    "objectID": "Session02.html#lack-of-perfect-multicollinearity",
    "href": "Session02.html#lack-of-perfect-multicollinearity",
    "title": "Feature Engineering & Variable Selection",
    "section": "4. Lack of perfect multicollinearity",
    "text": "4. Lack of perfect multicollinearity\n\nNone predictors should be a perfect linear combination of others.\nThis can happen if you over-engineer features\n\nThis is uncommon.\nYou’ll see an error that your coefficient matrix is singular or something."
  },
  {
    "objectID": "Session02.html#exogeneity",
    "href": "Session02.html#exogeneity",
    "title": "Feature Engineering & Variable Selection",
    "section": "5. Exogeneity",
    "text": "5. Exogeneity\n\nModel errors should be independent of the values of the features.\nIn particular, errors should have mean zero.\nIt’s always good to look at a histogram of your residuals (see also normality)."
  },
  {
    "objectID": "Session02.html#first-test",
    "href": "Session02.html#first-test",
    "title": "Feature Engineering & Variable Selection",
    "section": "First Test",
    "text": "First Test\n\nDetermine whether the errors are normally distributed, like Shapiro-Wilk (also, plot them)."
  },
  {
    "objectID": "Session02.html#few-dummies",
    "href": "Session02.html#few-dummies",
    "title": "Feature Engineering & Variable Selection",
    "section": "Few Dummies",
    "text": "Few Dummies\n\nwine %&gt;%\n  select(taster_name) %&gt;%\n  dummy_cols() %&gt;% # library(fastDummies)\n  select(1:4) %&gt;%\n  head()\n\n# A tibble: 6 × 4\n  taster_name        `taster_name_Alexander Peartree` taster_name_Anna Lee C. …¹\n  &lt;chr&gt;                                         &lt;int&gt;                      &lt;int&gt;\n1 Roger Voss                                        0                          0\n2 Paul Gregutt                                      0                          0\n3 Alexander Peartree                                1                          0\n4 Paul Gregutt                                      0                          0\n5 Michael Schachner                                 0                          0\n6 Kerin O’Keefe                                     0                          0\n# ℹ abbreviated name: ¹​`taster_name_Anna Lee C. Iijima`\n# ℹ 1 more variable: `taster_name_Anne Krebiehl MW` &lt;int&gt;"
  },
  {
    "objectID": "Session02.html#many-dummies",
    "href": "Session02.html#many-dummies",
    "title": "Feature Engineering & Variable Selection",
    "section": "Many Dummies",
    "text": "Many Dummies\n\nwine %&gt;%\n  select(variety) %&gt;%\n  mutate(variety = fct_lump(variety, 4)) %&gt;%\n  dummy_cols() %&gt;%\n  head()\n\n# A tibble: 6 × 6\n  variety    variety_Cabernet Sauvigno…¹ variety_Chardonnay `variety_Pinot Noir`\n  &lt;fct&gt;                            &lt;int&gt;              &lt;int&gt;                &lt;int&gt;\n1 Other                                0                  0                    0\n2 Other                                0                  0                    0\n3 Other                                0                  0                    0\n4 Pinot Noir                           0                  0                    1\n5 Other                                0                  0                    0\n6 Other                                0                  0                    0\n# ℹ abbreviated name: ¹​`variety_Cabernet Sauvignon`\n# ℹ 2 more variables: `variety_Red Blend` &lt;int&gt;, variety_Other &lt;int&gt;"
  },
  {
    "objectID": "Session02.html#aside-factoring",
    "href": "Session02.html#aside-factoring",
    "title": "Feature Engineering & Variable Selection",
    "section": "Aside: Factoring",
    "text": "Aside: Factoring\n\nSame modelling command\n\nmx &lt;- train(lprice ~ .,\n  data = wino_tr,\n  method = \"lm\",\n  trControl = trainControl(method = \"repeatedcv\", number = 5, repeats = 3)\n)\n\nI should factor this into a function.\n\n\ndo_training &lt;- function(df, formula) {\n  train(formula,\n    data = df,\n    method = \"lm\",\n    trControl = trainControl(method = \"repeatedcv\", number = 5, repeats = 3)\n  )\n}"
  },
  {
    "objectID": "Session01.html#precision-recall-exercise",
    "href": "Session01.html#precision-recall-exercise",
    "title": "Machine Learning Overview",
    "section": "Precision-recall Exercise",
    "text": "Precision-recall Exercise\n\nPrecision: TP / (TP + FP)\nRecall: TP / (TP + FN)\n1,000 credit card transactions\nThe company chooses a risk score cutoff of 77\nThere are 18 transactions with risk above 77.\n\n12 are actually fraud.\n20 fraudulent transactions have risk below 77.\n\nTODO Calculate precision, recall, and accuracy."
  },
  {
    "objectID": "Python01.html#agenda",
    "href": "Python01.html#agenda",
    "title": "Machine Learning in Python",
    "section": "Agenda",
    "text": "Agenda\n\nPython Overview\nReview of Regression\nClassification\nBasic Feature Engineering"
  },
  {
    "objectID": "Python01.html#pip",
    "href": "Python01.html#pip",
    "title": "Machine Learning in Python",
    "section": "Pip",
    "text": "Pip\n\nIn Python, we can typically install packages via pip\nIt is much typical to use pip at commandline.\n\npython -m pip install sampleproject\n\nHere is a “clean” way to do so from within the Python\n\n\nimport subprocess  # A base package we need to install other packages\nimport sys         # A base package we need to install other packages\ninstall = lambda package : subprocess.check_call([sys.executable, \n                                                  \"-m\", \n                                                  \"pip\", \n                                                  \"install\", \n                                                  package])"
  },
  {
    "objectID": "Python01.html#packages",
    "href": "Python01.html#packages",
    "title": "Machine Learning in Python",
    "section": "Packages",
    "text": "Packages\n\nI’ll build a list of packages at install them via a loop.\n\n\nimport subprocess\nimport sys\n\ninstall = lambda package : subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n\nTODO"
  },
  {
    "objectID": "Python01.html#setup",
    "href": "Python01.html#setup",
    "title": "Machine Learning in Python",
    "section": "Setup",
    "text": "Setup\n\nWe will work with a wine dataset that is enormous.\n\nJust to render a bit quickly, take a sample.\nYou are welcome to work with the full dataset!\n\n\nwine &lt;- readRDS(gzcon(url(\"https://cd-public.github.io/courses/rmls25/dat/wine.rds\")))\nwine &lt;- wine %&gt;% drop_na(points, price)\n# performance concession\n# wall &lt;- wine\n# wine = wall[sample(nrow(wall), 100), ]\nsummary(wine)"
  },
  {
    "objectID": "Python01.html#single-variable",
    "href": "Python01.html#single-variable",
    "title": "Machine Learning in Python",
    "section": "Single Variable",
    "text": "Single Variable\n\nPick the poshest province.\n\nwine &lt;- wine %&gt;%\n  mutate(bordeaux = (province == \"Bordeaux\"))\nwine &lt;- wine %&gt;% drop_na(bordeaux)\ntop_n(wine, 10, bordeaux)"
  },
  {
    "objectID": "Python01.html#regress",
    "href": "Python01.html#regress",
    "title": "Machine Learning in Python",
    "section": "Regress",
    "text": "Regress\n\nTake a quick regression model over the wine.\n\nm1 &lt;- lm(price ~ points, data = wine)\nget_regression_table(m1)"
  },
  {
    "objectID": "Python01.html#lets-draw-it",
    "href": "Python01.html#lets-draw-it",
    "title": "Machine Learning in Python",
    "section": "Let’s draw it",
    "text": "Let’s draw it\n#| echo: false\nwine %&gt;%\n  mutate(m1 = predict(m1)) %&gt;%\n  ggplot() +\n  geom_smooth(aes(points, price)) +\n  geom_line(aes(points, m1), size = 2, color = \"orange\") +\n  labs(\n    title = \"Regression of Price on Points\",\n    x = \"Points\", y = \"Price\"\n  )"
  },
  {
    "objectID": "Python01.html#multiple-regression",
    "href": "Python01.html#multiple-regression",
    "title": "Machine Learning in Python",
    "section": "Multiple regression",
    "text": "Multiple regression\nm2 &lt;- lm(price ~ points + bordeaux, data = wine)\nget_regression_table(m2)"
  },
  {
    "objectID": "Python01.html#lets-draw-it-1",
    "href": "Python01.html#lets-draw-it-1",
    "title": "Machine Learning in Python",
    "section": "Let’s draw it",
    "text": "Let’s draw it\n#| echo: false\nwine %&gt;%\n  mutate(m2 = predict(m2)) %&gt;%\n  ggplot() +\n  geom_smooth(aes(points, price)) +\n  geom_line(aes(x = points, y = m2, color = bordeaux), size = 2) +\n  labs(\n    title = \"Multiple Regression of Price on Points and Bordeaux\",\n    x = \"Points\", y = \"Price\"\n  )"
  },
  {
    "objectID": "Python01.html#how-about-with-an-interaction",
    "href": "Python01.html#how-about-with-an-interaction",
    "title": "Machine Learning in Python",
    "section": "How about with an interaction?",
    "text": "How about with an interaction?\nm3 &lt;- lm(price ~ points * bordeaux, data = wine)\nget_regression_table(m3)"
  },
  {
    "objectID": "Python01.html#lets-draw-it-2",
    "href": "Python01.html#lets-draw-it-2",
    "title": "Machine Learning in Python",
    "section": "Let’s draw it",
    "text": "Let’s draw it\n#| echo: false\nwine %&gt;%\n  mutate(m3 = predict(m3)) %&gt;%\n  ggplot() +\n  geom_smooth(aes(points, price)) +\n  geom_line(aes(x = points, y = m3, color = bordeaux), size = 2) +\n  labs(\n    title = \"Interaction Model: Price on Points and Bordeaux\",\n    x = \"Points\", y = \"Price\"\n  )"
  },
  {
    "objectID": "Python01.html#model-diagnostics",
    "href": "Python01.html#model-diagnostics",
    "title": "Machine Learning in Python",
    "section": "Model diagnostics",
    "text": "Model diagnostics\nget_regression_summaries(m1)\nget_regression_summaries(m2)\nget_regression_summaries(m3)"
  },
  {
    "objectID": "Python01.html#split-sample-using-caret",
    "href": "Python01.html#split-sample-using-caret",
    "title": "Machine Learning in Python",
    "section": "Split sample using Caret",
    "text": "Split sample using Caret\nset.seed(505)\ntrain_index &lt;- createDataPartition(wine$price, times = 1, p = 0.8, list = FALSE)\ntrain &lt;- wine[train_index, ]\ntest &lt;- wine[-train_index, ]\nhead(test)"
  },
  {
    "objectID": "Python01.html#compare-rmse-across-models",
    "href": "Python01.html#compare-rmse-across-models",
    "title": "Machine Learning in Python",
    "section": "Compare RMSE across models",
    "text": "Compare RMSE across models\n\nRetrain on models on the training set\n\nms &lt;- list(\n  lm(price ~ points, data = train),\n  lm(price ~ points + bordeaux, data = train),\n  lm(price ~ points * bordeaux, data = train)\n)\n\nTest them all under the same conditions.\n\nmap(ms, function(m) {\n  get_regression_points(m, newdata = test) %&gt;%\n    drop_na(residual) %&gt;%\n    mutate(sq_residuals = residual^2) %&gt;%\n    summarize(rmse = sqrt(mean(sq_residuals))) %&gt;%\n    pluck(\"rmse\")\n}) %&gt;% unlist()"
  },
  {
    "objectID": "Python01.html#group-exercise-30m",
    "href": "Python01.html#group-exercise-30m",
    "title": "Machine Learning in Python",
    "section": "Group Exercise (30m)",
    "text": "Group Exercise (30m)\n\nLoad the wine data set\nVisualize the relationship of points and price\nBonus: Color the observations based on whether the wine is from Bordeaux\nBonus+: Include regression lines\nBonus++: Pick a non-Bordeaux category."
  },
  {
    "objectID": "Python01.html#plot",
    "href": "Python01.html#plot",
    "title": "Machine Learning in Python",
    "section": "Plot",
    "text": "Plot\n\nPoints vs. price.\n\nwine %&gt;%\n  ggplot(aes(x = points, y = price)) +\n  geom_smooth()"
  },
  {
    "objectID": "Python01.html#bonus",
    "href": "Python01.html#bonus",
    "title": "Machine Learning in Python",
    "section": "Bonus",
    "text": "Bonus\n\nColor the Bordeaux region.\n\nwine %&gt;%\n  ggplot(aes(x = points, y = price, color = bordeaux)) +\n  geom_smooth()"
  },
  {
    "objectID": "Python01.html#bonus-1",
    "href": "Python01.html#bonus-1",
    "title": "Machine Learning in Python",
    "section": "Bonus+",
    "text": "Bonus+\n\nInclude regression lines\n\nwine %&gt;%\n  mutate(m = predict(lm(price ~ points, data = wine))) %&gt;%\n  ggplot() +\n  geom_smooth(aes(x = points, y = price, color = bordeaux)) +\n  geom_line(aes(x = points, y = m), colour = \"magenta\")"
  },
  {
    "objectID": "Python01.html#bonus-2",
    "href": "Python01.html#bonus-2",
    "title": "Machine Learning in Python",
    "section": "Bonus++",
    "text": "Bonus++\n\nLet’s look at “reserve”.\n\nwine %&gt;%\n  mutate(reserve = grepl(\"Reserve\", designation)) %&gt;%\n  ggplot(aes(x = points, y = price, color = reserve)) +\n  geom_smooth()"
  },
  {
    "objectID": "Python01.html#bonus-3",
    "href": "Python01.html#bonus-3",
    "title": "Machine Learning in Python",
    "section": "Bonus",
    "text": "Bonus\n\nAnglophones to Francophiles.\n\nwine %&gt;%\n  mutate(reservæ = grepl(\"Reserve\", designation, ignore.case = TRUE) |\n    grepl(\"Reserva\", designation, ignore.case = TRUE)) %&gt;%\n  ggplot(aes(x = points, y = price, color = reservæ)) +\n  geom_smooth()"
  },
  {
    "objectID": "Python01.html#rⴢservæ",
    "href": "Python01.html#rⴢservæ",
    "title": "Machine Learning in Python",
    "section": "RჂservæ",
    "text": "RჂservæ\n\nCross the Alps.\n\nwine %&gt;%\n  mutate(rჂservæ = grepl(\"Reserve|Reserva|Riserva\", designation, ignore.case = TRUE)) %&gt;%\n  ggplot(aes(x = points, y = price, color = rჂservæ)) +\n  geom_smooth()"
  },
  {
    "objectID": "Python01.html#the-math-of-it",
    "href": "Python01.html#the-math-of-it",
    "title": "Machine Learning in Python",
    "section": "The math of it…",
    "text": "The math of it…\n\nSuppose I’m trying to predict sex based on height.\n\nDon’t do this in real life (obviously).\n\nWe start by\n\ndefining the outcome and predictors, and…\ncreating training and test data."
  },
  {
    "objectID": "Python01.html#partition-our-data",
    "href": "Python01.html#partition-our-data",
    "title": "Machine Learning in Python",
    "section": "Partition our Data",
    "text": "Partition our Data\ndata(heights) # from library(dslabs)\ny &lt;- heights$sex\nx &lt;- heights$height\nset.seed(505)\ntest_index &lt;- createDataPartition(y, times = 1, p = 0.5, list = FALSE)\ntest_set &lt;- heights[test_index, ]\ntrain_set &lt;- heights[-test_index, ]\nsummary(heights)\nNote: this vignette is adapted from this book"
  },
  {
    "objectID": "Python01.html#guessing",
    "href": "Python01.html#guessing",
    "title": "Machine Learning in Python",
    "section": "Guessing",
    "text": "Guessing\n\nLet’s start by developing the simplest possible machine algorithm: guessing the outcome.\n\ny_hat &lt;- sample(c(\"Male\", \"Female\"), length(test_index), replace = TRUE)\nRecall:\n\nY hat (written ŷ ) is the predicted value of y (the dependent variable) in a regression equation. It can also be considered to be the average value of the response variable."
  },
  {
    "objectID": "Python01.html#accuracy",
    "href": "Python01.html#accuracy",
    "title": "Machine Learning in Python",
    "section": "Accuracy",
    "text": "Accuracy\n\nThe overall accuracy is simply defined as the overall proportion that is predicted correctly:\n\nmean(y_hat == test_set$sex)\n\nWhat would we have expected the accuracy to be?\n\nWhat much would we have expected accuracy to deviate from that expectionation?"
  },
  {
    "objectID": "Python01.html#lets-do-better",
    "href": "Python01.html#lets-do-better",
    "title": "Machine Learning in Python",
    "section": "Let’s do better…",
    "text": "Let’s do better…\nsummary &lt;- heights %&gt;%\n  group_by(sex) %&gt;%\n  summarize(mean(height), sd(height))\nsummary"
  },
  {
    "objectID": "Python01.html#a-simple-predictive-model",
    "href": "Python01.html#a-simple-predictive-model",
    "title": "Machine Learning in Python",
    "section": "A simple predictive model",
    "text": "A simple predictive model\n\nIdea: Predict \"Male\" if observation is within 2 standard deviations\n\nmale_mean_less_2sd &lt;- summary[2, ][\"mean(height)\"] - 2 * summary[2, ][\"sd(height)\"]\n\ny_hat &lt;- ifelse(x &gt; male_mean_less_2sd, \"Male\", \"Female\") %&gt;%\n  factor(levels = levels(test_set$sex))\n\nc(male_mean_less_2sd, mean(y == y_hat))\n\nThe accuracy goes up from ~0.50 to about ~0.80!!"
  },
  {
    "objectID": "Python01.html#lets-optimize",
    "href": "Python01.html#lets-optimize",
    "title": "Machine Learning in Python",
    "section": "Let’s optimize",
    "text": "Let’s optimize\ncutoff &lt;- seq(61, 70)\nget_accuracy &lt;- function(x) {\n  y_hat &lt;- ifelse(train_set$height &gt; x, \"Male\", \"Female\")\n  mean(y_hat == train_set$sex)\n}\naccuracy &lt;- map(cutoff, get_accuracy)\n\nunlist(accuracy)\n\nMost are much higher than 0.5!!"
  },
  {
    "objectID": "Python01.html#lets-take-a-gander",
    "href": "Python01.html#lets-take-a-gander",
    "title": "Machine Learning in Python",
    "section": "Let’s take a gander",
    "text": "Let’s take a gander\n\nEasier for me to see it.\n\nplot(cutoff, accuracy)"
  },
  {
    "objectID": "Python01.html#optimal-cutoff",
    "href": "Python01.html#optimal-cutoff",
    "title": "Machine Learning in Python",
    "section": "Optimal Cutoff",
    "text": "Optimal Cutoff\nbest_cutoff &lt;- cutoff[which.max(accuracy)]\nbest_cutoff\n\nShould we be cutting at an integer?"
  },
  {
    "objectID": "Python01.html#apply-evaluate",
    "href": "Python01.html#apply-evaluate",
    "title": "Machine Learning in Python",
    "section": "Apply & Evaluate",
    "text": "Apply & Evaluate\ny_hat &lt;- ifelse(test_set$height &gt; best_cutoff, \"Male\", \"Female\")\nmean(y_hat == test_set$sex)"
  },
  {
    "objectID": "Python01.html#confusion-matrix",
    "href": "Python01.html#confusion-matrix",
    "title": "Machine Learning in Python",
    "section": "Confusion matrix",
    "text": "Confusion matrix\ntable(predicted = y_hat, actual = test_set$sex) %&gt;%\n  as.data.frame() %&gt;%\n  ggplot(aes(x = predicted, y = actual)) +\n  geom_tile(aes(fill = Freq), color = \"white\") +\n  scale_fill_gradient(low = \"white\", high = \"blue\") +\n  geom_text(aes(label = Freq), vjust = \"center\", color = \"black\", size = 24) +\n  labs(title = \"Confusion Matrix\", x = \"Predicted\", y = \"Actual\")"
  },
  {
    "objectID": "Python01.html#accuracy-by-sex",
    "href": "Python01.html#accuracy-by-sex",
    "title": "Machine Learning in Python",
    "section": "Accuracy by sex",
    "text": "Accuracy by sex\ntest_set %&gt;%\n  mutate(y_hat = y_hat) %&gt;%\n  group_by(sex) %&gt;%\n  summarize(accuracy = mean(y_hat == sex))\n \nIt’s raining men."
  },
  {
    "objectID": "Python01.html#debrief",
    "href": "Python01.html#debrief",
    "title": "Machine Learning in Python",
    "section": "Debrief",
    "text": "Debrief\n\n\nheights %&gt;%\n  ggplot() +\n  geom_boxplot(aes(height, sex))\n\nslices &lt;- heights %&gt;%\n  group_by(sex) %&gt;%\n  tally()\npie(slices$n, labels = slices$sex)"
  },
  {
    "objectID": "Python01.html#moral-of-the-story",
    "href": "Python01.html#moral-of-the-story",
    "title": "Machine Learning in Python",
    "section": "Moral of the story",
    "text": "Moral of the story"
  },
  {
    "objectID": "Python01.html#other-ethical-issues",
    "href": "Python01.html#other-ethical-issues",
    "title": "Machine Learning in Python",
    "section": "Other ethical issues",
    "text": "Other ethical issues\n\n\n\nDemographic data\nProfit optimizing\nAutonomous cars\nRecommendation engines\n\n\n\nFair housing\nCriminal sentencing\nChoice of classification model\nDrone warfare"
  },
  {
    "objectID": "Python01.html#jameson-on-ethics",
    "href": "Python01.html#jameson-on-ethics",
    "title": "Machine Learning in Python",
    "section": "Jameson on Ethics",
    "text": "Jameson on Ethics\n\nReasonable people will disagree over subtle matters of right and wrong… thus, the important part of data ethics is committing to consider the ethical consequences of your choices.\nThe difference between “regular” ethics and data ethics is that algorithms scale really easily. Thus, seemingly small decisions can have wide-ranging impact."
  },
  {
    "objectID": "Python01.html#calvin-on-ethics",
    "href": "Python01.html#calvin-on-ethics",
    "title": "Machine Learning in Python",
    "section": "Calvin on Ethics",
    "text": "Calvin on Ethics\n\nNo ethical [computation] under capitalism\n\n\nUsage of data | computing is ethicial iff it challenges rather than strengthens existing power relations."
  },
  {
    "objectID": "Python01.html#ml-terms",
    "href": "Python01.html#ml-terms",
    "title": "Machine Learning in Python",
    "section": "ML Terms",
    "text": "ML Terms\nDefinition of ML: using data to find a function that minimizes prediction error.\n\n\n\nFeatures\nVariables\nOutcome variable\nRegression\n\n\n\nRMSE\nClassification\nConfusion matrix\nSplit Samples"
  },
  {
    "objectID": "Python01.html#features",
    "href": "Python01.html#features",
    "title": "Machine Learning in Python",
    "section": "Features",
    "text": "Features\n\nDefinition: Individual measurable properties or attributes of data.\n\nExample: Age, income, and education level in a dataset predicting loan approval."
  },
  {
    "objectID": "Python01.html#variables",
    "href": "Python01.html#variables",
    "title": "Machine Learning in Python",
    "section": "Variables",
    "text": "Variables\n\nDefinition: Data points that can change and impact predictions.\n\nExample: Independent variables like weather, and dependent variables like crop yield."
  },
  {
    "objectID": "Python01.html#outcome-variable",
    "href": "Python01.html#outcome-variable",
    "title": "Machine Learning in Python",
    "section": "Outcome Variable",
    "text": "Outcome Variable\n\nDefinition: The target or dependent variable the model predicts.\n\nExample: Predicting “passed” or “failed” for a student’s exam result."
  },
  {
    "objectID": "Python01.html#features-vs.-variables",
    "href": "Python01.html#features-vs.-variables",
    "title": "Machine Learning in Python",
    "section": "Features vs. Variables",
    "text": "Features vs. Variables\n\nFeatures: Inputs to the model, often selected or engineered from raw data.\n\nExample: “Average monthly income” derived from raw transaction data.\n\n\nVariables: Broader term encompassing both inputs (independent) and outputs (dependent).\n\nExample: “House price” (dependent variable) depends on features like size and location."
  },
  {
    "objectID": "Python01.html#regression",
    "href": "Python01.html#regression",
    "title": "Machine Learning in Python",
    "section": "Regression",
    "text": "Regression\n\nDefinition: Statistical method to model the relationship between variables.\n\nExample: Linear regression predicts house prices based on size and location."
  },
  {
    "objectID": "Python01.html#rmse-root-mean-square-error",
    "href": "Python01.html#rmse-root-mean-square-error",
    "title": "Machine Learning in Python",
    "section": "RMSE (Root Mean Square Error)",
    "text": "RMSE (Root Mean Square Error)\n\nDefinition: A metric to measure prediction accuracy by averaging squared errors.\n\nExample: Lower RMSE in predicting drug response indicates a better model fit."
  },
  {
    "objectID": "Python01.html#classification",
    "href": "Python01.html#classification",
    "title": "Machine Learning in Python",
    "section": "Classification",
    "text": "Classification\n\nDefinition: Task of predicting discrete categories or labels.\n\nExample: Classifying emails as “spam” or “not spam.”"
  },
  {
    "objectID": "Python01.html#confusion-matrix-1",
    "href": "Python01.html#confusion-matrix-1",
    "title": "Machine Learning in Python",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\nDefinition: A table showing model performance in classification tasks.\n\nExample: Matrix rows show true values; columns show predicted outcomes.\n\n#| echo: false\ntable(predicted = y_hat, actual = test_set$sex) %&gt;%\n  as.data.frame() %&gt;%\n  ggplot(aes(x = predicted, y = actual)) +\n  geom_tile(aes(fill = Freq), color = \"white\") +\n  scale_fill_gradient(low = \"white\", high = \"blue\") +\n  geom_text(aes(label = Freq), vjust = \"center\", color = \"black\", size = 24) +\n  labs(title = \"Confusion Matrix\", x = \"Predicted\", y = \"Actual\")"
  },
  {
    "objectID": "Python01.html#split-samples",
    "href": "Python01.html#split-samples",
    "title": "Machine Learning in Python",
    "section": "Split Samples",
    "text": "Split Samples\n\nDefinition: Dividing data into training and testing subsets for validation.\n\nExample: 80% training, 20% testing ensures unbiased model evaluation.\n\n\ntest_set &lt;- heights[test_index, ]\ntrain_set &lt;- heights[-test_index, ]"
  },
  {
    "objectID": "Python01.html#precision-recall-tradeoff",
    "href": "Python01.html#precision-recall-tradeoff",
    "title": "Machine Learning in Python",
    "section": "Precision-recall tradeoff",
    "text": "Precision-recall tradeoff\n\nPrecision: TP / (TP + FP)\nRecall: TP / (TP + FN)\nImagine I have a fraud-detection model that gives 1,000 credit card transactions each a risk score."
  },
  {
    "objectID": "Python01.html#precision-recall-tradeoff-1",
    "href": "Python01.html#precision-recall-tradeoff-1",
    "title": "Machine Learning in Python",
    "section": "Precision-recall tradeoff",
    "text": "Precision-recall tradeoff\n\nImagine I have a fraud-detection model that gives 1,000 credit card transactions each a risk score.\nThe company chooses a risk score cutoff of 77 (for some reason).\nThere are 18 transactions with risk above 77. 12 are actually fraud. 20 fraudulent transactions have risk below 77.\nWhat are precision, recall, and accuracy?"
  },
  {
    "objectID": "Python01.html#precision-recall-exercise",
    "href": "Python01.html#precision-recall-exercise",
    "title": "Machine Learning in Python",
    "section": "Precision-recall Exercise",
    "text": "Precision-recall Exercise\n\nPrecision: TP / (TP + FP)\nRecall: TP / (TP + FN)\n1,000 credit card transactions\nThe company chooses a risk score cutoff of 77\nThere are 18 transactions with risk above 77.\n\n12 are actually fraud.\n20 fraudulent transactions have risk below 77.\n\nTODO Calculate precision, recall, and accuracy."
  },
  {
    "objectID": "Python01.html#solutions",
    "href": "Python01.html#solutions",
    "title": "Machine Learning in Python",
    "section": "Solutions",
    "text": "Solutions\n- Definitions\n  - Precision: TP / (TP + FP)\n  - Recall:    TP / (TP + FN)\n- Computation\n  - Precision: 12 / (12 + 06)  ~= 67%\n  - Recall:    12 / (12 + 20)  ~= 38%\n  - Accuracy: (12 + 962)/1000  ~= 97%"
  },
  {
    "objectID": "Python01.html#precision-recall-tradeoff-2",
    "href": "Python01.html#precision-recall-tradeoff-2",
    "title": "Machine Learning in Python",
    "section": "Precision-recall tradeoff",
    "text": "Precision-recall tradeoff\n\nPrecision: TP / (TP + FP)\nRecall: TP / (TP + FN)\nImage: Hands-on machine learning, A. Geron"
  },
  {
    "objectID": "Python01.html#quarto",
    "href": "Python01.html#quarto",
    "title": "Machine Learning in Python",
    "section": "Quarto",
    "text": "Quarto\n\nI switch from a R backend to a Python backend.\nI add the following below my title in my .qmd header:\n\njupyter: python3"
  }
]